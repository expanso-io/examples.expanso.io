---
title: Step 4 - Configure Batching
sidebar_label: Step 4 - Configure Batching
sidebar_position: 7
description: Optimize cloud storage costs and performance through intelligent message batching strategies
keywords: [batching, cost-optimization, performance, cloud-storage, throughput]
---

# Step 4: Configure Batching

Learn how to configure intelligent batching strategies that dramatically reduce cloud storage costs while optimizing performance. This critical step can reduce your S3 storage costs by 80-95% through efficient batch uploads.

## What You'll Build

In this step, you'll implement batching strategies for:

- **Cost optimization** - Reduce S3 API calls from thousands to dozens per hour
- **Performance tuning** - Balance latency vs throughput for your use case
- **Size-based batching** - Group messages by byte size for consistent uploads
- **Time-based batching** - Ensure maximum latency guarantees
- **Hybrid strategies** - Combine multiple batching triggers for optimal results

## Why Batching Matters

**Cost Reduction:** Amazon S3 charges per API request - batching reduces costs by 80-95%
**Performance:** Larger uploads have better throughput than many small uploads
**Network Efficiency:** Fewer connections reduce network overhead and latency
**Analytics Optimization:** Larger files are more efficient for analytics queries

## Batching Cost Analysis

### Without Batching (Individual Messages)
- **Messages per hour:** 3,600 (1 per second)
- **S3 PUT requests per hour:** 3,600
- **Monthly S3 API cost:** ~$18 (3,600 × 24 × 30 × $0.0005)
- **File count per day:** 86,400 small files

### With Batching (100 messages per batch)
- **Messages per hour:** 3,600 (same volume)  
- **S3 PUT requests per hour:** 36 (100× reduction)
- **Monthly S3 API cost:** ~$0.18 (36 × 24 × 30 × $0.0005)
- **File count per day:** 864 larger files

**Cost savings: $17.82/month (99% reduction)**

## Batching Concepts

### Count-Based Batching

Group messages by count for consistent batch sizes:

```yaml
batching:
  count: 100  # Upload every 100 messages
```

### Time-Based Batching  

Ensure maximum latency by time limits:

```yaml
batching:
  period: 30s  # Upload at least every 30 seconds
```

### Size-Based Batching

Control upload size for network optimization:

```yaml
batching:
  byte_size: 1048576  # Upload when batch reaches 1MB
```

### Hybrid Batching (Recommended)

Combine multiple triggers for optimal behavior:

```yaml
batching:
  count: 100       # Batch size target
  period: 30s      # Maximum latency
  byte_size: 5MB   # Maximum batch size
```

## Implementation

### Basic Batching Setup

Start with simple count and time-based batching:

```yaml title="step4-basic-batching.yaml"
input:
  generate:
    interval: 1s
    mapping: |
      root.id = uuid_v4()
      root.timestamp = now()
      root.level = ["INFO", "WARN", "ERROR"].index(random_int() % 3)
      root.service = ["auth-service", "payment-service", "user-service"].index(random_int() % 3)
      root.message = "Demo log from " + this.service
      root.user_id = "user_" + (random_int() % 10)
      root.request_id = uuid_v4()

pipeline:
  processors:
    # Add lineage metadata
    - mapping: |
        root = this
        root.lineage_node_id = env("NODE_ID").or("edge-node-demo")
        root.lineage_pipeline = "log-enrichment-s3-demo"
        root.lineage_processed_at = now()

    # Restructure format
    - mapping: |
        root.event = {
          "id": this.id,
          "timestamp": this.timestamp,
          "level": this.level,
          "service": this.service,
          "message": this.message,
          "user_id": this.user_id,
          "request_id": this.request_id
        }
        root.metadata = {
          "node_id": this.lineage_node_id,
          "pipeline": this.lineage_pipeline,
          "processed_at": this.lineage_processed_at
        }

output:
  # Test with file output first (before S3)
  file:
    path: /tmp/batched-logs/batch_${!timestamp_unix()}.jsonl
    codec: lines
    batching:
      count: 10        # Small batch for testing
      period: 30s      # Maximum 30-second latency
      
# Monitor batching metrics
metrics:
  prometheus:
    prefix: batch_demo
```

Deploy basic batching:

```bash
# Create output directory
mkdir -p /tmp/batched-logs

# Set environment
export NODE_ID="edge-node-01"

# Deploy batching pipeline  
expanso pipeline deploy step4-basic-batching.yaml --name basic-batching

# Monitor batch creation
watch -n 5 'ls -la /tmp/batched-logs/ | tail -5'

# Check batch file contents
tail -f /tmp/batched-logs/*.jsonl
```

**Expected behavior:**
- New batch file created every 10 messages OR every 30 seconds
- Files named with Unix timestamp: `batch_1705315800.jsonl`
- Each file contains exactly 10 JSON lines (unless time trigger activated)

### Advanced Batching with Size Optimization

Implement sophisticated batching with size and performance controls:

```yaml title="step4-advanced-batching.yaml"
input:
  generate:
    interval: 500ms  # Higher frequency for testing
    mapping: |
      # Generate varied-size messages for realistic testing
      root.id = uuid_v4()
      root.timestamp = now()
      root.level = ["INFO", "INFO", "WARN", "ERROR", "DEBUG"].index(random_int() % 5)
      
      let services = [
        "auth-service", "payment-service", "user-service", 
        "notification-service", "analytics-service"
      ]
      root.service = $services.index(random_int() % 5)
      root.user_id = "user_" + (random_int() % 100)
      root.request_id = uuid_v4()
      
      # Vary message content size for realistic batching
      let message_base = "Application log from " + this.service
      root.message = $message_base + " with additional context " + "x".repeat(random_int() % 200)
      
      # Add variable-size contextual data
      root.context = match {
        this.service == "payment-service" => {
          "transaction_id": uuid_v4(),
          "amount": random_int() % 10000,
          "currency": "USD",
          "merchant_details": "Large merchant name with extended information " + "y".repeat(random_int() % 100)
        }
        this.service == "analytics-service" => {
          "event_data": range(0, random_int() % 20).map(i -> {"field_" + $i: "value_" + $i})
        }
        _ => {"simple": "context"}
      }

pipeline:
  processors:
    # Enhanced lineage with performance tracking
    - mapping: |
        root = this
        let processing_start = now()
        
        root.lineage = {
          "node_id": env("NODE_ID").or("edge-node-" + uuid_v4().slice(0, 8)),
          "node_region": env("AWS_REGION").or("us-east-1"),
          "pipeline": "log-enrichment-batching-demo",
          "processed_at": $processing_start,
          "sequence_number": counter("processed"),
          "batch_candidate": true
        }

    # Format restructuring  
    - mapping: |
        root.event = {
          "id": this.id,
          "timestamp": this.timestamp,
          "level": this.level,
          "service": this.service,
          "message": this.message,
          "user_id": this.user_id,
          "request_id": this.request_id,
          "context": this.context
        }
        
        root.metadata = {
          "lineage": this.lineage,
          "message_size_bytes": this.string().length(),
          "batch_ready": true
        }

output:
  broker:
    pattern: fan_out
    outputs:
      # Size-optimized batching
      - file:
          path: /tmp/batched-logs/size_batch_${!timestamp_unix()}.jsonl
          codec: lines
          batching:
            byte_size: 10240    # 10KB batches
            period: 1m          # Max 1-minute latency
            count: 1000         # Fallback message limit
      
      # Count-optimized batching
      - file:
          path: /tmp/batched-logs/count_batch_${!timestamp_unix()}.jsonl
          codec: lines
          batching:
            count: 50           # Fixed count batches
            period: 45s         # Max 45-second latency
      
      # Time-optimized batching (low latency)
      - file:
          path: /tmp/batched-logs/time_batch_${!timestamp_unix()}.jsonl
          codec: lines
          batching:
            period: 15s         # Low latency priority
            count: 200          # Allow larger batches if available
            byte_size: 50KB     # Prevent huge batches

# Enhanced metrics for batching analysis
metrics:
  prometheus:
    prefix: advanced_batching
    labels:
      pipeline: "log-enrichment"
      environment: ${ENVIRONMENT}
```

Deploy advanced batching:

```bash
# Set environment
export ENVIRONMENT="development"
export AWS_REGION="us-west-2"

# Clean previous test files
rm -f /tmp/batched-logs/*

# Deploy advanced pipeline
expanso pipeline stop basic-batching
expanso pipeline deploy step4-advanced-batching.yaml --name advanced-batching

# Monitor different batching strategies
echo "=== Size-based batches ==="
watch -n 10 'ls -lh /tmp/batched-logs/size_batch_* 2>/dev/null | tail -3'

echo "=== Count-based batches ==="
watch -n 10 'ls -lh /tmp/batched-logs/count_batch_* 2>/dev/null | tail -3'

echo "=== Time-based batches ==="
watch -n 10 'ls -lh /tmp/batched-logs/time_batch_* 2>/dev/null | tail -3'
```

### Production Batching with Performance Monitoring

Implement production-ready batching with comprehensive monitoring:

```yaml title="step4-production-batching.yaml"
input:
  generate:
    interval: 200ms  # High throughput for production simulation
    mapping: |
      root.id = uuid_v4()
      root.timestamp = now()
      
      # Realistic log level distribution
      let level_rand = random_int() % 100
      root.level = if $level_rand < 70 {
        "INFO"
      } else if $level_rand < 85 {
        "WARN"  
      } else if $level_rand < 95 {
        "ERROR"
      } else {
        "FATAL"
      }
      
      # Production services with realistic distribution
      let service_weights = [
        {"name": "api-gateway", "weight": 30},
        {"name": "auth-service", "weight": 20},
        {"name": "user-service", "weight": 15},
        {"name": "payment-service", "weight": 15},
        {"name": "notification-service", "weight": 10},
        {"name": "analytics-service", "weight": 10}
      ]
      # Weighted random selection (simplified)
      root.service = $service_weights.index(random_int() % 6).name
      
      # Rich contextual data
      root.user_id = "user_" + (random_int() % 10000)
      root.session_id = "session_" + (random_int() % 1000)
      root.request_id = uuid_v4()
      root.trace_id = uuid_v4()
      
      # Performance metrics
      root.duration_ms = random_int() % 5000 + 10
      root.status_code = match {
        this.level == "INFO" => [200, 201, 202, 204].index(random_int() % 4)
        this.level == "WARN" => [400, 401, 403, 404, 429].index(random_int() % 5)
        this.level == "ERROR" => [500, 502, 503, 504].index(random_int() % 4)
        _ => 500
      }
      
      # Production-realistic message content
      root.message = match {
        this.service == "api-gateway" => "Request processed: " + this.status_code + " in " + this.duration_ms + "ms"
        this.service == "auth-service" => if this.level == "ERROR" { "Authentication failed for user " + this.user_id } else { "User authenticated successfully" }
        this.service == "payment-service" => if this.level == "ERROR" { "Payment processing failed" } else { "Payment processed: $" + (random_int() % 1000 + 10) }
        _ => "Service operation completed with status " + this.status_code
      }

pipeline:
  processors:
    # Add comprehensive lineage and performance tracking
    - mapping: |
        root = this
        let processing_start_time = now()
        
        root.lineage = {
          "node_id": env("NODE_ID").or("edge-node-prod-" + uuid_v4().slice(0, 6)),
          "node_region": env("AWS_REGION").or("us-east-1"),
          "node_zone": env("AVAILABILITY_ZONE").or("us-east-1a"),
          "pipeline": "log-enrichment-production",
          "pipeline_version": env("PIPELINE_VERSION").or("3.0.0"),
          "processed_at": $processing_start_time,
          "processing_start": $processing_start_time,
          "sequence_number": counter("messages_processed"),
          "environment": env("ENVIRONMENT").or("production"),
          "deployment_id": env("DEPLOYMENT_ID").or("unknown")
        }

    # Enhanced restructuring with batch optimization hints
    - mapping: |
        root.event = {
          "id": this.id,
          "timestamp": this.timestamp,
          "level": this.level,
          "service": this.service,
          "message": this.message,
          "user_id": this.user_id,
          "session_id": this.session_id,
          "request_id": this.request_id,
          "trace_id": this.trace_id,
          "duration_ms": this.duration_ms,
          "status_code": this.status_code
        }
        
        root.metadata = {
          "lineage": this.lineage,
          "quality": {
            "message_size_bytes": this.string().length(),
            "is_error": this.level == "ERROR" || this.level == "FATAL",
            "is_high_priority": this.level == "FATAL" || this.status_code >= 500,
            "batch_priority": if this.level == "FATAL" { "high" } else { "normal" }
          },
          "processing": {
            "transformations_applied": ["lineage", "restructure"],
            "ready_for_batch": true,
            "compression_candidate": this.string().length() > 500
          }
        }

    # Add batch performance tracking
    - mapping: |
        root = this
        root.metadata.batch_context = {
          "batch_timestamp": now(),
          "estimated_batch_size": this.string().length(),
          "batch_id_candidate": "batch_" + now().format_timestamp("20060102_150405", "UTC"),
          "routing_key": this.event.service + "_" + this.event.level
        }

output:
  broker:
    pattern: fan_out
    outputs:
      # Production-optimized batching for normal priority
      - file:
          path: /tmp/production-logs/prod_${!timestamp_unix()}_${!hostname()}.jsonl
          codec: lines
          batching:
            count: 100              # Optimal batch size for most use cases
            period: 2m              # 2-minute maximum latency for normal logs
            byte_size: 1048576      # 1MB maximum batch size
            processors:
              # Compress batches for storage efficiency
              - compress:
                  algorithm: gzip
                  level: 6

      # High-priority batching for errors
      - file:
          path: /tmp/production-logs/errors_${!timestamp_unix()}_${!hostname()}.jsonl
          codec: lines
          processors:
            # Filter for high-priority messages only
            - bloblang: |
                root = if this.metadata.quality.is_high_priority { this } else { deleted() }
          batching:
            count: 20               # Smaller batches for faster processing
            period: 30s             # Low latency for error logs
            byte_size: 524288       # 512KB max for quick uploads

      # Analytics-optimized batching
      - file:
          path: /tmp/production-logs/analytics_${!timestamp("%Y%m%d_%H")}_${!hostname()}.jsonl
          codec: lines
          processors:
            # Transform to analytics-friendly format
            - mapping: |
                root = {
                  "timestamp": this.event.timestamp,
                  "service": this.event.service,
                  "level": this.event.level,
                  "duration_ms": this.event.duration_ms,
                  "status_code": this.event.status_code,
                  "user_id": this.event.user_id,
                  "node_id": this.metadata.lineage.node_id,
                  "environment": this.metadata.lineage.environment
                }
          batching:
            count: 1000             # Large batches for analytics efficiency
            period: 5m              # Longer batching window for throughput
            byte_size: 5242880      # 5MB batches for optimal analytics performance

# Comprehensive metrics for production monitoring
metrics:
  prometheus:
    prefix: production_batching
    labels:
      environment: ${ENVIRONMENT}
      node_id: ${NODE_ID}
      pipeline_version: ${PIPELINE_VERSION}
    push_gateway:
      url: ${PROMETHEUS_PUSHGATEWAY_URL}
      job_name: log_enrichment_batching
```

Deploy production batching:

```bash
# Set production environment variables
export NODE_ID="edge-node-prod-01"
export AWS_REGION="us-west-2"
export AVAILABILITY_ZONE="us-west-2a"
export ENVIRONMENT="production"
export PIPELINE_VERSION="3.0.0"
export DEPLOYMENT_ID="deploy_$(date +%Y%m%d_%H%M%S)"

# Create production log directories
mkdir -p /tmp/production-logs

# Deploy production pipeline
expanso pipeline stop advanced-batching
expanso pipeline deploy step4-production-batching.yaml --name production-batching

# Monitor different batch types
echo "=== Normal Priority Batches ==="
watch -n 15 'ls -lh /tmp/production-logs/prod_* 2>/dev/null | tail -3'

echo "=== Error Priority Batches ==="
watch -n 15 'ls -lh /tmp/production-logs/errors_* 2>/dev/null | tail -3'

echo "=== Analytics Batches ==="
watch -n 15 'ls -lh /tmp/production-logs/analytics_* 2>/dev/null | tail -3'

# Check compression effectiveness
echo "=== Compression Analysis ==="
find /tmp/production-logs -name "prod_*" -exec sh -c 'echo "File: $1, Size: $(wc -c < "$1") bytes"' _ {} \;
```

## Batching Performance Optimization

### Batch Size Tuning

Optimize batch sizes for your specific use case:

```yaml
# Test different batch sizes
batching:
  # Small batches: Lower latency, higher API costs
  count: 10
  period: 10s
  
  # Medium batches: Balanced latency and cost
  count: 100
  period: 60s
  
  # Large batches: Higher latency, lower costs
  count: 1000
  period: 300s
```

### Dynamic Batching

Adjust batching based on message characteristics:

```yaml
# Route to different batching strategies
processors:
  - branch:
      request_map: 'root = this'
      result_map: 'root = this'
      processors:
        # High-priority messages: small batches
        - switch:
          - check: 'this.level == "FATAL" || this.status_code >= 500'
            processors:
              - mapping: 'root.batch_strategy = "high_priority"'
        
        # Normal messages: standard batching  
        - mapping: 'root.batch_strategy = "standard"'
```

### Memory-Aware Batching

Prevent memory issues with large batches:

```yaml
batching:
  count: 1000
  byte_size: 10485760  # 10MB limit prevents memory issues
  period: 300s
  
  # Add memory monitoring
  processors:
    - mapping: |
        if env("MEMORY_USAGE_PCT").number() > 80 {
          # Force smaller batches under memory pressure
          root.batch_override = {
            "count": 100,
            "reason": "memory_pressure"
          }
        }
```

## Monitoring and Alerting

### Batch Performance Metrics

Track key batching performance indicators:

```bash
# Monitor batch file sizes
find /tmp/production-logs -name "*.jsonl" -exec sh -c 'echo "$(wc -l < "$1") lines, $(wc -c < "$1") bytes: $1"' _ {} \; | sort -n

# Calculate average batch sizes
find /tmp/production-logs -name "prod_*" -exec wc -l {} \; | awk '{sum+=$1; count++} END {print "Average batch size:", sum/count, "messages"}'

# Monitor batching latency (file creation frequency)
ls -lt /tmp/production-logs/prod_* | head -5 | awk '{print $6, $7, $8, $9}' | while read date time file; do
  echo "Created: $date $time - $file"
done
```

### Cost Analysis

Calculate actual cost savings from batching:

```bash
# Calculate S3 API costs without batching
messages_per_hour=18000  # 5 messages per second
api_cost_per_1000=0.50   # $0.0005 per PUT request
unbatched_cost=$(echo "scale=2; $messages_per_hour * 24 * 30 * $api_cost_per_1000 / 1000" | bc)

# Calculate S3 API costs with batching (100 messages per batch)
batch_size=100
batched_requests_per_hour=$(echo "$messages_per_hour / $batch_size" | bc)
batched_cost=$(echo "scale=2; $batched_requests_per_hour * 24 * 30 * $api_cost_per_1000 / 1000" | bc)

echo "Monthly cost without batching: \$$unbatched_cost"
echo "Monthly cost with batching: \$$batched_cost"
echo "Monthly savings: \$$(echo "$unbatched_cost - $batched_cost" | bc)"
```

## Troubleshooting Batching Issues

### Batches Not Being Created

**Problem:** Files not appearing in output directory

**Solutions:**

1. **Check batch triggers:**
```yaml
# Lower thresholds for testing
batching:
  count: 1        # Single message batches
  period: 5s      # Very short timeout
```

2. **Add debug logging:**
```yaml
# Monitor batch accumulation
metrics:
  prometheus:
    prefix: batch_debug
    
processors:
  - mapping: |
      root = this
      root.debug_batch_info = {
        "message_count": counter("debug_messages"),
        "timestamp": now()
      }
```

### Memory Issues with Large Batches

**Problem:** High memory usage or out-of-memory errors

**Solutions:**

1. **Limit batch sizes:**
```yaml
batching:
  byte_size: 5242880  # 5MB max
  count: 500          # Reasonable message count
```

2. **Add memory monitoring:**
```yaml
# Monitor memory usage
processors:
  - mapping: |
      let memory_pct = env("MEMORY_USAGE").number()
      if $memory_pct > 85 {
        root.batch_warning = "high_memory_usage"
      }
```

### Uneven Batch Sizes

**Problem:** Inconsistent batch sizes affecting performance

**Solutions:**

1. **Use hybrid triggers:**
```yaml
batching:
  count: 100      # Target size
  period: 60s     # Backup trigger
  byte_size: 1MB  # Size limit
```

2. **Monitor batch distribution:**
```bash
# Analyze batch size distribution
find /tmp/production-logs -name "*.jsonl" -exec wc -l {} \; | cut -d' ' -f1 | sort -n | uniq -c
```

## Key Takeaways

After completing this step, you understand:

✅ **Cost Optimization:** How batching reduces cloud storage costs by 80-95%
✅ **Batching Strategies:** Count, time, and size-based batching approaches  
✅ **Performance Tuning:** Balancing latency, throughput, and resource usage
✅ **Production Patterns:** Multi-tier batching for different message priorities
✅ **Monitoring:** Tracking batch performance and cost savings

## Next Steps

Your batching strategy is optimized! The final step configures S3 export for cloud storage:

<div style={{display: 'flex', gap: '1.5rem', marginTop: '2rem', marginBottom: '3rem', flexWrap: 'wrap', justifyContent: 'flex-start'}}>
  <a href="./step-5-export-s3" className="button button--primary button--lg" style={{display: 'inline-flex', alignItems: 'center', justifyContent: 'center', textDecoration: 'none', borderRadius: '8px', padding: '1rem 2rem', fontWeight: '600', minWidth: '240px', boxShadow: '0 2px 8px rgba(0,0,0,0.15)', cursor: 'pointer', transition: 'all 0.2s ease'}}>
    Step 5: Export to S3
  </a>
  <a href="./complete-log-enrichment" className="button button--secondary button--lg" style={{display: 'inline-flex', alignItems: 'center', justifyContent: 'center', textDecoration: 'none', borderRadius: '8px', padding: '1rem 2rem', fontWeight: '600', minWidth: '240px', boxShadow: '0 2px 8px rgba(0,0,0,0.15)', cursor: 'pointer', transition: 'all 0.2s ease'}}>
    Skip to Complete Pipeline
  </a>
</div>

---

**Next:** [Export to S3](./step-5-export-s3) to integrate with cloud storage and analytics

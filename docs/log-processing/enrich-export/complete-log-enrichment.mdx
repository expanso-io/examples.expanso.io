---
title: Complete Log Enrichment Pipeline
sidebar_label: Complete Pipeline
sidebar_position: 9
description: Production-ready log enrichment pipeline with all features integrated for scalable S3 export
keywords: [complete-pipeline, production, log-enrichment, s3-export, monitoring]
---

import CodeBlock from '@theme/CodeBlock';
import completeYaml from '!!raw-loader!../../../examples/log-processing/enrich-export-complete.yaml';

# Complete Log Enrichment Pipeline

This is the complete, production-ready log enrichment pipeline that combines all the techniques from the step-by-step tutorials. Deploy this pipeline for immediate log enrichment with metadata tracking, intelligent batching, and optimized S3 storage.

## Pipeline Features

âœ… **Flexible Input Sources** - Support for generated, file, and syslog inputs
âœ… **Comprehensive Lineage** - Full audit trail and processing metadata
âœ… **Analytics-Optimized Structure** - Clean event/metadata separation
âœ… **Intelligent Batching** - Cost-optimized uploads with performance tuning
âœ… **S3 Integration** - Partitioned, compressed storage with lifecycle management
âœ… **Error Handling** - Robust retry logic and dead letter queues
âœ… **Monitoring** - Comprehensive metrics and alerting
âœ… **Security** - Credential management and access controls

## Complete Pipeline Configuration

<CodeBlock language="yaml" title="enrich-export-complete.yaml" showLineNumbers>
  {completeYaml}
</CodeBlock>

<a
  href="/files/log-processing/enrich-export-complete.yaml"
  download
  className="button button--primary button--lg margin-top--md"
>
  ðŸ“¥ Download Complete Pipeline
</a>

## Quick Deployment

### Prerequisites

Ensure these requirements are met before deployment:

- âœ… AWS credentials configured (`aws configure` or IAM role)
- âœ… S3 bucket created with write permissions
- âœ… Expanso platform access configured
- âœ… Environment variables set (see below)

### Environment Configuration

Set these environment variables for production deployment:

```bash
# Core identification
export NODE_ID="edge-node-prod-01"
export AWS_REGION="us-west-2" 
export AVAILABILITY_ZONE="us-west-2a"
export INSTANCE_TYPE="c5.large"

# Pipeline configuration
export PIPELINE_VERSION="5.0.0"
export ENVIRONMENT="production"
export DEPLOYMENT_ID="deploy_$(date +%Y%m%d_%H%M%S)"
export GIT_COMMIT="$(git rev-parse --short HEAD 2>/dev/null || echo 'unknown')"

# S3 storage configuration
export S3_BUCKET_NAME="your-production-logs"
export S3_ERROR_BUCKET_NAME="your-production-errors"  
export AWS_PROFILE="expanso-prod"

# Optional: monitoring configuration
export PROMETHEUS_PUSHGATEWAY_URL="http://pushgateway:9091"
export LOG_LEVEL="info"
export DEBUG_MODE="false"

# Data source configuration (choose one)
export INPUT_MODE="generate"  # Options: generate, file, syslog
export LOG_FILE_PATH="/var/log/app/*.log"  # For file input
export SYSLOG_ADDRESS="0.0.0.0:1514"      # For syslog input
```

### Deployment Commands

Deploy the complete pipeline:

```bash
# Download the complete pipeline
wget https://examples.expanso.io/files/log-processing/enrich-export-complete.yaml

# Validate configuration
expanso pipeline validate enrich-export-complete.yaml

# Deploy to production
expanso pipeline deploy enrich-export-complete.yaml \
  --name log-enrichment-production \
  --environment production

# Verify deployment
expanso pipeline status log-enrichment-production

# Monitor initial operation
expanso pipeline logs log-enrichment-production --follow --lines 20
```

## Input Source Configuration

The pipeline supports multiple input sources. Choose the appropriate configuration:

### Generated Data (Testing)

For development and testing with synthetic data:

```yaml
input:
  generate:
    interval: ${GENERATE_INTERVAL:-1s}
    mapping: |
      # Production-realistic synthetic logs
      root.id = uuid_v4()
      root.timestamp = now()
      root.level = ["INFO", "INFO", "INFO", "WARN", "ERROR"].index(random_int() % 5)
      root.service = ["api-gateway", "auth-service", "payment-service", "user-service"].index(random_int() % 4)
      root.user_id = "user_" + (random_int() % 10000)
      root.request_id = uuid_v4()
      root.message = "Generated log from " + this.service
```

### File Input (Production)

For real application log files:

```yaml
input:
  file:
    paths:
      - ${LOG_FILE_PATH:-/var/log/app/*.log}
    codec: lines
    scanner:
      lines:
        max_buffer_size: 1000000  # 1MB buffer
    
    # Process existing files and watch for new ones
    multipart: false
    max_buffer: 1000
    
    # Add file context to logs
    processors:
      - mapping: |
          root = this.parse_json().or({"raw_message": this})
          root.file_context = {
            "source_file": file_path(),
            "file_offset": file_offset(),
            "ingested_at": now()
          }
```

### Syslog Input (Network Logs)

For centralized syslog collection:

```yaml
input:
  socket_server:
    network: "udp"
    address: "${SYSLOG_ADDRESS:-0.0.0.0:1514}"
    
    # Syslog parsing
    processors:
      - mapping: |
          # Parse RFC5424 syslog format
          let parsed = this.parse_syslog()
          root = $parsed.structured_data.or({})
          root.syslog = {
            "facility": $parsed.facility,
            "severity": $parsed.severity,
            "timestamp": $parsed.timestamp,
            "hostname": $parsed.hostname,
            "app_name": $parsed.app_name,
            "proc_id": $parsed.proc_id,
            "msg_id": $parsed.msg_id,
            "message": $parsed.message
          }
          root.id = uuid_v4()
          root.timestamp = $parsed.timestamp
          root.level = match {
            $parsed.severity <= 3 => "ERROR"
            $parsed.severity <= 4 => "WARN" 
            $parsed.severity <= 6 => "INFO"
            _ => "DEBUG"
          }
```

## Production Optimization

### Performance Tuning

Optimize for your specific throughput requirements:

```yaml
# High-throughput configuration
pipeline:
  processors:
    - mapping: |
        # Pre-calculate expensive operations
        let processing_time = now()
        let node_info = env("NODE_ID") + ":" + env("AWS_REGION")
        
        # Efficient field assignments
        root = this
        root.lineage_processed_at = $processing_time
        root.lineage_node_info = $node_info

# Optimized batching for cost vs latency
output:
  aws_s3:
    batching:
      # Tune based on your requirements:
      # High throughput: count=1000, period=10m
      # Low latency: count=50, period=30s
      # Balanced: count=200, period=2m (default)
      count: ${BATCH_COUNT:-200}
      period: ${BATCH_PERIOD:-2m}
      byte_size: ${BATCH_SIZE:-5242880}  # 5MB default
```

### Memory Management

Configure memory usage for your deployment:

```yaml
# Memory-conscious configuration
buffer:
  memory:
    limit: "${BUFFER_MEMORY:-100MB}"
    
pipeline:
  processors:
    # Use efficient transformations
    - mapping: |
        # Avoid large intermediate objects
        root.event = this.pick("id", "timestamp", "level", "service", "message", "user_id", "request_id")
        root.metadata = {
          "node_id": env("NODE_ID"),
          "processed_at": now()
        }
```

### Error Handling

Implement comprehensive error recovery:

```yaml
# Dead letter queue for failed messages
output:
  broker:
    pattern: fan_out
    outputs:
      # Primary S3 export
      - aws_s3:
          bucket: ${S3_BUCKET_NAME}
          # ... S3 configuration ...
      
      # Dead letter queue for failed uploads
      - file:
          path: /var/log/expanso/failed_uploads_${!timestamp_unix()}.jsonl
          codec: lines
        processors:
          - mapping: |
              # Only capture messages that failed S3 upload
              root = if meta("aws_s3_upload_failed") {
                {
                  "original_message": this,
                  "failure_reason": meta("failure_reason"),
                  "failed_at": now(),
                  "retry_count": meta("retry_count").or(0)
                }
              } else {
                deleted()
              }
```

## Monitoring and Alerting

### Pipeline Metrics

Monitor key pipeline performance indicators:

```yaml
metrics:
  prometheus:
    prefix: log_enrichment_prod
    labels:
      environment: ${ENVIRONMENT}
      node_id: ${NODE_ID}
      pipeline_version: ${PIPELINE_VERSION}
      deployment_id: ${DEPLOYMENT_ID}
    
    # Custom metrics
    metric_relabeling:
      - source_labels: [__name__]
        regex: "log_enrichment_prod_(.*)"
        target_label: "metric_name"
        replacement: "${1}"
    
    push_gateway:
      url: ${PROMETHEUS_PUSHGATEWAY_URL}
      job_name: "log_enrichment"
      push_interval: "30s"
```

### Health Checks

Implement pipeline health monitoring:

```bash
#!/bin/bash
# health_check.sh - Pipeline health monitoring script

PIPELINE_NAME="log-enrichment-production"

# Check pipeline status
status=$(expanso pipeline status $PIPELINE_NAME --format json | jq -r '.status')

if [ "$status" != "RUNNING" ]; then
  echo "CRITICAL: Pipeline $PIPELINE_NAME is not running (status: $status)"
  exit 2
fi

# Check recent message processing
recent_messages=$(expanso pipeline metrics $PIPELINE_NAME | grep "messages_processed_total" | tail -1)
if [ -z "$recent_messages" ]; then
  echo "WARNING: No recent messages processed"
  exit 1
fi

# Check S3 upload success rate
upload_errors=$(aws logs filter-log-events \
  --log-group-name "/expanso/pipeline/$PIPELINE_NAME" \
  --start-time $(date -d "5 minutes ago" +%s000) \
  --filter-pattern "ERROR" | jq '.events | length')

if [ "$upload_errors" -gt 10 ]; then
  echo "WARNING: High error rate detected ($upload_errors errors in last 5 minutes)"
  exit 1
fi

echo "OK: Pipeline is healthy"
exit 0
```

### Alerting Rules

Configure alerts for operational issues:

```yaml
# Prometheus alerting rules
groups:
  - name: log_enrichment_alerts
    rules:
      - alert: LogEnrichmentPipelineDown
        expr: up{job="log_enrichment"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Log enrichment pipeline is down"
          description: "Pipeline {{ $labels.pipeline }} on node {{ $labels.node_id }} has been down for more than 1 minute"
      
      - alert: HighS3UploadErrorRate  
        expr: rate(log_enrichment_s3_upload_errors_total[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High S3 upload error rate"
          description: "S3 upload error rate is {{ $value }} errors/second for pipeline {{ $labels.pipeline }}"
      
      - alert: LogProcessingLatencyHigh
        expr: histogram_quantile(0.95, rate(log_enrichment_processing_duration_seconds_bucket[5m])) > 5
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "High log processing latency"
          description: "95th percentile processing latency is {{ $value }}s for pipeline {{ $labels.pipeline }}"
```

## Security Configuration

### Credential Management

Secure credential handling for production:

```yaml
# Use IAM roles instead of static credentials (recommended)
output:
  aws_s3:
    bucket: ${S3_BUCKET_NAME}
    # No credentials section - uses IAM role
    region: ${AWS_REGION}

# OR use AWS profiles for development
output:
  aws_s3:
    bucket: ${S3_BUCKET_NAME}
    credentials:
      profile: ${AWS_PROFILE}
    region: ${AWS_REGION}

# OR use environment variables (least secure)
output:
  aws_s3:
    bucket: ${S3_BUCKET_NAME}
    credentials:
      id: ${AWS_ACCESS_KEY_ID}
      secret: ${AWS_SECRET_ACCESS_KEY}
    region: ${AWS_REGION}
```

### Data Privacy

Implement data privacy controls:

```yaml
pipeline:
  processors:
    # Data classification and masking
    - mapping: |
        root = this
        
        # Classify data sensitivity
        root.data_classification = match {
          this.user_id != null || this.email != null => "pii"
          this.payment_data != null => "financial"
          _ => "general"
        }
        
        # Mask sensitive data if required
        if env("MASK_PII") == "true" && root.data_classification == "pii" {
          root.user_id = this.user_id.hash("sha256").slice(0, 8)
          root.ip_address = "masked"
        }
```

### Access Controls

Implement principle of least privilege:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:PutObjectAcl"
      ],
      "Resource": [
        "arn:aws:s3:::your-production-logs/*",
        "arn:aws:s3:::your-production-errors/*"
      ],
      "Condition": {
        "StringEquals": {
          "s3:x-amz-server-side-encryption": "AES256"
        }
      }
    }
  ]
}
```

## Scaling and High Availability

### Horizontal Scaling

Deploy multiple pipeline instances:

```bash
# Deploy multiple instances with different node IDs
for i in {1..3}; do
  export NODE_ID="edge-node-prod-0$i"
  expanso pipeline deploy enrich-export-complete.yaml \
    --name "log-enrichment-prod-0$i" \
    --environment production
done
```

### Load Balancing

Distribute load across multiple pipelines:

```yaml
# Use different input configurations per instance
input:
  file:
    paths:
      - /var/log/app/shard_${NODE_SHARD:-1}/*.log  # Different shard per instance
    codec: lines
```

### Disaster Recovery

Implement cross-region disaster recovery:

```yaml
# Multi-region S3 export
output:
  broker:
    pattern: fan_out
    outputs:
      # Primary region
      - aws_s3:
          bucket: ${S3_BUCKET_NAME}
          region: ${AWS_REGION}
          # ... configuration ...
      
      # Disaster recovery region
      - aws_s3:
          bucket: ${S3_BACKUP_BUCKET_NAME}
          region: ${AWS_BACKUP_REGION}
          # ... configuration ...
```

## Performance Benchmarks

### Typical Performance Metrics

Expected performance for the complete pipeline:

| Metric | Value | Notes |
|--------|-------|-------|
| **Throughput** | 10,000 msgs/sec | Single c5.large instance |
| **Latency P95** | 150ms | Including S3 batching |
| **Memory Usage** | 200-400MB | With 5MB batch sizes |
| **CPU Usage** | 30-50% | During normal operation |
| **Storage Cost** | $0.15/GB/month | With compression and IA |
| **API Cost** | $0.10/million msgs | With 200-message batches |

### Optimization Checklist

- [ ] **Batch sizes** tuned for your latency requirements
- [ ] **Compression** enabled for storage cost reduction
- [ ] **Partitioning** configured for analytics efficiency
- [ ] **Monitoring** and alerting configured
- [ ] **Error handling** and dead letter queues implemented
- [ ] **Security** policies and credential management configured
- [ ] **Disaster recovery** and backup strategies in place

## Next Steps

Your complete log enrichment pipeline is ready! Consider these next steps:

### Analytics Integration

- Set up **AWS Glue** for data cataloging
- Configure **AWS Athena** for ad-hoc queries  
- Implement **Amazon Kinesis Analytics** for real-time processing
- Connect to **business intelligence** tools

### Advanced Features

- Add **machine learning** for anomaly detection
- Implement **real-time alerting** for critical events
- Configure **data lineage** tracking for compliance
- Set up **cost optimization** automation

### Operational Excellence

- Implement **automated testing** for pipeline changes
- Set up **CI/CD** for configuration management  
- Configure **capacity planning** and auto-scaling
- Establish **incident response** procedures

<div style={{display: 'flex', gap: '1.5rem', marginTop: '2rem', marginBottom: '3rem', flexWrap: 'wrap', justifyContent: 'flex-start'}}>
  <a href="./troubleshooting" className="button button--primary button--lg" style={{display: 'inline-flex', alignItems: 'center', justifyContent: 'center', textDecoration: 'none', borderRadius: '8px', padding: '1rem 2rem', fontWeight: '600', minWidth: '240px', boxShadow: '0 2px 8px rgba(0,0,0,0.15)', cursor: 'pointer', transition: 'all 0.2s ease'}}>
    Troubleshooting Guide
  </a>
  <a href="../filter-severity" className="button button--secondary button--lg" style={{display: 'inline-flex', alignItems: 'center', justifyContent: 'center', textDecoration: 'none', borderRadius: '8px', padding: '1rem 2rem', fontWeight: '600', minWidth: '240px', boxShadow: '0 2px 8px rgba(0,0,0,0.15)', cursor: 'pointer', transition: 'all 0.2s ease'}}>
    Explore Related Examples
  </a>
</div>

---

**Related:** [Troubleshooting](./troubleshooting) for common issues and solutions

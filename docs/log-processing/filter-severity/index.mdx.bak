---
title: Log Severity Filtering
sidebar_label: Introduction
sidebar_position: 1
description: Filter application logs by severity level, routing errors to files and warnings to stdout
keywords: [log-processing, severity-filtering, error-routing, log-analysis, monitoring]
---

# Log Severity Filtering

**Transform noisy application logs into actionable intelligence by filtering on severity and routing critical errors to dedicated files**. This step-by-step guide teaches you 3 essential log processing techniques through hands-on exercises.

## The Problem

Application logs contain massive amounts of information, but most entries are informational noise. You need to focus on errors and warnings while ensuring critical errors are captured for investigation.

```json
{"timestamp":"2024-01-15T10:30:00Z","level":"INFO","message":"User logged in","user_id":"12345"}     // ❌ Noise
{"timestamp":"2024-01-15T10:30:01Z","level":"DEBUG","message":"Database query took 5ms"}            // ❌ Noise  
{"timestamp":"2024-01-15T10:30:02Z","level":"ERROR","message":"Payment failed","error":"timeout"}   // ✅ Critical
{"timestamp":"2024-01-15T10:30:03Z","level":"WARN","message":"High memory usage","usage":"85%"}     // ✅ Important
```

**The challenge:** Extract actionable insights from high-volume logs while ensuring critical errors are never lost.

## The Solution: 3 Log Processing Techniques

This guide teaches you how to apply the right technique for each log processing challenge:

### 1. **JSON Parsing with Fallback** → Mixed Log Formats
Handle both structured JSON and plain text logs gracefully.
- **Use case:** Applications that output mixed log formats
- **Method:** Parse JSON where possible, preserve raw text when not
- **Result:** Uniform processing pipeline for heterogeneous log sources

### 2. **Severity-Based Filtering** → Noise Reduction  
Eliminate low-priority log entries to focus on actionable events.
- **Use case:** High-volume applications with DEBUG/INFO noise
- **Method:** Filter pipeline to only ERROR and WARN levels
- **Result:** 80-95% reduction in log volume with no loss of critical data

### 3. **Conditional Routing** → Error Segregation
Route different severity levels to appropriate destinations.
- **Use case:** Separate critical error investigation from general monitoring
- **Method:** Switch output based on severity level
- **Result:** Errors in dedicated files, warnings to monitoring systems

## Why Process at the Edge?

**Regulatory Compliance:** Keep sensitive error logs within controlled environments (GDPR, HIPAA, SOX)  
**Cost Optimization:** Filter out 80-95% of log volume before expensive storage/transmission  
**Latency Reduction:** Real-time alerting on critical errors without central processing delays  
**Security:** Error logs containing sensitive data never leave the secure perimeter

## What You'll Learn

By the end of this guide, you'll be able to:

✅ **Parse mixed log formats** with fallback handling for robust data ingestion  
✅ **Filter by severity** to eliminate noise and focus on actionable events  
✅ **Route conditionally** to send errors and warnings to appropriate destinations  
✅ **Monitor pipeline health** with processing metadata and error tracking  
✅ **Handle production scenarios** including high volume, file rotation, and compliance

## Get Started

### Option 1: Step-by-Step Tutorial (Recommended)
**Build** the log filtering pipeline incrementally, one concept at a time.

1. [**Setup Guide**](./setup) - Environment configuration and sample log deployment
2. [**Step 1: Parse JSON & Add Metadata**](./step-1-parse-json-add-metadata) - Handle mixed formats with processing tracking
3. [**Step 2: Filter by Severity**](./step-2-filter-by-severity) - Eliminate noise with level-based filtering  
4. [**Step 3: Route by Severity**](./step-3-route-by-severity) - Direct errors and warnings to appropriate outputs

### Option 2: Jump to Complete Pipeline
**Download** the complete, production-ready log filtering solution.

[**→ Get Complete Pipeline**](./complete-filter-severity)

## Who This Guide Is For

- **DevOps Engineers** monitoring application health and investigating errors
- **Security Teams** needing to segregate and analyze error logs for threats  
- **Compliance Officers** ensuring log processing meets regulatory requirements
- **Site Reliability Engineers** building robust log processing infrastructure

## Prerequisites

- Expanso edge data processing platform installed ([Installation Guide](https://docs.expanso.io/install))
- Sample application logs available for testing
- Basic familiarity with YAML configuration and log formats

## Time to Complete

- **Step-by-Step Tutorial:** 20-30 minutes
- **Quick Deploy:** 5 minutes

## Real-World Impact

**Before Log Filtering:**
```
- Log volume: 10GB/day
- Storage cost: $300/month  
- Error detection time: 15-30 minutes
- Compliance audit prep: 8 hours
```

**After Log Filtering:**
```
- Log volume: 800MB/day (92% reduction)
- Storage cost: $25/month (91% savings)
- Error detection time: <1 minute (real-time)
- Compliance audit prep: 30 minutes (pre-segregated errors)
```

---

## Next Steps

Ready to start? Choose your learning path:

<div style={{display: 'flex', gap: '1rem', marginTop: '2rem', marginBottom: '2rem', flexWrap: 'wrap'}}>
  <a href="./setup" className="button button--primary button--lg" style={{flex: '1', minWidth: '200px'}}>
    Step-by-Step Tutorial
  </a>
  <a href="./complete-filter-severity" className="button button--secondary button--lg" style={{flex: '1', minWidth: '200px'}}>
    Download Complete Pipeline
  </a>
</div>

**Questions?** Check [Troubleshooting](./troubleshooting) or see [Related Examples](#related-examples) below.

## Related Examples

- [**Production Pipeline**](../production-pipeline) - Complete production log processing with monitoring and alerting
- [**Parse Logs**](../../data-transformation/parse-logs) - Advanced parsing techniques for complex log formats
- [**Enrich Export**](../enrich-export) - Add contextual data and export to analytics platforms

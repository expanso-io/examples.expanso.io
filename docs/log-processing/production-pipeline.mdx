---
title: Build a Log Processing Pipeline
sidebar_label: Log Processing
sidebar_position: 2
description: Build a real-world pipeline that processes application logs
keywords: [tutorial, logs, parsing, elasticsearch, s3, fan-out]
---

import CodeBlock from '@theme/CodeBlock';
import pipelineYaml from '!!raw-loader!../../examples/log-processing/production-pipeline.yaml';


# Build a Log Processing Pipeline

In this tutorial, you'll build a production-ready log processing pipeline that ingests application logs, parses and enriches them, filters out noise, and sends them to multiple destinations. By the end, you'll have a working system that routes logs to Elasticsearch for real-time search and S3 for long-term storage.

This is a realistic example of how companies handle log aggregation at the edgeâ€”processing data close to where it's generated before sending it to centralized systems.

This tutorial takes about 25-30 minutes to complete.

## What You'll Build

You'll create a pipeline that:

1. Receives JSON-formatted application logs via HTTP
2. Parses and validates the log structure
3. Enriches logs with metadata (node location, pipeline ID, timestamps)
4. Filters out debug and trace logs to reduce noise
5. Routes production logs to two destinations:
   - **Elasticsearch** for real-time search and dashboarding
   - **S3** for long-term archive and compliance

Here's what the data flow looks like:

```mermaid
graph LR
    App[Application] -->|HTTP POST| Input[HTTP Input]
    Input --> Parse[Parse JSON]
    Parse --> Enrich[Add Metadata]
    Enrich --> Filter[Filter Debug Logs]
    Filter --> FanOut{Fan-Out}
    FanOut -->|Production Logs| ES[Elasticsearch]
    FanOut -->|All Logs| S3[S3 Archive]
```

## What You'll Learn

- How to accept logs via HTTP endpoints
- How to parse and validate JSON log structures
- How to enrich logs with contextual metadata
- How to filter logs based on severity levels
- How to send data to multiple destinations (fan-out pattern)
- How to configure batching for efficient storage
- How to handle errors and validate your pipeline

## Prerequisites

Before starting, you'll need:

- **Expanso installed and running** - Complete [Getting Started](https://docs.expanso.io/getting-started) first
- **An edge node connected** to your orchestrator
- **Basic familiarity with JSON** log formats
- **AWS credentials configured** (for S3) or you can use local file storage instead
- **Elasticsearch instance** (optional - we'll show file-based alternatives)

:::tip Don't Have Elasticsearch or S3?
You can complete this tutorial using local file outputs instead. We'll show you both approaches.
:::

## Step 1: Understand the Log Format

Let's start by looking at the application logs you'll be processing. These are typical structured JSON logs from a web application:

```json
{
  "timestamp": "2025-10-20T10:30:45.123Z",
  "level": "INFO",
  "service": "api-gateway",
  "message": "Request processed successfully",
  "request_id": "req_abc123",
  "user_id": "user_456",
  "endpoint": "/api/v1/users",
  "method": "GET",
  "status_code": 200,
  "duration_ms": 45,
  "ip_address": "192.168.1.100"
}
```

**Log Levels:**
- `DEBUG` - Detailed diagnostic information (noisy, filtered in production)
- `INFO` - Informational messages about normal operations
- `WARN` - Warning messages about potential issues
- `ERROR` - Error messages about failures
- `FATAL` - Critical failures that require immediate attention

## Step 2: Create the Pipeline Configuration

Let's build the pipeline step by step. Create a file called `log-pipeline.yaml`:

<CodeBlock language="yaml" title="log-pipeline.yaml" showLineNumbers>
  {pipelineYaml}
</CodeBlock>

<a
  href="/files/log-processing/production-pipeline.yaml"
  download
  className="button button--primary button--lg margin-top--md"
>
  ðŸ“¥ Download Pipeline
</a>

---


Let's break down what this pipeline does:

**Input Section:**
- Accepts HTTP POST requests on port 8080 at `/logs/ingest`
- Rate-limited to 1000 requests per second
- 10-second timeout per request

**Processing Pipeline:**
1. **Parse JSON** - Validates incoming logs are valid JSON
2. **Validate Fields** - Ensures required fields exist, throws error if missing
3. **Add Enrichment** - Adds node context, pipeline info, and timestamps
4. **Calculate Severity** - Assigns numeric scores to log levels
5. **Filter Debug Logs** - Removes DEBUG logs from the stream
6. **Redact Sensitive Data** - Hashes IP addresses and removes password fields
7. **Add Computed Fields** - Calculates hour, day of week, and latency categories

**Output Section:**
Uses the fan-out pattern to send logs to three destinations:
1. **Elasticsearch** - Real-time search (INFO and above only)
2. **S3** - Long-term archive (all logs, batched and compressed)
3. **Local File** - Backup copy (for safety)

## Step 3: Set Up Environment Variables

Before deploying, you'll need to configure credentials. Create an environment configuration:

```bash title="Set environment variables"
# Elasticsearch configuration
export ELASTICSEARCH_URL="http://your-elasticsearch:9200"

# AWS S3 configuration
export S3_BUCKET="your-company-logs"
export AWS_PROFILE="production"
export AWS_REGION="us-west-2"

# Node metadata (automatically set by Expanso edge)
export NODE_ID="edge-node-001"
export NODE_REGION="us-west-datacenter-1"
```

:::tip Using Local Files Instead?
If you don't have Elasticsearch or S3 set up yet, you can modify the pipeline to use local files for both outputs. Replace the `elasticsearch` and `aws_s3` outputs with additional `file` outputs pointing to different paths.
:::

## Step 4: Deploy the Pipeline

Now let's deploy the pipeline to your edge node:

```bash
expanso job deploy log-pipeline.yaml
```

You'll see output like:

```
âœ“ Job 'log-processing-pipeline' created
âœ“ Evaluation triggered
âœ“ Scheduling to matching nodes...
âœ“ Deployed to 2 nodes (edge-node-001, edge-node-002)
```

**Check the deployment status:**

```bash
expanso job status log-processing-pipeline
```

You should see:

```
Job: log-processing-pipeline
Status: running
Type: pipeline
Executions:
  - Node: edge-node-001
    State: running
    Since: 15 seconds ago
    Health: healthy
  - Node: edge-node-002
    State: running
    Since: 12 seconds ago
    Health: healthy
```

## Step 5: Send Test Logs

Let's send some test logs to verify the pipeline is working. We'll send logs at different severity levels:

```bash title="Send an INFO log"
curl -X POST http://localhost:8080/logs/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "timestamp": "2025-10-20T10:30:45.123Z",
    "level": "INFO",
    "service": "api-gateway",
    "message": "User login successful",
    "request_id": "req_001",
    "user_id": "user_123",
    "endpoint": "/auth/login",
    "method": "POST",
    "status_code": 200,
    "duration_ms": 45,
    "ip_address": "192.168.1.100"
  }'
```

```bash title="Send a WARNING log"
curl -X POST http://localhost:8080/logs/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "timestamp": "2025-10-20T10:31:12.456Z",
    "level": "WARN",
    "service": "api-gateway",
    "message": "Slow database query detected",
    "request_id": "req_002",
    "user_id": "user_456",
    "endpoint": "/api/v1/reports",
    "method": "GET",
    "status_code": 200,
    "duration_ms": 1850,
    "ip_address": "192.168.1.105"
  }'
```

```bash title="Send an ERROR log"
curl -X POST http://localhost:8080/logs/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "timestamp": "2025-10-20T10:32:30.789Z",
    "level": "ERROR",
    "service": "payment-service",
    "message": "Payment processing failed",
    "request_id": "req_003",
    "user_id": "user_789",
    "endpoint": "/api/v1/payments",
    "method": "POST",
    "status_code": 500,
    "duration_ms": 3200,
    "error_code": "PAYMENT_GATEWAY_TIMEOUT",
    "ip_address": "192.168.1.110"
  }'
```

```bash title="Send a DEBUG log (will be filtered)"
curl -X POST http://localhost:8080/logs/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "timestamp": "2025-10-20T10:33:00.000Z",
    "level": "DEBUG",
    "service": "api-gateway",
    "message": "Cache hit for user profile",
    "request_id": "req_004",
    "user_id": "user_123",
    "cache_key": "profile:user_123"
  }'
```

Each request should return `200 OK`.

## Step 6: Verify Elasticsearch Output

Check that logs made it to Elasticsearch (if you're using it):

```bash title="Query Elasticsearch for recent logs"
curl -X GET "http://localhost:9200/logs-*/_search?pretty" \
  -H "Content-Type: application/json" \
  -d '{
    "query": {
      "range": {
        "timestamp_normalized": {
          "gte": "now-1h"
        }
      }
    },
    "sort": [
      { "timestamp_normalized": "desc" }
    ],
    "size": 10
  }'
```

You should see your processed logs with all the enrichment metadata:

```json
{
  "hits": {
    "total": { "value": 3 },
    "hits": [
      {
        "_source": {
          "timestamp": "2025-10-20T10:32:30.789Z",
          "timestamp_normalized": "2025-10-20T10:32:30Z",
          "level": "ERROR",
          "service": "payment-service",
          "message": "Payment processing failed",
          "severity_score": 4,
          "is_high_priority": true,
          "metadata": {
            "node_id": "edge-node-001",
            "node_region": "us-west-datacenter-1",
            "pipeline_name": "log-processing-pipeline",
            "processed_at": "2025-10-20T10:32:30.850Z",
            "pipeline_version": "1.0"
          },
          "hour": "10",
          "day_of_week": "Tuesday",
          "latency_category": "very_slow"
        }
      }
    ]
  }
}
```

Notice:
- The DEBUG log was filtered out (only 3 hits, not 4)
- Each log has enrichment metadata
- Severity scores are calculated
- IP addresses are hashed
- Time-based fields are computed

## Step 7: Verify S3 Output

Check that logs are being archived to S3:

```bash title="List recent S3 objects"
aws s3 ls s3://your-company-logs/logs/ --recursive --human-readable
```

You should see compressed log files:

```
2025-10-20 10:35:00   2.1 MB logs/1729421700123456789/logs.jsonl.gz
```

**Download and inspect a log file:**

```bash
# Download the file
aws s3 cp s3://your-company-logs/logs/1729421700123456789/logs.jsonl.gz .

# Decompress and view
gunzip logs.jsonl.gz
cat logs.jsonl | jq .
```

You'll see all logs (including the filtered DEBUG log, because S3 gets everything):

```json
{"timestamp":"2025-10-20T10:30:45.123Z","level":"INFO",...,"archived_at":"2025-10-20T10:35:00.000Z"}
{"timestamp":"2025-10-20T10:31:12.456Z","level":"WARN",...,"archived_at":"2025-10-20T10:35:00.000Z"}
{"timestamp":"2025-10-20T10:32:30.789Z","level":"ERROR",...,"archived_at":"2025-10-20T10:35:00.000Z"}
```

Wait, where's the DEBUG log? Actually, our filter removes it before it reaches any output. Let's verify the local backup instead.

## Step 8: Check the Local Backup

View the local backup file on the edge node:

```bash
# If you have SSH access to the edge node
ssh edge-node-001

# View recent logs
tail -f /var/log/expanso/logs-*.jsonl | jq .
```

You should see the same processed logs. The DEBUG log won't appear here either because it was filtered out in the processing pipeline.

## Step 9: Test Error Handling

Let's test what happens when we send invalid logs:

```bash title="Send a log missing required fields"
curl -X POST http://localhost:8080/logs/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "level": "INFO",
    "message": "This log is missing timestamp and service"
  }'
```

You'll receive a `400 Bad Request` response. The pipeline rejected the log because it failed validation.

**Send malformed JSON:**

```bash title="Send invalid JSON"
curl -X POST http://localhost:8080/logs/ingest \
  -H "Content-Type: application/json" \
  -d 'This is not JSON'
```

Again, you'll get a `400 Bad Request`. The `json_documents` processor caught the error.

## Step 10: Monitor Pipeline Metrics

Check the pipeline's health metrics:

```bash title="View Prometheus metrics"
curl http://localhost:8080/metrics
```

You'll see metrics like:

```
# HELP pipeline_input_received Total messages received
pipeline_input_received{input="http_server",path="/logs/ingest"} 4

# HELP pipeline_processor_error_total Processor errors
pipeline_processor_error_total{processor="mapping"} 2

# HELP pipeline_output_sent_total Messages sent to output
pipeline_output_sent_total{output="elasticsearch"} 3
pipeline_output_sent_total{output="s3_archive"} 3
pipeline_output_sent_total{output="local_backup"} 3

# HELP pipeline_output_error_total Output errors
pipeline_output_error_total{output="elasticsearch"} 0
pipeline_output_error_total{output="s3_archive"} 0
```

**Key metrics to watch:**
- **pipeline_input_received** - Total logs received
- **pipeline_processor_error_total** - Failed validations/processing
- **pipeline_output_sent_total** - Successful deliveries to each destination
- **pipeline_output_error_total** - Failed deliveries (retry needed)

:::tip Monitor These Metrics
In production, you'll want to alert on:
- High processor error rates (validation failures)
- Output error rates (connectivity issues)
- Throughput drops (upstream problems)
:::

## Step 11: Scale the Pipeline

As your log volume grows, you can scale horizontally by deploying to more edge nodes:

```yaml title="Update the selector to target more nodes"
selector:
  match_labels:
    role: log-collector  # Remove region constraint
```

Re-deploy:

```bash
expanso job deploy log-pipeline.yaml
```

The pipeline will now run on all nodes labeled `role: log-collector`, regardless of region.

## Step 12: Update the Filter Logic

Let's say you want to also filter out very old logs (older than 7 days). Update the pipeline:

```yaml title="Add age filter in log-pipeline.yaml"
# Add this processor after the debug filter
- mapping: |
    root = this

    # Calculate log age
    let log_time = this.timestamp.parse_timestamp_strptime("%Y-%m-%dT%H:%M:%S%.fZ")
    let age_hours = (now().timestamp() - log_time.timestamp()) / 3600

    # Drop logs older than 7 days
    root = if age_hours > 168 {
      deleted()
    } else {
      this
    }
```

Re-deploy the updated pipeline:

```bash
expanso job deploy log-pipeline.yaml
```

You'll see:

```
âœ“ Job 'log-processing-pipeline' updated (v1 â†’ v2)
âœ“ Evaluation triggered
âœ“ Updating existing executions...
âœ“ Updated 2 executions
```

The pipeline automatically updates on all running nodes.

## What You Learned

Congratulations! You've built a production-ready log processing pipeline. Here's what you accomplished:

- âœ… Created an HTTP endpoint for log ingestion
- âœ… Parsed and validated JSON log structures
- âœ… Enriched logs with contextual metadata
- âœ… Calculated severity scores and derived fields
- âœ… Filtered out debug logs and old logs
- âœ… Redacted sensitive information (PII)
- âœ… Implemented the fan-out pattern for multiple destinations
- âœ… Configured batching and compression for efficiency
- âœ… Added error handling and validation
- âœ… Monitored pipeline health with metrics
- âœ… Scaled the pipeline across multiple nodes
- âœ… Updated the pipeline without downtime

## Key Patterns You've Learned

**Fan-Out Pattern:**
The ability to send processed data to multiple destinations is critical for real-world systems. You'll use this pattern when you need:
- Real-time search (Elasticsearch) + long-term storage (S3)
- Analytics (data warehouse) + monitoring (metrics system)
- Primary destination + backup/audit trail

**Enrichment Pattern:**
Adding metadata at the edge (node ID, region, timestamps) makes logs more valuable for debugging and analysis. The logs carry their processing context with them.

**Filter Pattern:**
Filtering at the edge reduces costs and noise:
- Less data transferred to cloud (bandwidth savings)
- Smaller Elasticsearch indices (cost savings)
- Cleaner data for analysts (time savings)

**Batching Pattern:**
Batching logs before sending to S3 reduces API calls and improves efficiency:
- 1000 logs per batch = 1 S3 PUT operation
- Without batching = 1000 S3 PUT operations
- Cost savings: ~99% reduction in S3 API costs

## Common Use Cases

You can adapt this pipeline for:

**Application Monitoring:**
- Collect logs from microservices
- Route errors to alerting systems
- Archive for debugging

**Security Analysis:**
- Collect access logs
- Filter suspicious patterns
- Send to SIEM systems

**Compliance Logging:**
- Collect audit trails
- Redact PII automatically
- Store immutably in S3

**IoT Data Collection:**
- Collect sensor readings (similar to logs)
- Filter noise/outliers
- Route to analytics and storage

## Next Steps

Now that you've built a log processing pipeline, here's where to go next:

**Learn Advanced Patterns:**
- [How to Remove PII from Data](/examples/data-security/remove-pii) - Advanced redaction techniques

**Optimize Your Pipeline:**

**Understand the Architecture:**

**API Reference:**

## Troubleshooting

**Logs not appearing in Elasticsearch:**

```bash
# Check if Elasticsearch is reachable
curl http://your-elasticsearch:9200/_cluster/health

# Check pipeline metrics for output errors
curl http://localhost:8080/metrics | grep pipeline_output_error

# View pipeline logs
expanso job logs log-processing-pipeline
```

**S3 uploads failing:**

```bash
# Verify AWS credentials
aws s3 ls s3://your-company-logs/

# Check if the bucket exists
aws s3 mb s3://your-company-logs/

# Verify IAM permissions (need s3:PutObject)
aws iam get-user

# Check pipeline logs for specific errors
expanso job logs log-processing-pipeline | grep -i s3
```

**Pipeline rejecting valid logs:**

```bash
# Check the validation logic
# View recent errors in pipeline logs
expanso job logs log-processing-pipeline --tail 100

# Test validation with a minimal log
curl -X POST http://localhost:8080/logs/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "timestamp": "2025-10-20T10:00:00.000Z",
    "level": "INFO",
    "service": "test",
    "message": "minimal test log"
  }'
```

**High memory usage:**

```bash
# Check batch sizes (reduce if too high)
# Current config uses:
# - Elasticsearch: 100 messages per batch
# - S3: 1000 messages or 10MB per batch

# Reduce batch sizes in the pipeline config:
# batching:
#   count: 50        # Reduce from 100
#   period: 5s

# Re-deploy the updated pipeline
expanso job deploy log-pipeline.yaml
```

**Logs delayed in appearing:**

This is expected! Batching introduces latency:
- Elasticsearch: Up to 5 seconds (or 100 messages)
- S3: Up to 1 minute (or 1000 messages)
- Local backup: Up to 10 seconds (or 100 messages)

To reduce latency, decrease batch periods:

```yaml
batching:
  count: 100
  period: 1s  # Reduce from 5s
```

Trade-off: Lower latency = higher API costs and overhead.

**Pipeline not starting:**

```bash
# Check execution status
expanso job executions log-processing-pipeline

# View detailed execution logs
expanso job logs log-processing-pipeline

# Common issues:
# 1. Port 8080 already in use
# 2. Missing environment variables
# 3. Invalid YAML syntax
# 4. Node selector doesn't match any nodes

# Verify node labels
expanso node list
expanso node describe <node-id>
```

**Need More Help?**

- Search [GitHub Issues](https://github.com/expanso-io/expanso/issues)
- Join our [Discord Community](https://discord.gg/expanso)

---

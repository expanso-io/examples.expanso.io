---
title: Troubleshooting Production Log Processing Pipeline
sidebar_label: Troubleshooting
sidebar_position: 10
description: Comprehensive troubleshooting guide for production log processing pipeline issues including HTTP input, validation, enrichment, filtering, redaction, and fan-out problems
keywords: [troubleshooting, debugging, errors, performance, monitoring, diagnostics]
---

# Troubleshooting Production Log Processing Pipeline

This comprehensive guide helps you diagnose and resolve issues with your production log processing pipeline. Issues are organized by pipeline stage and severity level.

## Quick Diagnostic Commands

Before diving into specific issues, run these commands to get an overview of pipeline health:

```bash
# Check overall pipeline status
expanso job status log-processing-production-complete

# Check recent logs for errors
expanso job logs log-processing-production-complete --tail 100 | grep -i error

# Check HTTP endpoint health
curl http://localhost:8080/health

# View processing metrics
curl http://localhost:8080/metrics | grep -E "(error|failed|success)"

# Check resource usage
expanso node describe $(expanso node list -o name | head -1)
```

---

## HTTP Input Issues

### Issue: Logs Not Being Received

**Symptom:** HTTP requests return errors or no logs appear in outputs

**Diagnosis:**
```bash
# Test endpoint manually
curl -v http://localhost:8080/logs/ingest \
  -H "Content-Type: application/json" \
  -H "X-API-Key: $LOG_API_KEY" \
  -d '{"test": "message"}'

# Check if port is open
netstat -tlnp | grep :8080

# Check rate limiting
curl http://localhost:8080/metrics | grep rate_limit
```

**Common Causes & Solutions:**

**1. Authentication Failures**
```bash
# Check API key
echo "Current API key: $LOG_API_KEY"
echo "Expected format: base64 string, 32+ characters"

# Test with explicit key
curl -X POST http://localhost:8080/logs/ingest \
  -H "Content-Type: application/json" \
  -H "X-API-Key: your-actual-api-key" \
  -d '{"test": "auth test"}'
```

**2. Port Binding Issues**
```bash
# Check what's using port 8080
lsof -i :8080

# If port in use, kill process or change port
export LOG_PROCESSING_PORT=8081
expanso job deploy production-pipeline-complete.yaml
```

**3. Pipeline Not Running**
```bash
# Check deployment status
expanso job executions log-processing-production-complete

# If not running, check logs for startup errors
expanso job logs log-processing-production-complete | head -50

# Common startup issues:
# - Invalid YAML syntax
# - Missing environment variables  
# - Node selector doesn't match any nodes
```

### Issue: High Latency on HTTP Requests

**Symptom:** Requests take >100ms to complete

**Diagnosis:**
```bash
# Check request duration metrics
curl http://localhost:8080/metrics | grep http_request_duration

# Test with simple payload
time curl -X POST http://localhost:8080/logs/ingest \
  -H "Content-Type: application/json" \
  -H "X-API-Key: $LOG_API_KEY" \
  -d '{"level":"INFO","message":"latency test"}'
```

**Solutions:**

**1. Reduce Processing Complexity**
```yaml
# Simplify pipeline for debugging
pipeline:
  processors:
    - json_documents: {}
    - mapping: |
        root = {
          "timestamp": now(),
          "level": this.level,
          "message": this.message
        }
```

**2. Optimize Batching**
```yaml
# Reduce batch sizes for faster response
output:
  file:
    batching:
      count: 10      # Reduce from 50
      period: 1s     # Reduce from 10s
```

**3. Check System Resources**
```bash
# Monitor CPU and memory during requests
top -p $(pgrep expanso)

# Check disk I/O
iostat -x 1

# If high resource usage, consider:
# - Deploying to more nodes
# - Reducing processing complexity
# - Adding resource limits
```

### Issue: Rate Limiting Too Aggressive

**Symptom:** Legitimate requests getting 429 responses

**Diagnosis:**
```bash
# Check rate limit metrics
curl http://localhost:8080/metrics | grep rate

# Check current rate limit setting
expanso job get log-processing-production-complete -o yaml | grep rate_limit
```

**Solutions:**

**1. Increase Rate Limit**
```bash
# Temporarily increase limit
export RATE_LIMIT="5000/1s"
expanso job deploy production-pipeline-complete.yaml
```

**2. Implement Burst Allowance**
```yaml
input:
  http_server:
    rate_limit: "1000/1s"
    burst_limit: 200  # Allow bursts
```

**3. Use Per-Client Rate Limiting**
```yaml
# Rate limit by client IP
rate_limit_key: "${!meta(\"http_remote_addr\")}"
```

---

## Data Validation Issues

### Issue: Valid Logs Being Rejected

**Symptom:** 400 errors for seemingly valid JSON logs

**Diagnosis:**
```bash
# Check validation errors
expanso job logs log-processing-production-complete | grep -i validation

# Test with minimal valid log
curl -X POST http://localhost:8080/logs/ingest \
  -H "Content-Type: application/json" \
  -H "X-API-Key: $LOG_API_KEY" \
  -d '{
    "timestamp": "2025-10-20T10:30:45.123Z",
    "level": "INFO",
    "service": "test",
    "message": "minimal test"
  }'

# Check what fields are being validated
grep -A 20 "required_fields" production-pipeline-complete.yaml
```

**Solutions:**

**1. Review Required Fields**
```yaml
# Make fields optional temporarily
- mapping: |
    root = this
    root.timestamp = this.timestamp.or(now())
    root.level = this.level.or("INFO")
    root.service = this.service.or("unknown")
    root.message = this.message.or("No message provided")
```

**2. Add Field Defaults**
```yaml
# Provide better defaults
- mapping: |
    root = this
    
    # Default missing fields
    root.service = root.service.or(meta("http_user_agent").split("/")[0])
    root.level = root.level.uppercase().or("INFO")
```

**3. Debug Validation Logic**
```yaml
# Add debug output
- mapping: |
    # Log what's being validated
    print("Validating: " + this.string())
    root = this
```

### Issue: Timestamp Parsing Failures

**Symptom:** Logs show timestamp errors or use current time

**Diagnosis:**
```bash
# Check for timestamp errors
expanso job logs log-processing-production-complete | grep timestamp

# Test different timestamp formats
for ts in "2025-10-20T10:30:45Z" "2025-10-20 10:30:45" "1729421445"; do
  curl -X POST http://localhost:8080/logs/ingest \
    -H "Content-Type: application/json" \
    -H "X-API-Key: $LOG_API_KEY" \
    -d "{\"timestamp\":\"$ts\",\"level\":\"INFO\",\"service\":\"ts-test\",\"message\":\"timestamp test\"}"
done
```

**Solutions:**

**1. Add More Timestamp Formats**
```yaml
# Extend timestamp parsing
- mapping: |
    root.timestamp = match {
      # Add your specific format
      this.timestamp.parse_timestamp_strptime("%Y-%m-%d %H:%M:%S").catch(null) != null =>
        this.timestamp.parse_timestamp_strptime("%Y-%m-%d %H:%M:%S").format_timestamp("2006-01-02T15:04:05.000Z"),
      
      # More lenient parsing
      _ => now().format_timestamp("2006-01-02T15:04:05.000Z")
    }
```

**2. Use Lenient Timestamp Handling**
```yaml
# More forgiving timestamp processing
- mapping: |
    root.timestamp = this.timestamp.or(now()).string().
      parse_timestamp_strptime("%Y-%m-%dT%H:%M:%S%.fZ").
      catch(now()).
      format_timestamp("2006-01-02T15:04:05.000Z")
```

---

## Processing Performance Issues

### Issue: High Memory Usage

**Symptom:** Pipeline consuming excessive memory, potential OOM errors

**Diagnosis:**
```bash
# Check memory usage
ps aux | grep expanso
free -h

# Check pipeline metrics
curl http://localhost:8080/metrics | grep memory

# Monitor batch sizes
expanso job logs log-processing-production-complete | grep batch
```

**Solutions:**

**1. Reduce Batch Sizes**
```yaml
# Smaller batches use less memory
output:
  elasticsearch:
    batching:
      count: 50      # Reduce from 100
      byte_size: 2MB # Reduce from 5MB
```

**2. Optimize Object Processing**
```yaml
# Process fields individually to reduce memory
- mapping: |
    # Instead of copying entire object
    root = {
      "timestamp": this.timestamp,
      "level": this.level,
      "service": this.service,
      "message": this.message
      # Add only needed fields
    }
```

**3. Use Streaming Processing**
```yaml
# Process in smaller chunks
processors:
  - archive:
      format: json_array
  - mapping: |
      # Process in batches of 10
      root = this.chunk(10).map_each(chunk -> process_chunk(chunk)).flatten()
  - unarchive:
      format: json_array
```

### Issue: Slow Processing Throughput

**Symptom:** Logs backing up, increased latency

**Diagnosis:**
```bash
# Check processing rates
curl http://localhost:8080/metrics | grep "rate\|throughput"

# Monitor queue depths
expanso job logs log-processing-production-complete | grep queue

# Check CPU usage
top -p $(pgrep expanso)
```

**Solutions:**

**1. Optimize Heavy Operations**
```yaml
# Conditional processing
- mapping: |
    # Skip expensive operations for debug logs
    root = if this.level == "DEBUG" {
      this.without("complex_field_processing")
    } else {
      apply_full_processing(this)
    }
```

**2. Parallelize Processing**
```yaml
# Use multiple parallel processors
pipeline:
  processors:
    - parallel:
        cap: 4  # Process 4 messages in parallel
        processors:
          - mapping: | 
              # Your processing logic
```

**3. Scale Horizontally**
```bash
# Deploy to more nodes
expanso node label additional-node role=log-collector

# Pipeline will automatically deploy to new nodes
```

---

## Enrichment Issues

### Issue: Missing Environment Variables

**Symptom:** Metadata fields showing "unknown" values

**Diagnosis:**
```bash
# Check environment variables
env | grep -E "(NODE_|PIPELINE_|ENVIRONMENT)"

# Check what pipeline sees
expanso job logs log-processing-production-complete | grep "env("
```

**Solutions:**

**1. Set Missing Variables**
```bash
# Set required environment variables
export NODE_ID="$(hostname)-$(date +%s)"
export NODE_REGION="us-west-2"
export ENVIRONMENT="production"

# Redeploy pipeline
expanso job deploy production-pipeline-complete.yaml
```

**2. Add Fallback Values**
```yaml
# Better fallback logic
- mapping: |
    root.metadata.node = {
      "id": env("NODE_ID").or(hostname()).or("unknown-" + uuid_v4().slice(0,8)),
      "region": env("NODE_REGION").or("unknown-region"),
      "environment": env("ENVIRONMENT").or("development")
    }
```

### Issue: Inconsistent Metadata

**Symptom:** Different logs have different metadata structures

**Diagnosis:**
```bash
# Check metadata consistency
curl -s output-files | jq '.metadata | keys' | sort | uniq -c
```

**Solutions:**

**1. Standardize Metadata Structure**
```yaml
# Always include all metadata fields
- mapping: |
    # Ensure consistent structure
    root.metadata = {
      "node": root.metadata.node.or({}),
      "pipeline": root.metadata.pipeline.or({}),
      "processing": root.metadata.processing.or({})
    }
```

---

## Filtering and Scoring Issues

### Issue: Important Logs Being Filtered

**Symptom:** Critical events not appearing in outputs

**Diagnosis:**
```bash
# Check filtering statistics
grep "filtered" /var/log/expanso/*-metrics-*.jsonl | tail -10

# Check what's being filtered
grep "filter_reason" /var/log/expanso/filtered-logs-*.jsonl | head -10
```

**Solutions:**

**1. Adjust Filtering Thresholds**
```bash
# Reduce minimum priority score
export MIN_PRIORITY_SCORE="2"  # Reduce from 3
expanso job deploy production-pipeline-complete.yaml
```

**2. Add Service Exceptions**
```yaml
# Never filter critical services
- mapping: |
    root.should_process = if ["payment-service", "auth-service"].contains(this.service) {
      true
    } else {
      root.should_process
    }
```

**3. Review Scoring Logic**
```yaml
# Add debug output for scoring
- mapping: |
    print("Service: " + this.service + ", Score: " + this.severity_score.string())
    root = this
```

### Issue: Too Much Noise in Outputs

**Symptom:** Storage costs high, too many low-priority logs

**Diagnosis:**
```bash
# Check priority distribution
grep "final_priority" /var/log/expanso/*-logs-*.jsonl | \
  cut -d: -f2 | jq -r .final_priority | sort | uniq -c
```

**Solutions:**

**1. Increase Filtering Aggressiveness**
```bash
export MIN_PRIORITY_SCORE="4"  # More aggressive
export FILTER_HEALTH_CHECKS="true"
export FILTER_CACHE_HITS="true"
```

**2. Add More Noise Patterns**
```yaml
# Filter additional noise patterns
- mapping: |
    let message = this.message.lowercase()
    root.should_process = if [
      message.contains("routine"),
      message.contains("scheduled"), 
      message.contains("keepalive"),
      message.contains("heartbeat")
    ].any(filter -> filter) {
      false
    } else {
      root.should_process
    }
```

---

## Privacy and Redaction Issues

### Issue: PII Not Being Detected

**Symptom:** Sensitive data passing through unredacted

**Diagnosis:**
```bash
# Check for unredacted PII
grep -E "(email|phone|ssn)" /var/log/expanso/*-logs-*.jsonl | \
  grep -v "REDACTED" | head -5

# Test PII detection manually
echo "test@example.com" | grep -E "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}"
```

**Solutions:**

**1. Improve Detection Patterns**
```yaml
# More comprehensive email pattern
let email_pattern = "[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)+"
```

**2. Add Field Name Detection**
```yaml
# Check field names in addition to content
let is_email_field = key.lowercase().contains("email") || 
                     key.lowercase().contains("mail") ||
                     key.lowercase().contains("user_name")
```

**3. Test Redaction Thoroughly**
```bash
# Test various PII formats
test_cases=(
  "john.doe@company.com"
  "555-123-4567" 
  "(555) 123-4567"
  "123-45-6789"
  "4532-1234-5678-9012"
)

for pii in "${test_cases[@]}"; do
  curl -X POST http://localhost:8080/logs/ingest \
    -H "Content-Type: application/json" \
    -H "X-API-Key: $LOG_API_KEY" \
    -d "{\"message\":\"Test with $pii\",\"test_field\":\"$pii\"}"
done
```

### Issue: Over-Redaction Removing Useful Data

**Symptom:** Important analytics data being redacted unnecessarily

**Solutions:**

**1. Use Pseudonymization Instead**
```yaml
# Hash instead of redact for analytics
{(key + "_hash"): value.hash("sha256").slice(0, 16)}
```

**2. Add Whitelist for Safe Values**
```yaml
# Don't redact test/demo data
root = if value.contains("example.com") || value.contains("test.") {
  {key: value}  # Keep test data
} else {
  apply_redaction(key, value)
}
```

---

## Fan-Out and Output Issues

### Issue: Elasticsearch Delivery Failures

**Symptom:** Logs not appearing in Elasticsearch

**Diagnosis:**
```bash
# Check Elasticsearch health
curl "${ELASTICSEARCH_URL}/_cluster/health"

# Check output metrics
curl http://localhost:8080/metrics | grep elasticsearch

# Check authentication
curl -u "${ELASTICSEARCH_USERNAME}:${ELASTICSEARCH_PASSWORD}" \
     "${ELASTICSEARCH_URL}/_cat/indices?v"
```

**Solutions:**

**1. Fix Authentication**
```bash
# Set credentials
export ELASTICSEARCH_USERNAME="your_username"
export ELASTICSEARCH_PASSWORD="your_password"

# Or use API key
export ELASTICSEARCH_API_KEY="your_api_key"
```

**2. Adjust Batch Sizes**
```yaml
elasticsearch:
  batching:
    count: 50      # Reduce batch size
    period: 10s    # More frequent sends
  timeout: 60s     # Longer timeout
```

**3. Add Fallback**
```yaml
elasticsearch:
  # Main config...
  fallback:
    file:
      path: "/var/log/expanso/elasticsearch-fallback.jsonl"
```

### Issue: S3 Upload Failures

**Symptom:** S3 metrics showing errors

**Diagnosis:**
```bash
# Test S3 access
aws s3 ls s3://$S3_BUCKET/

# Check IAM permissions
aws iam get-user

# Check specific error messages
expanso job logs log-processing-production-complete | grep -i s3
```

**Solutions:**

**1. Fix IAM Permissions**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow", 
      "Action": [
        "s3:PutObject",
        "s3:PutObjectAcl"
      ],
      "Resource": "arn:aws:s3:::your-bucket/*"
    }
  ]
}
```

**2. Use Fallback Storage**
```yaml
aws_s3:
  # Main config...
  fallback:
    file:
      path: "/var/log/expanso/s3-fallback.jsonl"
      compression: gzip
```

### Issue: Alert Webhook Timeouts

**Symptom:** Critical alerts not being delivered

**Diagnosis:**
```bash
# Test webhook manually
curl -X POST "$ALERT_WEBHOOK_URL" \
  -H "Content-Type: application/json" \
  -d '{"test": "manual alert"}'

# Check timeout metrics
curl http://localhost:8080/metrics | grep webhook
```

**Solutions:**

**1. Reduce Timeout**
```yaml
http:
  url: "${ALERT_WEBHOOK_URL}"
  timeout: 5s      # Reduce timeout
  max_retries: 5   # More retries
```

**2. Add Email Fallback**
```yaml
# If webhook fails, send email
fallback:
  smtp:
    host: "smtp.company.com"
    to: ["oncall@company.com"]
```

---

## General Performance Issues

### Issue: High CPU Usage

**Diagnosis:**
```bash
# Check CPU usage
top -p $(pgrep expanso)
htop

# Profile specific operations
perf top -p $(pgrep expanso)
```

**Solutions:**

**1. Reduce Processing Complexity**
```yaml
# Simplify regex patterns
let simple_email_check = value.contains("@") && value.contains(".")
if simple_email_check {
  # Apply full email regex only if basic check passes
}
```

**2. Use More Efficient Operations**
```yaml
# Use built-in functions instead of regex where possible
this.timestamp.parse_timestamp("RFC3339")  # Instead of strptime
```

### Issue: High Disk I/O

**Diagnosis:**
```bash
# Check I/O usage
iostat -x 1
iotop

# Check file sizes
du -sh /var/log/expanso/*
```

**Solutions:**

**1. Increase Batch Sizes**
```yaml
# Larger batches = fewer I/O operations
file:
  batching:
    count: 1000     # Increase batch size
    period: 60s     # Less frequent writes
```

**2. Use Compression**
```yaml
file:
  compression: gzip  # Reduce disk usage
```

---

## Emergency Procedures

### Pipeline Completely Down

```bash
# 1. Check if nodes are healthy
expanso node list

# 2. Check if pipeline is deployed
expanso job list | grep log-processing

# 3. Redeploy pipeline
expanso job deploy production-pipeline-complete.yaml

# 4. Check for errors
expanso job logs log-processing-production-complete

# 5. If still failing, deploy minimal pipeline
cat > emergency-pipeline.yaml << EOF
name: emergency-log-processing
type: pipeline
config:
  input:
    http_server:
      address: "0.0.0.0:8080"
      path: /logs/ingest
  pipeline:
    processors:
      - json_documents: {}
  output:
    file:
      path: "/var/log/expanso/emergency-logs.jsonl"
EOF

expanso job deploy emergency-pipeline.yaml
```

### Data Loss Prevention

```bash
# 1. Immediately backup current logs
tar -czf emergency-backup-$(date +%s).tar.gz /var/log/expanso/

# 2. Stop pipeline to prevent further loss
expanso job delete log-processing-production-complete

# 3. Assess data loss scope
find /var/log/expanso/ -name "*.jsonl" -newer emergency-backup*.tar.gz

# 4. Implement recovery pipeline with local backup only
# (Remove external dependencies that might be failing)
```

### Security Incident Response

```bash
# 1. Immediately rotate API keys
export LOG_API_KEY="$(openssl rand -base64 32)"

# 2. Deploy with new key
expanso job deploy production-pipeline-complete.yaml

# 3. Audit recent access
grep "unauthorized\|failed.*auth" /var/log/expanso/*.log

# 4. Check for data exfiltration
grep -E "(REDACTED|sensitive)" /var/log/expanso/*.jsonl | \
  grep -v "***REDACTED***"  # Look for unredacted sensitive data
```

---

## Prevention and Monitoring

### Set Up Proactive Monitoring

```bash
# Create monitoring script
cat > monitor-pipeline.sh << 'EOF'
#!/bin/bash

# Check pipeline health
if ! curl -f http://localhost:8080/health >/dev/null 2>&1; then
  echo "ALERT: Pipeline health check failed" | mail -s "Pipeline Down" ops@company.com
fi

# Check error rates
error_rate=$(curl -s http://localhost:8080/metrics | grep http_requests_total | grep "5[0-9][0-9]" | awk '{sum+=$2} END {print sum+0}')
if [ "$error_rate" -gt 10 ]; then
  echo "ALERT: High error rate: $error_rate" | mail -s "High Errors" ops@company.com  
fi

# Check disk usage
disk_usage=$(df /var/log/expanso | tail -1 | awk '{print $5}' | sed 's/%//')
if [ "$disk_usage" -gt 80 ]; then
  echo "ALERT: High disk usage: $disk_usage%" | mail -s "Disk Full" ops@company.com
fi
EOF

chmod +x monitor-pipeline.sh

# Run every 5 minutes
echo "*/5 * * * * /path/to/monitor-pipeline.sh" | crontab -
```

### Regular Health Checks

```bash
# Create comprehensive health check
cat > health-check.sh << 'EOF'
#!/bin/bash

echo "=== Pipeline Health Check $(date) ==="

# 1. Pipeline status
echo "Pipeline Status:"
expanso job status log-processing-production-complete

# 2. HTTP endpoint
echo -e "\nHTTP Endpoint:"
curl -w "Response time: %{time_total}s\n" -o /dev/null -s http://localhost:8080/health

# 3. Processing metrics
echo -e "\nProcessing Metrics:"
curl -s http://localhost:8080/metrics | grep -E "(requests_total|errors_total|duration)"

# 4. Output health
echo -e "\nOutput Health:"
if [ ! -z "$ELASTICSEARCH_URL" ]; then
  curl -s "${ELASTICSEARCH_URL}/_cluster/health" | jq .status
fi

# 5. Disk usage
echo -e "\nDisk Usage:"
du -sh /var/log/expanso/

# 6. Recent errors
echo -e "\nRecent Errors:"
expanso job logs log-processing-production-complete --tail 20 | grep -i error || echo "No recent errors"

echo "=== Health Check Complete ==="
EOF

chmod +x health-check.sh
```

## Getting Additional Help

If you're still experiencing issues after trying these troubleshooting steps:

1. **Collect Diagnostic Information:**
   - Pipeline logs: `expanso job logs log-processing-production-complete`
   - System metrics: `curl http://localhost:8080/metrics`
   - Node status: `expanso node describe <node-id>`
   - Configuration: `expanso job get log-processing-production-complete -o yaml`

2. **Community Support:**
   - [GitHub Issues](https://github.com/expanso-io/expanso/issues)
   - [Discord Community](https://discord.gg/expanso)
   - [Documentation](https://docs.expanso.io)

3. **Enterprise Support:**
   - Submit ticket with diagnostic information
   - Include error messages and steps to reproduce
   - Specify urgency level and business impact

Remember: Always backup your data and configuration before making significant changes to troubleshoot issues.

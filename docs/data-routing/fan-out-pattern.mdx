---
title: How to Send Data to Multiple Destinations
sidebar_label: Fan-Out Pattern
sidebar_position: 2
description: Send the same data to Kafka, S3, and Elasticsearch simultaneously
keywords: [fan-out, broker, multiple outputs, kafka, s3, elasticsearch]
---

import CodeBlock from '@theme/CodeBlock';
import pipelineYaml from '!!raw-loader!../../examples/data-routing/fan-out-pattern.yaml';


# How to Send Data to Multiple Destinations

When processing data at the edge, you often need to send the same information to multiple destinations simultaneously. For example, you might want to stream real-time events to Kafka for processing, archive data to S3 for compliance, and index logs in Elasticsearch for search - all from the same pipeline.

The broker output with fan-out pattern enables this by sending each message to multiple outputs concurrently. Each destination operates independently with its own retry logic, batching configuration, and error handling. If one destination fails, the others continue processing without interruption.

## Problem Statement

Consider a sensor data pipeline at the edge. Your requirements might include:

- **Real-time processing**: Stream data to Kafka for immediate analysis
- **Long-term storage**: Archive all raw data to S3 for regulatory compliance
- **Search capability**: Index processed data in Elasticsearch for operational queries
- **Local backup**: Write to local files when cloud services are unreachable

Without the fan-out pattern, you would need to duplicate your pipeline configuration multiple times or implement complex custom logic to handle multiple destinations. The broker output solves this by providing a single, declarative way to send data to any combination of destinations.

## Prerequisites

Before starting, ensure you have:

- An Expanso edge node running and connected to the orchestrator
- Access credentials for your destination services (Kafka, S3, Elasticsearch)
- Network connectivity from edge nodes to cloud services

## Solution Overview

The broker output uses a fan-out pattern to distribute messages to multiple destinations:

```
         â”Œâ”€â†’ Kafka (real-time)
Input â”€â”€â”€â”¼â”€â†’ S3 (archive)
         â””â”€â†’ Elasticsearch (search)
```

Key characteristics of the fan-out pattern:

- **Concurrent delivery**: Each output receives messages in parallel, not sequentially
- **Independent failure handling**: If Elasticsearch is down, Kafka and S3 continue processing
- **Per-output configuration**: Each destination has its own batching, compression, and retry settings
- **No message duplication overhead**: The broker efficiently distributes the same message to all outputs

## Step-by-Step Implementation

### Step 1: Configure the Broker Output

Create a new pipeline configuration file called `multi-destination-pipeline.yaml`:

<CodeBlock language="yaml" title="multi-destination-pipeline.yaml" showLineNumbers>
  {pipelineYaml}
</CodeBlock>

<a
  href="/files/data-routing/fan-out-pattern.yaml"
  download
  className="button button--primary button--lg margin-top--md"
>
  ðŸ“¥ Download Pipeline
</a>

---


This sets up an HTTP server to receive sensor data. Now add the broker output with fan-out pattern:

```yaml title="multi-destination-pipeline.yaml (continued)"
  output:
    broker:
      pattern: fan_out
      outputs:
        # Outputs will be added in the following steps
```

The `pattern: fan_out` configuration tells the broker to send each message to all outputs concurrently.

### Step 2: Add Kafka for Real-Time Processing

Add the Kafka output for real-time event streaming:

```yaml title="multi-destination-pipeline.yaml (add to outputs)"
        - kafka:
            addresses:
              - kafka-1.example.com:9092
              - kafka-2.example.com:9092
              - kafka-3.example.com:9092
            topic: sensor-events

            # Kafka-specific batching for throughput
            batching:
              count: 100
              period: 1s

            # Compression to reduce network usage
            compression: snappy

            # Idempotent writes for exactly-once semantics
            idempotent_write: true

            # Max time to wait for acknowledgment
            ack_replicas: true
            max_in_flight: 1

            # Authentication
            sasl:
              mechanism: SCRAM-SHA-512
              user: ${KAFKA_USERNAME}
              password: ${KAFKA_PASSWORD}

            # TLS for encryption in transit
            tls:
              enabled: true
              skip_cert_verify: false
```

This configuration:

- Uses multiple Kafka brokers for high availability
- Batches up to 100 messages or 1 second (whichever comes first)
- Compresses data with Snappy for efficient network transfer
- Enables idempotent writes to prevent duplicate messages
- Requires acknowledgment from all replicas before confirming delivery

### Step 3: Add S3 for Long-Term Archive

Add the S3 output for compliance and long-term storage:

```yaml title="multi-destination-pipeline.yaml (add to outputs)"
        - aws_s3:
            bucket: sensor-data-archive

            # Organize by date and hour
            path: raw-events/${!timestamp_unix_date("2006/01/02")}/${!timestamp_unix_hour()}/events_${!count("files")}.jsonl

            # Larger batches for S3 efficiency
            batching:
              count: 1000
              period: 5m

            # Compress before upload to save storage costs
            content_type: application/x-ndjson
            content_encoding: gzip

            # S3 storage class for cost optimization
            storage_class: INTELLIGENT_TIERING

            # Server-side encryption
            server_side_encryption: AES256

            # AWS credentials from environment
            credentials:
              id: ${AWS_ACCESS_KEY_ID}
              secret: ${AWS_SECRET_ACCESS_KEY}

            # Region configuration
            region: us-west-2

            # Upload timeout for large batches
            timeout: 60s
```

This configuration:

- Creates daily and hourly partitioned paths for efficient querying
- Uses larger batches (1000 messages or 5 minutes) to reduce S3 API costs
- Compresses with gzip to minimize storage costs
- Uses Intelligent Tiering for automatic cost optimization
- Enables server-side encryption for data at rest

### Step 4: Add Elasticsearch for Search

Add the Elasticsearch output for operational search and analytics:

```yaml title="multi-destination-pipeline.yaml (add to outputs)"
        - elasticsearch:
            urls:
              - https://es-1.example.com:9200
              - https://es-2.example.com:9200
              - https://es-3.example.com:9200

            # Use date-based indices for easier management
            index: sensor-events-${!timestamp_unix_date("2006-01-02")}

            # Document ID from message field to enable updates
            id: ${!json("event_id")}

            # Moderate batching for search responsiveness
            batching:
              count: 200
              period: 10s

            # Snappy compression for transport
            sniff: false
            gzip_compression: true

            # Authentication
            basic_auth:
              enabled: true
              username: ${ES_USERNAME}
              password: ${ES_PASSWORD}

            # TLS configuration
            tls:
              enabled: true
              skip_cert_verify: false

            # Retry configuration for transient failures
            max_retries: 3
            backoff:
              initial_interval: 1s
              max_interval: 30s
```

This configuration:

- Creates daily indices for easier retention management
- Uses moderate batching (200 messages or 10 seconds) for search freshness
- Enables gzip compression to reduce network bandwidth
- Configures retry logic with exponential backoff for resilience

### Step 5: Configure Independent Error Handling

Each output can have different error handling strategies. Add a local file fallback for when cloud services are unavailable:

```yaml title="multi-destination-pipeline.yaml (add to outputs)"
        - fallback:
            - kafka: # Same Kafka config from Step 2
                # ... (config omitted for brevity)

            # If Kafka fails, write to local buffer
            - file:
                path: /var/expanso/buffers/kafka-fallback-${!count("files")}.jsonl
                codec: lines
```

This creates a fallback chain: attempt Kafka first, and if it fails after all retries, write to a local file buffer. You can apply this pattern to any output.

### Step 6: Add Processing and Enrichment

Before the broker output, add processing to enrich data with edge context:

```yaml title="multi-destination-pipeline.yaml (add before output)"
  pipeline:
    processors:
      # Add edge node metadata
      - mapping: |
          root = this
          root.edge_node_id = env("NODE_ID").or("unknown")
          root.edge_location = env("NODE_LOCATION").or("unknown")
          root.processing_timestamp = now()

      # Validate required fields
      - mapping: |
          # Ensure critical fields exist
          if !this.exists("event_id") {
            throw("missing required field: event_id")
          }
          if !this.exists("sensor_id") {
            throw("missing required field: sensor_id")
          }
          root = this

      # Add message routing metadata
      - mapping: |
          root = this
          root.destinations = {
            "kafka": true,
            "s3": true,
            "elasticsearch": true
          }
```

This processing:

- Enriches each message with edge node context
- Validates that critical fields are present
- Adds metadata about which destinations will receive the message

## Complete Working Example

Here's the full pipeline configuration combining all steps:

```yaml title="complete-multi-destination-pipeline.yaml"
name: multi-destination-sensor-data
description: Send sensor data to Kafka, S3, and Elasticsearch with independent error handling
type: pipeline
namespace: production
labels:
  environment: production
  data-type: sensor-metrics
  pattern: fan-out

config:
  # Input: HTTP server receiving sensor events
  input:
    http_server:
      address: 0.0.0.0:8080
      path: /events
      timeout: 5s
      rate_limit: "1000/s"

  # Processing: Enrich and validate
  pipeline:
    processors:
      # Add edge metadata
      - mapping: |
          root = this
          root.edge_node_id = env("NODE_ID").or("unknown")
          root.edge_location = env("NODE_LOCATION").or("unknown")
          root.processing_timestamp = now()

      # Validate required fields
      - mapping: |
          if !this.exists("event_id") {
            throw("missing required field: event_id")
          }
          if !this.exists("sensor_id") {
            throw("missing required field: sensor_id")
          }
          root = this

  # Output: Broker with fan-out to multiple destinations
  output:
    broker:
      pattern: fan_out
      outputs:
        # 1. Kafka for real-time processing
        - kafka:
            addresses:
              - kafka-1.example.com:9092
              - kafka-2.example.com:9092
            topic: sensor-events
            batching:
              count: 100
              period: 1s
            compression: snappy
            idempotent_write: true
            ack_replicas: true
            sasl:
              mechanism: SCRAM-SHA-512
              user: ${KAFKA_USERNAME}
              password: ${KAFKA_PASSWORD}
            tls:
              enabled: true

        # 2. S3 for long-term archive
        - aws_s3:
            bucket: sensor-data-archive
            path: raw-events/${!timestamp_unix_date("2006/01/02")}/${!timestamp_unix_hour()}/events_${!count("files")}.jsonl
            batching:
              count: 1000
              period: 5m
            content_encoding: gzip
            storage_class: INTELLIGENT_TIERING
            server_side_encryption: AES256
            credentials:
              id: ${AWS_ACCESS_KEY_ID}
              secret: ${AWS_SECRET_ACCESS_KEY}
            region: us-west-2

        # 3. Elasticsearch for search
        - elasticsearch:
            urls:
              - https://es-1.example.com:9200
            index: sensor-events-${!timestamp_unix_date("2006-01-02")}
            id: ${!json("event_id")}
            batching:
              count: 200
              period: 10s
            gzip_compression: true
            basic_auth:
              enabled: true
              username: ${ES_USERNAME}
              password: ${ES_PASSWORD}
            tls:
              enabled: true
            max_retries: 3
            backoff:
              initial_interval: 1s
              max_interval: 30s
```

## Deploy the Pipeline

Deploy this configuration to your edge nodes:

```bash title="Deploy the pipeline"
# Deploy the pipeline
expanso job deploy multi-destination-pipeline.yaml

# Verify deployment status
expanso job status multi-destination-sensor-data
```

## Production Considerations

### Independent Retry Logic

Each output maintains its own retry state. Configure retry behavior based on destination characteristics:

**Kafka**: Fast retries with circuit breaker

```yaml
kafka:
  # ... other config
  max_retries: 3
  backoff:
    initial_interval: 100ms
    max_interval: 5s
```

**S3**: Patient retries for transient AWS issues

```yaml
aws_s3:
  # ... other config
  max_retries: 10
  timeout: 60s
```

**Elasticsearch**: Moderate retries with backpressure handling

```yaml
elasticsearch:
  # ... other config
  max_retries: 5
  backoff:
    initial_interval: 1s
    max_interval: 30s
```

### Batching and Compression Trade-offs

Different destinations benefit from different batching strategies:

**Real-time destinations (Kafka, Elasticsearch)**:
- Smaller batches (100-200 messages)
- Shorter time windows (1-10 seconds)
- Lower latency for fresh data

**Archive destinations (S3)**:
- Larger batches (1000+ messages)
- Longer time windows (5-15 minutes)
- Better compression ratios and reduced API costs

**Compression selection**:
- **Snappy**: Fast compression, good for Kafka (low CPU overhead)
- **Gzip**: Better compression ratio, good for S3 (storage cost savings)
- **None**: Use when data is pre-compressed or CPU is constrained

### Bandwidth Optimization

Edge nodes often have limited bandwidth. Optimize network usage:

1. **Enable compression** on all outputs that support it
2. **Increase batch sizes** to reduce protocol overhead
3. **Filter before fanning out** to avoid sending unnecessary data
4. **Use local buffering** to handle connectivity interruptions

Example bandwidth-optimized configuration:

```yaml
output:
  broker:
    pattern: fan_out
    outputs:
      - kafka:
          compression: snappy
          batching:
            count: 500
            period: 5s

      - aws_s3:
          content_encoding: gzip
          batching:
            count: 5000
            period: 10m
```

### Edge Context: Local and Cloud Destinations

Edge deployments often combine local and cloud destinations:

```yaml
output:
  broker:
    pattern: fan_out
    outputs:
      # Local file for immediate access
      - file:
          path: /var/expanso/data/local-events-${!timestamp_unix_date("2006-01-02")}.jsonl
          codec: lines

      # Cloud destinations for central processing
      - kafka:
          addresses: [kafka.cloud.example.com:9092]
          # ... kafka config

      - aws_s3:
          bucket: cloud-archive
          # ... s3 config
```

This pattern ensures:

- **Data availability during network outages**: Local files remain accessible
- **Automatic cloud sync when connected**: Cloud destinations receive data when reachable
- **No data loss**: Local buffering preserves data during connectivity issues

## Verification

### Test Message Flow

Send a test event to verify all destinations receive data:

```bash title="Send test event"
curl -X POST http://edge-node:8080/events \
  -H "Content-Type: application/json" \
  -d '{
    "event_id": "test-001",
    "sensor_id": "sensor-42",
    "timestamp": "2025-01-20T10:00:00Z",
    "temperature": 23.5,
    "humidity": 45.2
  }'
```

### Verify Each Destination

**Kafka**: Check that the event appears in the topic

```bash
kafka-console-consumer --bootstrap-server kafka-1.example.com:9092 \
  --topic sensor-events \
  --from-beginning \
  | grep "test-001"
```

**S3**: Verify the object exists

```bash
aws s3 ls s3://sensor-data-archive/raw-events/ --recursive \
  | grep "$(date +%Y/%m/%d)"
```

**Elasticsearch**: Query the index

```bash
curl -X GET "https://es-1.example.com:9200/sensor-events-$(date +%Y-%m-%d)/_search" \
  -u "${ES_USERNAME}:${ES_PASSWORD}" \
  -H "Content-Type: application/json" \
  -d '{"query": {"term": {"event_id": "test-001"}}}'
```

## Troubleshooting

### Problem: One Destination Failing Affects Others

**Symptom**: When Elasticsearch is slow, Kafka messages are also delayed.

**Solution**: The fan-out pattern sends to all outputs concurrently, so this should not happen. Check your configuration:

1. Verify `pattern: fan_out` is set (not `fan_out_sequential`)
2. Check that outputs don't share resources (e.g., same buffer pool)
3. Review error logs for resource contention

### Problem: High Memory Usage with Large Batches

**Symptom**: Edge node memory grows when S3 batches accumulate.

**Solution**: Configure per-output memory limits:

```yaml
output:
  broker:
    pattern: fan_out
    outputs:
      - aws_s3:
          batching:
            count: 1000
            period: 5m
          # Limit memory per batch
          max_in_flight: 2
```

### Problem: Messages Missing from S3 but Present in Kafka

**Symptom**: S3 shows gaps in data while Kafka has all messages.

**Solution**: Check S3-specific error logs and retry configuration. Common causes:

- **Credential expiration**: Rotate AWS credentials
- **Network timeout**: Increase `timeout` value in S3 config
- **Rate limiting**: Add backoff configuration

### Problem: Duplicate Messages in Elasticsearch

**Symptom**: Same event appears multiple times in Elasticsearch.

**Solution**: Ensure document IDs are set for idempotent writes:

```yaml
elasticsearch:
  # Use event_id as document ID for upsert behavior
  id: ${!json("event_id")}
```

## See Also

**Related Guides:**
- [Route Data Based on Content](/examples/data-routing/content-routing) - Conditional routing patterns
- [Implement Circuit Breakers](/examples/data-routing/circuit-breakers) - Protect downstream systems

**Component Reference:**
- [Broker Output](https://docs.expanso.io/components/outputs/broker) - Complete broker configuration
- [Kafka Output](https://docs.expanso.io/components/outputs/kafka) - Kafka options
- [AWS S3 Output](https://docs.expanso.io/components/outputs/aws_s3) - S3 options
- [Elasticsearch Output](https://docs.expanso.io/components/outputs/elasticsearch_v2) - Elasticsearch options

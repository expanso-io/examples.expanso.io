---
title: Setup Environment for Priority Queues
sidebar_label: Setup
sidebar_position: 3
description: Configure environment variables, Kafka brokers, and deploy a shell priority queue pipeline
keywords: [setup, environment, configuration, deployment, kafka, priority-queues]
---

# Setup Environment for Priority Queues

Before building the priority queue system, you'll set up your Kafka environment, configure routing credentials, and deploy a minimal test pipeline to verify everything works.

## Prerequisites

- ✅ Expanso edge node installed ([Installation Guide](https://docs.expanso.io/getting-started/installation))
- ✅ Kafka cluster available (local or cloud)
- ✅ Basic familiarity with message queuing concepts

## Step 1: Configure Environment Variables

Priority queues require Kafka broker addresses and optional authentication credentials.

```bash
# Set Kafka broker addresses
export KAFKA_BROKERS="localhost:9092"

# For cloud Kafka (Confluent, AWS MSK, etc.), use multiple brokers
# export KAFKA_BROKERS="broker1.kafka.cloud:9092,broker2.kafka.cloud:9092,broker3.kafka.cloud:9092"

# Optional: Set authentication (for cloud Kafka)
export KAFKA_USERNAME="your-username"
export KAFKA_PASSWORD="your-password"
export KAFKA_SECURITY_PROTOCOL="SASL_SSL"
export KAFKA_SASL_MECHANISM="PLAIN"

# Set edge node identifier for tracking
export NODE_ID="edge-node-01"

# Verify environment variables
echo "Kafka Brokers: $KAFKA_BROKERS"
echo "Node ID: $NODE_ID"
```

## Step 2: Create Kafka Topics

Priority queues require separate topics for each priority level to enable differential processing.

```bash
# For local Kafka (adjust kafka-topics.sh path as needed)
KAFKA_HOME="/opt/kafka"  # or /usr/local/kafka

# Create priority-specific topics
$KAFKA_HOME/bin/kafka-topics.sh --create \
    --bootstrap-server $KAFKA_BROKERS \
    --topic logs-critical \
    --partitions 3 \
    --replication-factor 1 \
    --config retention.ms=86400000  # 24 hours for critical

$KAFKA_HOME/bin/kafka-topics.sh --create \
    --bootstrap-server $KAFKA_BROKERS \
    --topic logs-high \
    --partitions 6 \
    --replication-factor 1 \
    --config retention.ms=259200000  # 72 hours for high priority

$KAFKA_HOME/bin/kafka-topics.sh --create \
    --bootstrap-server $KAFKA_BROKERS \
    --topic logs-normal \
    --partitions 12 \
    --replication-factor 1 \
    --config retention.ms=604800000  # 7 days for normal

$KAFKA_HOME/bin/kafka-topics.sh --create \
    --bootstrap-server $KAFKA_BROKERS \
    --topic logs-low \
    --partitions 6 \
    --replication-factor 1 \
    --config retention.ms=604800000  # 7 days for low priority

# Verify topics were created
$KAFKA_HOME/bin/kafka-topics.sh --list --bootstrap-server $KAFKA_BROKERS | grep logs-
```

**Expected output:**
```
logs-critical
logs-high
logs-low
logs-normal
```

## Step 3: Deploy Shell Priority Queue Pipeline

Before adding sophisticated priority logic, deploy a minimal "shell" pipeline that just routes all messages to a normal priority queue. This verifies your Kafka setup works.

Create `shell-priority-queues.yaml`:

```yaml title="shell-priority-queues.yaml"
name: shell-priority-queues
description: Basic pipeline to test Kafka connectivity
type: pipeline
namespace: test

config:
  input:
    http_server:
      address: 0.0.0.0:8080
      path: /logs
      timeout: 5s

  pipeline:
    processors:
      # Parse JSON and add basic metadata
      - json_documents:
          parts: []
      
      - mapping: |
          root = this
          root.received_at = now()
          root.edge_node_id = env("NODE_ID").or("unknown")
          root.pipeline = "shell-priority-queues"

  output:
    kafka:
      addresses: ["${KAFKA_BROKERS}"]
      topic: logs-normal
      batching:
        count: 10
        period: 5s
```

Deploy the shell pipeline:

```bash
# Save the YAML content above to shell-priority-queues.yaml

# Deploy to Expanso
expanso pipeline deploy shell-priority-queues.yaml

# Verify deployment
expanso pipeline list | grep shell-priority
```

**Expected output:**
```
shell-priority-queues    test    pipeline    running    edge-node-01
```

## Step 4: Test Shell Pipeline

Send test messages to verify the pipeline receives and routes messages to Kafka.

```bash
# Test with a simple log message
curl -X POST http://localhost:8080/logs \
  -H "Content-Type: application/json" \
  -d '{
    "timestamp": "2024-01-15T10:30:00Z",
    "severity": "INFO",
    "message": "Application started successfully",
    "service": "api-gateway",
    "version": "1.2.3"
  }'

# Send a batch of test messages
for severity in CRITICAL ERROR WARNING INFO DEBUG; do
  curl -X POST http://localhost:8080/logs \
    -H "Content-Type: application/json" \
    -d '{
      "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'",
      "severity": "'$severity'",
      "message": "Test message with severity '$severity'",
      "service": "test-service"
    }'
done
```

Check that messages are arriving in Kafka:

```bash
# Consumer from the normal priority topic
$KAFKA_HOME/bin/kafka-console-consumer.sh \
    --bootstrap-server $KAFKA_BROKERS \
    --topic logs-normal \
    --from-beginning \
    --max-messages 10
```

**Expected output:** You should see JSON messages with added metadata:
```json
{"timestamp":"2024-01-15T10:30:00Z","severity":"INFO","message":"Application started successfully","service":"api-gateway","version":"1.2.3","received_at":"2024-01-15T10:30:01.234Z","edge_node_id":"edge-node-01","pipeline":"shell-priority-queues"}
```

:::tip Success!
If you see messages with the added metadata (`received_at`, `edge_node_id`, `pipeline`), your environment is correctly configured!

**Next step:** Replace the shell pipeline with severity-based routing in Step 1.
:::

## Step 5: Verify Priority Topics

Before proceeding, verify all required priority topics exist and are accessible:

```bash
# Run this verification script
for topic in critical high normal low; do
  echo "Testing topic: logs-$topic"
  
  # Test producer
  echo '{"test": "message", "topic": "logs-'$topic'"}' | \
    $KAFKA_HOME/bin/kafka-console-producer.sh \
      --bootstrap-server $KAFKA_BROKERS \
      --topic logs-$topic
  
  # Test consumer (read the test message back)
  timeout 5s $KAFKA_HOME/bin/kafka-console-consumer.sh \
    --bootstrap-server $KAFKA_BROKERS \
    --topic logs-$topic \
    --from-beginning \
    --max-messages 1
  
  echo "✅ Topic logs-$topic is accessible"
  echo
done
```

## Step 6: Set Up Monitoring (Optional)

Enable Expanso metrics to monitor priority queue performance:

```bash
# Check if metrics endpoint is available
curl http://localhost:8081/metrics | grep pipeline_output_sent_total

# Set up environment variable for metrics collection
export EXPANSO_METRICS_PORT=8081
export EXPANSO_METRICS_PATH="/metrics"
```

## Troubleshooting Setup

### Issue: Cannot Connect to Kafka

**Symptom:** `kafka: client has run out of available brokers`

**Solutions:**

1. **Verify Kafka is running:**
```bash
# Check if Kafka is listening
netstat -tln | grep 9092
# or
nc -zv localhost 9092
```

2. **Check firewall/network:**
```bash
# For cloud Kafka, test connectivity
telnet broker1.kafka.cloud 9092
```

3. **Verify broker addresses:**
```bash
# Ensure KAFKA_BROKERS includes port numbers
echo $KAFKA_BROKERS  # Should be "host:port,host:port"
```

### Issue: Topics Not Created

**Symptom:** `Topic 'logs-critical' does not exist`

**Solution:** Enable auto-creation in Kafka config or create topics manually:

```bash
# Check Kafka server.properties
grep auto.create.topics.enable /opt/kafka/config/server.properties

# Should be: auto.create.topics.enable=true
# If false, create topics manually (see Step 2 above)
```

### Issue: Authentication Failures

**Symptom:** `SASL authentication failed`

**Solution:** Verify credentials and security configuration:

```bash
# Test authentication with kafka-console-producer
echo "test" | $KAFKA_HOME/bin/kafka-console-producer.sh \
  --bootstrap-server $KAFKA_BROKERS \
  --topic test \
  --producer-property security.protocol=$KAFKA_SECURITY_PROTOCOL \
  --producer-property sasl.mechanism=$KAFKA_SASL_MECHANISM \
  --producer-property sasl.jaas.config="org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$KAFKA_USERNAME\" password=\"$KAFKA_PASSWORD\";"
```

### Issue: Pipeline Fails to Deploy

**Symptom:** `Error: failed to deploy pipeline`

**Solution:** Check YAML syntax and Expanso connectivity:

```bash
# Validate YAML syntax
yamllint shell-priority-queues.yaml

# Check Expanso node status
expanso node status

# Check detailed pipeline logs
expanso pipeline logs shell-priority-queues --follow
```

## Next Steps

With your environment configured and shell pipeline deployed, you're ready to implement priority-based routing:

<div style={{display: 'flex', gap: '1rem', marginTop: '2rem', marginBottom: '2rem', flexWrap: 'wrap'}}>
  <a href="./step-1-severity-routing" className="button button--primary button--lg" style={{flex: '1', minWidth: '200px'}}>
    Step 1: Severity Routing
  </a>
</div>

## What's Next

- **Step 1:** Replace shell pipeline with severity-based routing (CRITICAL → immediate, INFO → batched)
- **Step 2:** Add customer tier prioritization (enterprise → dedicated queue, free → bulk processing)  
- **Step 3:** Implement multi-criteria scoring combining severity, tier, and event type
- **Step 4:** Add age-based escalation to prevent message starvation

---
title: Interactive Content Splitting Explorer
sidebar_label: Interactive Explorer
sidebar_position: 2
description: Explore 5 stages of content splitting with live before/after comparisons
keywords: [content-splitting, interactive, tutorial, demo, unarchive, array splitting]
---

import DataPipelineExplorer from '@site/src/components/DataPipelineExplorer';
import { contentSplittingStages } from '../content-splitting-full.stages';

# Interactive Content Splitting Explorer

**See content splitting in action!** Use the interactive explorer below to step through 5 stages of message splitting transformation. Watch as bundled sensor arrays are progressively split into individual messages with preserved context.

## How to Use This Explorer

1. **Navigate** using arrow keys (← →) or click the numbered stage buttons
2. **Compare** the Input (left) and Output (right) JSON at each stage
3. **Observe** how the array is split (red strikethrough) and context is preserved (green highlight)
4. **Inspect** the YAML code showing exactly what processor was added
5. **Learn** from the stage description explaining the technique and benefit

## Interactive Content Splitting Explorer

<DataPipelineExplorer
  stages={contentSplittingStages}
  title="CONTENT SPLITTING"
  subtitle="5-Stage Progressive Transformation"
/>

## Understanding the Stages

### Stage 1: Original Input
A single message containing multiple sensor readings bundled in an array. Efficient for transport but prevents individual processing.

### Stage 2: Store Parent Context
Critical step: preserve parent context (device_id, timestamp, location) in metadata before splitting to avoid data loss.

### Stage 3: Split Array into Individual Messages
The `unarchive` processor with `format: json_array` transforms one message with an array into multiple messages.

### Stage 4: Restore Parent Context
Each split message initially only contains array element data. Restore parent context from metadata to create complete messages.

### Stage 5: Content-Based Routing  
Now that each reading is an individual message with full context, route based on temperature value for granular processing.

## What You've Learned

After exploring all 5 stages, you now understand:

✅ **Metadata Preservation** - Store parent context before splitting to prevent data loss
✅ **Array Processing** - Use unarchive processor to transform arrays into individual messages  
✅ **Context Restoration** - Rebuild complete messages from metadata after splitting
✅ **Granular Routing** - Route individual messages based on their specific content
✅ **Production Patterns** - Critical ordering of store → split → restore operations

## Try It Yourself

Ready to build this content splitting pipeline? Follow the step-by-step tutorial:

<div style={{display: 'flex', gap: '1rem', marginTop: '2rem', marginBottom: '2rem', flexWrap: 'wrap'}}>
  <a href="./setup" className="button button--primary button--lg" style={{flex: '1', minWidth: '200px'}}>
    Start Tutorial
  </a>
  <a href="./complete-content-splitting" className="button button--secondary button--lg" style={{flex: '1', minWidth: '200px'}}>
    Download Complete Pipeline
  </a>
</div>

## Deep Dive into Each Step

Want to understand each transformation in depth?

- [**Step 1: Split JSON Arrays**](./step-1-split-json-arrays) - Master the unarchive processor and metadata patterns
- [**Step 2: Split CSV Batches**](./step-2-split-csv-batches) - Handle file input and line-by-line processing
- [**Step 3: Split Nested Structures**](./step-3-split-nested-structures) - Work with complex JSON hierarchies
- [**Step 4: Advanced Patterns**](./step-4-advanced-patterns) - Split-and-rebatch optimization strategies
- [**Step 5: Production Considerations**](./step-5-production-considerations) - Memory limits, error handling, and edge deployment

## Common Questions

### Why store parent context in metadata instead of fields?
The `unarchive` processor replaces the entire message with each array element. Without metadata storage, parent context like device_id and timestamp would be permanently lost.

### Can I split multiple arrays in the same message?
Yes, but it requires multiple unarchive processors in sequence. Each processor can only split one array field at a time.

### What happens to message ordering after splitting?
Splitting breaks original ordering within the batch. Use sequence numbers or Kafka partition keys if ordering is critical.

### How do I handle very large arrays that might cause memory issues?
Add validation to reject arrays exceeding your size limit (typically 10,000 items). Process larger batches by splitting them externally first.

### Can I filter out unwanted items during splitting?
Yes! After splitting, use a mapping processor with conditional deletion to remove items you don't want to process.

---

**Next:** [Set up your environment](./setup) to build this content splitting pipeline yourself

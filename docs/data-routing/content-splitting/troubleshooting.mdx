---
title: Troubleshooting Content Splitting
sidebar_label: Troubleshooting
sidebar_position: 10
description: Solutions for common content splitting issues, debugging techniques, and performance optimization
keywords: [troubleshooting, debugging, errors, performance, memory-issues, production-support]
---

# Troubleshooting Content Splitting

**Comprehensive solutions for common content splitting issues in development and production.** This guide covers memory problems, parsing errors, routing failures, and performance optimization.

## Quick Diagnostic Commands

Start with these commands to identify the issue category:

```bash
# Check pipeline status
expanso pipeline status content-splitting-complete

# View recent logs
expanso pipeline logs content-splitting-complete --tail 100

# Check system resources
df -h /var/log/expanso  # Disk space
free -h                 # Memory usage
top -p $(pgrep expanso) # CPU usage

# Test basic connectivity
curl -I http://localhost:8080/api/v1/content/split

# Check Kafka connectivity
kafka-topics.sh --bootstrap-server $KAFKA_BROKERS --list
```

---

## Memory and Resource Issues

### Issue: Out of Memory Errors During Splitting

**Symptom:** Pipeline crashes with "out of memory" or becomes unresponsive with large messages

**Diagnosis:**
```bash
# Check system memory
free -h
cat /proc/meminfo | grep Available

# Check pipeline memory usage
ps aux | grep expanso
pmap -x $(pgrep expanso)

# Check message sizes in logs
grep "estimated_peak_memory" /var/log/expanso/content-splitting*.log | tail -10
```

**Solutions:**

**1. Reduce Memory Limits**
```yaml
# More aggressive memory validation
processors:
  - mapping: |
      let max_message_mb = 25  # Reduce from default 100MB
      let max_array_length = 1000  # Reduce from default 10000
      let message_mb = this.encode("json").bytes().length() / 1024 / 1024
      
      if $message_mb > $max_message_mb {
        throw("Message too large: " + $message_mb.string() + "MB (max: " + $max_message_mb.string() + "MB)")
      }
```

**2. Implement Streaming Processing**
```yaml
# Process large arrays in chunks
processors:
  - mapping: |
      if this.readings.length() > 500 {
        # Split into chunks for processing
        let chunk_size = 500
        let chunks = []
        let i = 0
        
        while $i < this.readings.length() {
          let end = match {
            $i + $chunk_size > this.readings.length() => this.readings.length()
            _ => $i + $chunk_size
          }
          
          $chunks = $chunks.append({
            "chunk_id": $i / $chunk_size,
            "readings": this.readings.slice($i, $end)
          })
          
          $i = $i + $chunk_size
        }
        
        root.processing_chunks = $chunks
      } else {
        root.processing_chunks = [{"chunk_id": 0, "readings": this.readings}]
      }
```

**3. Configure JVM Memory (if applicable)**
```bash
# Set JVM memory limits
export JAVA_OPTS="-Xmx2g -Xms512m -XX:MaxGCPauseMillis=200"
systemctl restart expanso
```

### Issue: Memory Fragmentation Over Time

**Symptom:** Pipeline performance degrades over hours/days, eventually crashes

**Solutions:**

**1. Periodic Memory Cleanup**
```yaml
processors:
  - mapping: |
      # Force garbage collection periodically
      if count("messages_processed") % 10000 == 0 {
        # Log memory usage for monitoring
        root.memory_stats = {
          "messages_processed": count("messages_processed"),
          "memory_usage_mb": system_memory_usage_mb(),
          "gc_suggested": true
        }
      }
      
      root = this
```

**2. Implement Pipeline Restarts**
```bash
# Cron job for periodic restarts
# Add to /etc/crontab
0 2 * * * root systemctl restart expanso  # Daily restart at 2 AM
```

### Issue: Disk Space Exhaustion from Logs

**Symptom:** Pipeline fails with "no space left on device" errors

**Solutions:**

**1. Implement Log Rotation**
```bash
# Create logrotate configuration
cat > /etc/logrotate.d/expanso << 'EOF'
/var/log/expanso/*.log {
    daily
    rotate 7
    compress
    delaycompress
    missingok
    notifempty
    copytruncate
    maxsize 100M
}
EOF
```

**2. Monitor Disk Usage**
```yaml
# Add disk monitoring to pipeline
processors:
  - mapping: |
      let disk_usage = system_disk_percent()
      
      if $disk_usage > 90 {
        throw("Disk usage critical: " + $disk_usage.string() + "%")
      } else if $disk_usage > 80 {
        # Reduce logging verbosity
        root.log_level = "ERROR"
      }
      
      root = this
```

---

## Input Processing Errors

### Issue: JSON Parsing Failures

**Symptom:** "Invalid JSON" or "field not found" errors

**Diagnosis:**
```bash
# Check raw input data
tail -f /var/log/expanso/content-splitting*.log | grep -i "json\|parse"

# Test with minimal payload
curl -X POST http://localhost:8080/api/v1/content/split \
  -H "Authorization: Bearer token1" \
  -H "Content-Type: application/json" \
  -d '{}'  # Empty object to test basic parsing
```

**Solutions:**

**1. Enhanced Input Validation**
```yaml
processors:
  - mapping: |
      # Comprehensive JSON validation
      if !this.exists() {
        throw("Request body is empty")
      }
      
      if this.type() != "object" {
        throw("Request body must be a JSON object, got: " + this.type())
      }
      
      # Check for required fields with detailed errors
      let required_fields = ["data_type", "source_identifier"]
      range $required_fields | {
        if !root.get(this.string()).exists() {
          throw("Missing required field: '" + this.string() + "'. Required fields: " + 
                $required_fields.join(", "))
        }
      }
      
      root = this
```

**2. Handle Malformed JSON**
```yaml
processors:
  - try:
      # Attempt normal processing
      - mapping: |
          root = this.encode("json").decode("json")  # Validate JSON structure
          
    - catch:
      # Handle malformed JSON
      - mapping: |
          root = {
            "error": "malformed_json",
            "error_details": error(),
            "raw_input_size": this.encode("json").bytes().length(),
            "suggested_action": "validate_json_syntax"
          }
```

### Issue: CSV Header Row Being Processed as Data

**Symptom:** First record appears with field names instead of data values

**Solutions:**

**1. Robust Header Detection**
```yaml
processors:
  # Multiple header detection patterns
  - mapping: |
      let line = this.string().lowercase()
      
      # Comprehensive header patterns
      let header_patterns = [
        "transaction_id,",
        "id,timestamp,",
        "device_id,",
        "order_id,",
        "sensor,value,",
        "timestamp,level,"
      ]
      
      let is_header = false
      range $header_patterns | {
        if $line.has_prefix(this.string()) {
          $is_header = true
        }
      }
      
      # Additional heuristic checks
      if !$is_header {
        # Check if line contains mostly text (header characteristics)
        let comma_count = ($line.length() - $line.replace_all(",", "").length())
        let alpha_count = $line.re_replace_all("[^a-z]", "").length()
        
        if $comma_count > 3 && $alpha_count > ($line.length() * 0.7) {
          $is_header = true
        }
      }
      
      if $is_header {
        root = deleted()  # Remove header
      } else {
        root = this
      }
```

### Issue: Array Field Not Found

**Symptom:** "array field 'readings' missing" errors

**Solutions:**

**1. Dynamic Field Detection**
```yaml
processors:
  - mapping: |
      # Auto-detect array field by data type
      let possible_array_fields = match {
        this.data_type == "sensor_array" => ["readings", "data", "measurements", "values"]
        this.data_type == "log_events" => ["events", "logs", "entries", "records"]
        this.data_type == "csv_batch" => ["records", "rows", "data", "lines"]
        _ => ["items", "data", "records"]
      }
      
      let found_field = ""
      range $possible_array_fields | {
        if root.get(this.string()).exists() && root.get(this.string()).type() == "array" {
          $found_field = this.string()
          break
        }
      }
      
      if $found_field == "" {
        # List available fields for debugging
        let available_fields = []
        range this.keys() | {
          $available_fields = $available_fields.append(this.string())
        }
        
        throw("No array field found. Expected one of: " + $possible_array_fields.join(", ") + 
              ". Available fields: " + $available_fields.join(", "))
      }
      
      root = this
      root.detected_array_field = $found_field
```

---

## Routing and Output Issues

### Issue: Messages Not Appearing in Expected Destinations

**Symptom:** Split messages not reaching Kafka topics or output files

**Diagnosis:**
```bash
# Check output routing logic
expanso pipeline logs content-splitting-complete | grep -i "switch\|route\|output"

# Test Kafka connectivity
kafka-console-producer.sh --bootstrap-server $KAFKA_BROKERS --topic test-topic

# Check topic creation
kafka-topics.sh --bootstrap-server $KAFKA_BROKERS --describe --topic sensors-standard

# Monitor Kafka consumer lag
kafka-consumer-groups.sh --bootstrap-server $KAFKA_BROKERS --describe --group expanso-pipeline
```

**Solutions:**

**1. Debug Output Routing**
```yaml
output:
  switch:
    cases:
      # Add debug logging for routing decisions
      - check: this.data_type == "sensor_array"
        output:
          broker:
            pattern: fan_out
            outputs:
              # Debug output
              - file:
                  path: /var/log/expanso/routing-debug-${!timestamp_date()}.jsonl
                  request_map: |
                    root = {
                      "routing_decision": "sensor_array",
                      "message_id": this.processing_metadata.request_id,
                      "classification": this.reading_classification,
                      "timestamp": now()
                    }
              
              # Actual business output
              - kafka:
                  # ... existing config
```

**2. Add Fallback Routes**
```yaml
output:
  switch:
    cases:
      # ... existing cases ...
      
      # Catch-all for unmatched messages
      - output:
          file:
            path: /var/log/expanso/unrouted-messages-${!timestamp_date()}.jsonl
            request_map: |
              root = {
                "message": this,
                "routing_failure": "no_matching_case",
                "data_type": this.data_type,
                "available_fields": this.keys()
              }
```

### Issue: Kafka Authentication Failures

**Symptom:** "SASL authentication failed" or connection timeout errors

**Solutions:**

**1. Verify Kafka Configuration**
```bash
# Test Kafka authentication
kafka-topics.sh \
  --bootstrap-server $KAFKA_BROKERS \
  --command-config /etc/kafka/client.properties \
  --list

# Check client properties
cat /etc/kafka/client.properties
```

**2. Configure SASL Authentication**
```yaml
output:
  kafka:
    addresses: ["${KAFKA_BROKERS}"]
    # SASL configuration
    sasl:
      mechanism: PLAIN
      username: ${KAFKA_USERNAME}
      password: ${KAFKA_PASSWORD}
    # TLS configuration  
    tls:
      enabled: true
      skip_cert_verify: false
      ca_file: /etc/ssl/certs/kafka-ca.crt
```

---

## Performance Issues

### Issue: High Processing Latency

**Symptom:** Messages take seconds to process instead of milliseconds

**Diagnosis:**
```bash
# Check processing times
grep "processing_duration_ms" /var/log/expanso/content-splitting*.log | \
  awk '{print $NF}' | sort -n | tail -10

# Monitor CPU and I/O
iostat -x 1 10
vmstat 1 10
```

**Solutions:**

**1. Optimize Hot Paths**
```yaml
processors:
  # Reduce expensive operations in hot path
  - mapping: |
      # Cache expensive calculations
      meta processing_start = now()
      
      # Use simpler field access patterns
      root = this
      root.device_id = this.device_id.or("unknown")  # Faster than complex logic
      
      # Avoid regex in tight loops
      root.sensor_type = match {
        this.sensor.has_prefix("temp") => "temperature"
        this.sensor.has_prefix("press") => "pressure" 
        _ => "other"
      }
```

**2. Implement Batching Optimization**
```yaml
output:
  kafka:
    # Optimize batch settings for throughput
    batching:
      count: 1000      # Larger batches
      period: 30s      # Longer collection time
      byte_size: 1MB   # Size-based batching
    
    # Producer optimization
    producer_config:
      linger_ms: 50    # Allow time for batching
      compression_type: lz4  # Fast compression
      batch_size: 32768      # 32KB batches
```

### Issue: Memory Leaks During High Volume

**Symptom:** Memory usage grows continuously under load

**Solutions:**

**1. Implement Backpressure**
```yaml
input:
  http_server:
    # Limit concurrent connections
    max_connections: 100
    read_timeout: 10s
    
    # Rate limiting
    rate_limit:
      requests_per_minute: 1000
      burst_size: 50
```

**2. Add Memory Monitoring**
```yaml
processors:
  - mapping: |
      # Track memory usage trends
      let memory_mb = system_memory_usage_mb()
      let message_count = count("total_messages")
      
      # Log memory warnings
      if $memory_mb > 1500 {  # 1.5GB warning threshold
        root.memory_warning = {
          "current_usage_mb": $memory_mb,
          "message_count": $message_count,
          "avg_memory_per_message": $memory_mb / $message_count
        }
      }
      
      root = this
```

---

## Authentication and Security Issues

### Issue: API Token Authentication Failures

**Symptom:** "Invalid API token" or "Authorization header required" errors

**Solutions:**

**1. Debug Authentication Logic**
```yaml
processors:
  - mapping: |
      # Enhanced authentication debugging
      let auth_header = meta("http_headers").authorization.or("")
      
      if $auth_header == "" {
        throw("Missing Authorization header. Headers: " + meta("http_headers").keys().join(", "))
      }
      
      if !$auth_header.has_prefix("Bearer ") {
        throw("Invalid Authorization format. Expected 'Bearer <token>', got: '" + 
              $auth_header.slice(0, 20) + "...'")
      }
      
      let token = $auth_header.slice(7, -1)
      let valid_tokens = env("VALID_API_TOKENS").or("").split(",")
      
      if $valid_tokens.length() == 0 {
        throw("No valid tokens configured. Check VALID_API_TOKENS environment variable")
      }
      
      if !$valid_tokens.contains($token) {
        throw("Token not found in valid tokens list. Token hash: " + 
              $token.hash("sha256").string().slice(0, 8))
      }
      
      root = this
```

**2. Token Management**
```bash
# Generate secure tokens
openssl rand -hex 32  # Generate new token

# Update environment variables
export VALID_API_TOKENS="token1,token2,new_token_here"

# Restart pipeline to pick up new tokens
expanso pipeline restart content-splitting-complete
```

### Issue: TLS Certificate Problems

**Symptom:** "certificate verify failed" or "connection refused" on HTTPS

**Solutions:**

**1. Check Certificate Status**
```bash
# Verify certificate
openssl x509 -in /etc/ssl/certs/expanso.crt -text -noout

# Check certificate expiry
openssl x509 -in /etc/ssl/certs/expanso.crt -checkend 86400

# Test TLS connection
openssl s_client -connect localhost:8080 -servername localhost
```

**2. Certificate Configuration**
```yaml
input:
  http_server:
    tls:
      enabled: true
      cert_file: /etc/ssl/certs/expanso.crt
      key_file: /etc/ssl/private/expanso.key
      # Allow self-signed for development
      client_cert_auth: false
      min_version: "1.2"
```

---

## Data Quality Issues

### Issue: Inconsistent Data Types After Splitting

**Symptom:** Some split messages have string values where numbers expected

**Solutions:**

**1. Implement Type Coercion**
```yaml
processors:
  - mapping: |
      root = this
      
      # Ensure consistent data types
      if this.value.exists() {
        root.value = match {
          this.value.type() == "string" && this.value.re_match("^[0-9.]+$") => this.value.number()
          this.value.type() == "number" => this.value
          _ => throw("Invalid value format: " + this.value.string() + " (type: " + this.value.type() + ")")
        }
      }
      
      if this.quantity.exists() {
        root.quantity = this.quantity.number()
      }
      
      if this.timestamp.exists() && this.timestamp.type() == "string" {
        root.timestamp = this.timestamp.parse_timestamp("2006-01-02T15:04:05Z")
      }
```

### Issue: Missing Required Fields After Context Restoration

**Symptom:** Split messages missing device_id, order_id, or other context fields

**Solutions:**

**1. Validate Context Restoration**
```yaml
processors:
  # After context restoration
  - mapping: |
      root = this
      
      # Validate required context was restored
      let required_context = match {
        this.data_type == "sensor_array" => ["device_id", "location", "timestamp"]
        this.data_type == "order_items" => ["order_id", "customer_id"]
        this.data_type == "log_events" => ["node_id", "application"]
        _ => []
      }
      
      let missing_fields = []
      range $required_context | {
        if !root.get(this.string()).exists() {
          $missing_fields = $missing_fields.append(this.string())
        }
      }
      
      if $missing_fields.length() > 0 {
        throw("Missing context fields after splitting: " + $missing_fields.join(", ") + 
              ". Check metadata preservation logic.")
      }
```

---

## Monitoring and Alerting

### Issue: No Visibility Into Pipeline Health

**Solutions:**

**1. Implement Comprehensive Metrics**
```yaml
# Dedicated metrics pipeline
name: content-splitting-metrics

config:
  input:
    generate:
      interval: 30s
      mapping: |
        root = {
          "timestamp": now(),
          "pipeline_metrics": {
            "messages_processed_last_30s": counter("messages_processed", "30s"),
            "errors_last_30s": counter("processing_errors", "30s"),
            "avg_processing_time_ms": gauge("avg_processing_time", "30s"),
            "memory_usage_mb": system_memory_usage_mb(),
            "cpu_usage_percent": system_cpu_percent()
          }
        }

  output:
    kafka:
      topic: pipeline-health-metrics
```

**2. Set Up Alerting Rules**
```yaml
# Alert configuration (pseudo-code for monitoring system)
alerts:
  - name: HighErrorRate
    condition: error_rate > 5%
    action: slack_webhook
    
  - name: HighMemoryUsage  
    condition: memory_usage > 80%
    action: email_ops_team
    
  - name: ProcessingLatency
    condition: avg_processing_time > 1000ms
    action: pagerduty_alert
```

## Getting Help

### Diagnostic Information to Collect

When reporting issues, include:

**1. System Information**
```bash
# System details
uname -a
free -h
df -h
uptime

# Expanso version
expanso version

# Pipeline configuration
expanso pipeline describe content-splitting-complete
```

**2. Logs and Traces**
```bash
# Recent logs
expanso pipeline logs content-splitting-complete --tail 500

# System logs
journalctl -u expanso --since "1 hour ago"

# Performance data
top -n 1 -p $(pgrep expanso)
```

**3. Sample Data**
- Anonymized sample input that causes the issue
- Expected vs. actual output
- Error messages and stack traces

### Community Resources

1. **Expanso Documentation**: https://docs.expanso.io
2. **Community Forum**: [Link to forum if available]
3. **GitHub Issues**: [Link to GitHub if available]
4. **Support Contact**: [Support email/contact info]

### Emergency Procedures

**Pipeline Completely Down:**
```bash
# Emergency restart
systemctl restart expanso

# Revert to last known good configuration
expanso pipeline deploy content-splitting-backup.yaml

# Enable emergency bypass (if available)
expanso pipeline enable bypass-mode
```

**Data Loss Prevention:**
```bash
# Enable dead letter queue routing
export EMERGENCY_DLQ_MODE=true
expanso pipeline restart content-splitting-complete

# Backup current state
expanso pipeline export content-splitting-complete > backup-$(date +%Y%m%d-%H%M%S).yaml
```

## Summary

This troubleshooting guide covers the most common issues in content splitting deployments:

✅ **Memory Management** - OOM errors, leaks, fragmentation  
✅ **Input Processing** - JSON parsing, CSV headers, field detection  
✅ **Routing Issues** - Destination failures, authentication problems  
✅ **Performance** - Latency optimization, throughput tuning  
✅ **Security** - Authentication, TLS, token management  
✅ **Data Quality** - Type consistency, context preservation  
✅ **Monitoring** - Health checks, metrics, alerting

For issues not covered here, collect the diagnostic information above and reach out through the community resources.

---
title: How to Implement Content-Based Splitting
sidebar_label: Introduction
sidebar_position: 1
description: Split messages into multiple parts based on content structure for granular routing and processing
keywords: [splitting, unarchive, array processing, batch processing, message splitting, data routing, edge computing]
---

# How to Implement Content-Based Splitting

**Transform bundled messages into individual events for precise routing and processing**. This step-by-step guide teaches you 5 essential splitting techniques through an interactive explorer and hands-on exercises.

## The Problem

Your data pipelines receive messages containing multiple logical events bundled together. A single HTTP request might carry an array of 100 sensor readings, a batch file might contain thousands of transactions, or a log stream might deliver multiline stack traces.

```json
{
  "device_id": "sensor-001",
  "timestamp": "2025-10-20T10:00:00Z",
  "readings": [
    {"sensor": "temp-1", "value": 72.5, "unit": "F"},  // ❌ Bundled together
    {"sensor": "temp-2", "value": 85.3, "unit": "F"},  // ❌ Can't route individually  
    {"sensor": "temp-3", "value": 68.1, "unit": "F"}   // ❌ No per-event metrics
  ]
}
```

**The challenge:** Processing these as single messages prevents granular routing, per-item transformations, and accurate observability.

## The Solution: 5 Content Splitting Techniques

This guide teaches you how to apply the right splitting technique for each data format:

### 1. **JSON Array Splitting** → Sensor Arrays, Event Batches
Split JSON arrays using the unarchive processor with metadata preservation
- **Use case:** IoT sensor arrays, API event batches, monitoring metrics
- **Method:** `unarchive` processor with `format: json_array` 
- **Result:** One message per array item with preserved parent context

### 2. **CSV Batch Splitting** → Transaction Files, Log Files
Split CSV files line-by-line with structured field parsing
- **Use case:** Payment transaction batches, audit logs, bulk import files
- **Method:** File input with line scanner and CSV parsing
- **Result:** One message per CSV row with typed fields

### 3. **Nested Structure Splitting** → Order Items, Product Catalogs
Split complex nested JSON while preserving hierarchical context
- **Use case:** E-commerce orders, inventory records, nested configurations
- **Method:** Field-specific unarchive with metadata injection
- **Result:** Individual child records with full parent context

### 4. **Advanced Patterns** → Split-and-Rebatch, Conditional Filtering
Combine splitting with routing, filtering, and intelligent re-batching
- **Use case:** Priority-based processing, conditional routing, efficiency optimization
- **Method:** Split for routing decisions, rebatch for delivery efficiency
- **Result:** Optimized throughput with granular control

### 5. **Production Optimization** → Edge Splitting, Memory Management
Implement enterprise-grade splitting with error handling and resource limits
- **Use case:** High-volume production systems, edge computing, memory-constrained environments
- **Method:** Size validation, atomic operations, edge-first processing
- **Result:** Reliable, scalable splitting with 85% bandwidth reduction

## Why Process at the Edge?

**Cost Reduction:** Split at the edge before cloud upload to reduce bandwidth by 85% while maintaining real-time alerting on critical data.

**Compliance:** Enable granular data access controls required by GDPR, PCI-DSS, and SOC 2 by processing individual records separately.

**Performance:** Route critical events immediately while batching routine data, reducing latency for high-priority alerts from minutes to seconds.

**Reliability:** Implement atomic splitting operations with dead letter queues to ensure no data loss during transformation failures.

## What You'll Learn

By the end of this guide, you'll be able to:

✅ **Split JSON arrays** while preserving parent context metadata
✅ **Process CSV batches** with header handling and typed field parsing  
✅ **Handle nested structures** with multi-level context preservation
✅ **Implement advanced patterns** including split-and-rebatch optimization
✅ **Deploy production systems** with memory limits, error handling, and edge processing

## Get Started

### Option 1: Interactive Explorer (Recommended)
**See** each splitting technique in action with side-by-side before/after views.

[**→ Launch Interactive Explorer**](./explorer)

### Option 2: Step-by-Step Tutorial
**Build** the content splitting pipeline incrementally, one concept at a time.

1. [**Setup Guide**](./setup) - Environment configuration and shell deployment
2. [**Step 1: Split JSON Arrays**](./step-1-split-json-arrays) - Basic array processing with metadata
3. [**Step 2: Split CSV Batches**](./step-2-split-csv-batches) - File input and line parsing
4. [**Step 3: Split Nested Structures**](./step-3-split-nested-structures) - Complex JSON handling
5. [**Step 4: Advanced Patterns**](./step-4-advanced-patterns) - Split-and-rebatch optimization
6. [**Step 5: Production Considerations**](./step-5-production-considerations) - Memory limits and error handling

### Option 3: Jump to Complete Pipeline
**Download** the complete, production-ready content splitting solution.

[**→ Get Complete Pipeline**](./complete-content-splitting)

## Who This Guide Is For

- **Data Engineers** implementing ETL pipelines with granular event processing
- **Platform Engineers** optimizing bandwidth and processing costs at the edge
- **DevOps Teams** ensuring reliable data transformation with comprehensive error handling

## Prerequisites

- Expanso edge node running and connected to orchestrator
- Access credentials for destination services (Kafka, S3, etc.)
- Basic familiarity with YAML configuration ([Expanso Documentation](https://docs.expanso.io))

## Time to Complete

- **Interactive Explorer:** 10 minutes
- **Step-by-Step Tutorial:** 45-60 minutes  
- **Quick Deploy:** 5 minutes

## Real-World Impact

**Before Content Splitting:**
```
- Bandwidth Usage: 1GB full uploads
- Processing Latency: 30+ seconds (cloud processing)
- Alert Latency: 5+ minutes
- Storage Costs: 100% of raw data
- Routing Granularity: Bundle-level only
```

**After Content Splitting:**
```
- Bandwidth Usage: 150MB critical data only (85% reduction)
- Processing Latency: &lt;1 second (edge processing) 
- Alert Latency: &lt;10 seconds (immediate routing)
- Storage Costs: 15% of raw data (intelligent filtering)
- Routing Granularity: Individual event level
```

---

## Next Steps

Ready to start? Choose your learning path:

<div style={{display: 'flex', gap: '1.5rem', marginTop: '2rem', marginBottom: '3rem', flexWrap: 'wrap', justifyContent: 'flex-start'}}>
  <a href="./explorer" className="button button--primary button--lg" style={{display: 'inline-flex', alignItems: 'center', justifyContent: 'center', textDecoration: 'none', borderRadius: '8px', padding: '1rem 2rem', fontWeight: '600', minWidth: '240px', boxShadow: '0 2px 8px rgba(0,0,0,0.15)', cursor: 'pointer', transition: 'all 0.2s ease'}}>
    Interactive Explorer
  </a>
  <a href="./setup" className="button button--secondary button--lg" style={{display: 'inline-flex', alignItems: 'center', justifyContent: 'center', textDecoration: 'none', borderRadius: '8px', padding: '1rem 2rem', fontWeight: '600', minWidth: '240px', boxShadow: '0 2px 8px rgba(0,0,0,0.15)', cursor: 'pointer', transition: 'all 0.2s ease'}}>
    Step-by-Step Tutorial
  </a>
</div>

**Questions?** Check [Troubleshooting](./troubleshooting) or see [Related Examples](#related-examples) below.

## Related Examples

- [**Content Routing**](../content-routing) - Route split messages to different destinations
- [**Fan-Out Pattern**](../fan-out-pattern) - Send individual messages to multiple destinations  
- [**Circuit Breakers**](../circuit-breakers) - Add resilience to splitting pipelines

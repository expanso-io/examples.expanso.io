---
title: "Step 4: Advanced Patterns"
sidebar_label: "Step 4: Advanced Patterns"
sidebar_position: 7
description: Master split-and-rebatch optimization, conditional filtering, and message ordering strategies
keywords: [split-rebatch, conditional-filtering, message-ordering, batch-optimization, throughput-optimization]
---

# Step 4: Advanced Patterns

**Master sophisticated splitting patterns that balance individual message processing with delivery efficiency.** These advanced techniques are essential for high-throughput systems requiring both granular control and optimal performance.

## Learning Objectives

By completing this step, you'll understand:

✅ **Split-and-Rebatch Strategy** - Split for processing, rebatch for efficient delivery  
✅ **Conditional Filtering** - Remove unwanted items during or after splitting  
✅ **Message Ordering** - Preserve sequence relationships when needed  
✅ **Priority-Based Processing** - Route split messages based on urgency  
✅ **Throughput Optimization** - Balance processing latency with delivery efficiency

## The Problem: Processing vs. Delivery Efficiency

Many systems need to split messages for granular processing decisions, but then want to rebatch items for efficient delivery to downstream systems. Consider a log monitoring system:

```json title="Input: Mixed severity log batch"
{
  "batch_id": "batch-001",
  "node_id": "server-web-01",
  "collected_at": "2025-10-20T10:00:00Z",
  "environment": "production",
  "events": [
    {"level": "INFO", "message": "User logged in", "user_id": "user123"},
    {"level": "ERROR", "message": "Database connection failed", "service": "auth"},
    {"level": "DEBUG", "message": "Cache hit for user profile", "user_id": "user123"},
    {"level": "CRITICAL", "message": "Out of memory", "service": "api"},
    {"level": "INFO", "message": "Request completed", "duration_ms": 45},
    {"level": "WARNING", "message": "High CPU usage", "cpu_percent": 85},
    {"level": "INFO", "message": "User logged out", "user_id": "user123"},
    {"level": "ERROR", "message": "Payment service timeout", "service": "payment"}
  ]
}
```

**Efficiency challenges:**
- **Individual routing:** Each event needs different processing (CRITICAL → immediate alert, INFO → archive)
- **Delivery batching:** Downstream systems prefer batches of 1000+ events for efficiency
- **Priority handling:** Critical events need immediate delivery, INFO events can wait
- **Resource optimization:** Avoid overwhelming destinations with too many small batches

**Business impact:**
- Alert systems overwhelmed by individual event deliveries
- Storage systems inefficient with small batch writes
- Network overhead from excessive small message transmissions
- Complex downstream systems can't optimize for batch processing

## The Solution: Split-and-Rebatch Architecture

The solution splits messages for routing decisions, then intelligently rebatches based on destination and priority:

### Core Pattern: Split → Route → Rebatch by Priority

```yaml
pipeline:
  processors:
    # Step 1: Store batch context
    - mapping: |
        meta batch_id = this.batch_id
        meta node_id = this.node_id
        meta collected_at = this.collected_at
        meta environment = this.environment
        meta original_size = this.events.length()
        root = this

    # Step 2: Split for individual routing decisions
    - unarchive:
        format: json_array
        field: events

    # Step 3: Restore context and classify
    - mapping: |
        root = this
        root.batch_id = meta("batch_id")
        root.node_id = meta("node_id") 
        root.collected_at = meta("collected_at")
        root.environment = meta("environment")
        root.original_batch_size = meta("original_size")
        
        # Classify for routing and rebatching
        root.priority = match {
          this.level == "CRITICAL" => "immediate"
          this.level == "ERROR" => "high"
          this.level == "WARNING" => "medium"
          _ => "low"
        }
        
        root.destination_type = match {
          this.level == "CRITICAL" || this.level == "ERROR" => "alerting"
          this.level == "WARNING" => "analysis"
          _ => "archive"
        }

output:
  switch:
    cases:
      # Immediate: No batching (1 message = 1 delivery)
      - check: this.priority == "immediate"
        output:
          http_client:
            url: ${ALERT_WEBHOOK_URL}/critical
            verb: POST
            batching:
              count: 1      # No batching - immediate delivery
              period: 1s

      # High priority: Small batches, frequent delivery
      - check: this.priority == "high"
        output:
          kafka:
            topic: alerts-high-priority
            batching:
              count: 10     # Small batches
              period: 5s    # Frequent delivery

      # Medium priority: Moderate batches
      - check: this.priority == "medium"
        output:
          kafka:
            topic: analysis-warnings
            batching:
              count: 100    # Moderate batches
              period: 30s   # Less frequent

      # Low priority: Large batches, infrequent delivery
      - output:
          s3:
            bucket: ${LOG_ARCHIVE_BUCKET}
            path: logs/${!json("environment")}/${!timestamp_date()}/
            batching:
              count: 10000  # Large batches
              period: 300s  # 5-minute intervals
```

## Implementation: Complete Event Processing Pipeline

Let's build a comprehensive log processing system with advanced filtering, ordering, and optimization:

```yaml title="advanced-event-processor.yaml"
name: advanced-event-processor
description: Advanced log processing with split-rebatch optimization and filtering
type: pipeline
namespace: production

config:
  input:
    http_server:
      address: 0.0.0.0:8080
      path: /logs/batch
      timeout: 30s

  pipeline:
    processors:
      # Step 1: Validate and analyze input
      - mapping: |
          if !this.events.exists() || this.events.type() != "array" {
            throw("events must be an array")
          }
          
          let event_count = this.events.length()
          if $event_count == 0 {
            throw("events array cannot be empty")
          }
          
          if $event_count > 10000 {
            throw("Batch too large: " + $event_count.string() + " events (max: 10000)")
          }
          
          root = this

      # Step 2: Store comprehensive batch metadata
      - mapping: |
          meta batch_id = this.batch_id.or(uuid_v4())
          meta node_id = this.node_id.or("unknown")
          meta collected_at = this.collected_at.or(now())
          meta environment = this.environment.or("development")
          meta original_size = this.events.length()
          meta batch_received_at = now()
          
          # Calculate batch statistics for optimization
          let critical_count = 0
          let error_count = 0
          let warning_count = 0
          let info_count = 0
          let debug_count = 0
          
          range this.events | {
            if this.level == "CRITICAL" { $critical_count = $critical_count + 1 }
            else if this.level == "ERROR" { $error_count = $error_count + 1 }
            else if this.level == "WARNING" { $warning_count = $warning_count + 1 }
            else if this.level == "INFO" { $info_count = $info_count + 1 }
            else if this.level == "DEBUG" { $debug_count = $debug_count + 1 }
          }
          
          meta critical_count = $critical_count
          meta error_count = $error_count
          meta warning_count = $warning_count
          meta info_count = $info_count
          meta debug_count = $debug_count
          meta has_critical = $critical_count > 0
          
          root = this

      # Step 3: Split events for individual processing
      - unarchive:
          format: json_array
          field: events

      # Step 4: Restore context and apply business logic
      - mapping: |
          root = this
          
          # Restore batch context
          root.batch_id = meta("batch_id")
          root.node_id = meta("node_id")
          root.collected_at = meta("collected_at")
          root.environment = meta("environment")
          root.original_batch_size = meta("original_size")
          root.batch_received_at = meta("batch_received_at")
          
          # Add sequence information for ordering
          root.event_sequence = count("seq_" + meta("batch_id"))
          root.global_sequence = count("global_seq")
          
          # Calculate processing latency
          root.processing_latency_ms = (now() - meta("batch_received_at")).milliseconds()
          
          # Classify priority and destination
          root.severity_level = this.level.or("INFO").uppercase()
          root.priority = match {
            root.severity_level == "CRITICAL" => "immediate"
            root.severity_level == "ERROR" => "high"  
            root.severity_level == "WARNING" => "medium"
            root.severity_level == "INFO" => "low"
            root.severity_level == "DEBUG" => "lowest"
            _ => "low"
          }
          
          # Determine destination type
          root.destination_type = match {
            root.severity_level == "CRITICAL" => "alerting"
            root.severity_level == "ERROR" => "alerting"
            root.severity_level == "WARNING" => "monitoring"
            root.severity_level == "INFO" => "analytics"
            _ => "archive"
          }
          
          # Add service context if available
          root.service_name = this.service.or("unknown")
          root.requires_investigation = root.severity_level == "CRITICAL" || 
                                       root.severity_level == "ERROR"
          
          # Calculate business impact scoring
          root.impact_score = match {
            root.severity_level == "CRITICAL" => 100
            root.severity_level == "ERROR" && this.service == "payment" => 80
            root.severity_level == "ERROR" && this.service == "auth" => 70  
            root.severity_level == "ERROR" => 60
            root.severity_level == "WARNING" && this.service == "payment" => 40
            root.severity_level == "WARNING" => 30
            _ => 10
          }

      # Step 5: Conditional filtering based on environment and level
      - mapping: |
          # Filter out debug messages in production
          if meta("environment") == "production" && root.severity_level == "DEBUG" {
            root = deleted()
          }
          
          # Filter out low-impact info messages during high-load periods
          else if meta("has_critical") == true && root.severity_level == "INFO" {
            # If batch has critical events, filter out routine info to reduce noise
            if !this.message.string().contains("login") && 
               !this.message.string().contains("logout") {
              root = deleted()
            }
          }
          
          else {
            root = this
          }

  output:
    switch:
      cases:
        # Critical events: Immediate delivery + multiple destinations
        - check: this.priority == "immediate"
          output:
            broker:
              pattern: fan_out
              outputs:
                # Immediate HTTP alert
                - http_client:
                    url: ${CRITICAL_ALERT_URL}
                    verb: POST
                    headers:
                      X-Severity: CRITICAL
                      X-Environment: ${!json("environment")}
                      X-Node: ${!json("node_id")}
                    timeout: 5s
                    batching:
                      count: 1
                      period: 1s

                # Slack notification
                - http_client:
                    url: ${SLACK_WEBHOOK_URL}
                    verb: POST
                    headers:
                      Content-Type: application/json
                    timeout: 10s
                    batching:
                      count: 1
                      period: 1s

                # High-priority Kafka topic
                - kafka:
                    addresses: ["${KAFKA_BROKERS}"]
                    topic: events-critical
                    key: ${!json("node_id")}
                    headers:
                      severity: CRITICAL
                      environment: ${!json("environment")}
                    compression: snappy
                    batching:
                      count: 1
                      period: 1s

        # High priority: Small batches for quick processing
        - check: this.priority == "high"
          output:
            kafka:
              addresses: ["${KAFKA_BROKERS}"]
              topic: events-errors
              key: ${!json("service_name")}
              headers:
                severity: ERROR
                impact_score: ${!json("impact_score")}
              partition: ${!json("service_name").hash("xxhash64").mod(3)}
              batching:
                count: 25
                period: 10s
                compression: lz4

        # Medium priority: Moderate batches for analysis
        - check: this.priority == "medium"
          output:
            kafka:
              addresses: ["${KAFKA_BROKERS}"]
              topic: events-warnings
              key: ${!json("node_id")}
              batching:
                count: 200
                period: 60s
                compression: gzip

        # Low priority: Large batches for efficient storage
        - check: this.priority == "low"
          output:
            s3:
              bucket: ${LOG_ANALYTICS_BUCKET}
              path: events/${!json("environment")}/${!timestamp_date()}/${!timestamp_hour()}/
              batching:
                count: 5000
                period: 300s
              compression: gzip
              content_type: application/x-ndjson

        # Lowest priority: Very large batches, long intervals
        - output:
            s3:
              bucket: ${LOG_ARCHIVE_BUCKET}
              path: archive/${!json("environment")}/${!timestamp_date()}/
              batching:
                count: 50000
                period: 1800s  # 30 minutes
              compression: gzip
```

## Advanced Filtering Patterns

### Pattern 1: Dynamic Filtering Based on System Load

Adjust filtering based on current system performance:

```yaml
pipeline:
  processors:
    # Check system load and adjust filtering
    - mapping: |
        # Store system metrics in metadata
        meta cpu_usage = system_cpu_percent()
        meta memory_usage = system_memory_percent()
        meta system_overloaded = meta("cpu_usage").number() > 80 || 
                                meta("memory_usage").number() > 90
        root = this

    - unarchive:
        format: json_array
        field: events

    # Dynamic filtering based on system load
    - mapping: |
        root = this
        
        # During system overload, be more aggressive with filtering
        if meta("system_overloaded") == true {
          # Filter out all DEBUG and INFO during overload
          if this.level == "DEBUG" || this.level == "INFO" {
            root = deleted()
          }
          # Even filter some WARNING messages
          else if this.level == "WARNING" && !this.message.contains("critical") {
            root = deleted()  
          }
        } else {
          # Normal filtering: only DEBUG in production
          if meta("environment") == "production" && this.level == "DEBUG" {
            root = deleted()
          }
        }
```

### Pattern 2: Content-Based Filtering with Regex

Filter messages based on content patterns:

```yaml
pipeline:
  processors:
    - unarchive:
        format: json_array
        field: events

    - mapping: |
        # Define filtering patterns
        let noise_patterns = [
          ".*health check.*",
          ".*heartbeat.*", 
          ".*ping.*",
          ".*GET /status.*",
          ".*Connection keep-alive.*"
        ]
        
        let message = this.message.string().lowercase()
        let is_noise = false
        
        # Check if message matches any noise pattern
        range $noise_patterns | {
          if $message.re_match(this.string()) {
            $is_noise = true
          }
        }
        
        # Filter out noise unless it's an error
        if $is_noise == true && this.level != "ERROR" && this.level != "CRITICAL" {
          root = deleted()
        } else {
          root = this
        }
```

### Pattern 3: Sampling for High-Volume Streams

Implement intelligent sampling for very high-volume log streams:

```yaml
pipeline:
  processors:
    - unarchive:
        format: json_array
        field: events

    # Intelligent sampling based on level and content
    - mapping: |
        root = this
        
        # Never sample critical or error events
        if this.level == "CRITICAL" || this.level == "ERROR" {
          root.sampled = false
        }
        # Sample 50% of warnings
        else if this.level == "WARNING" {
          root.sampled = random() > 0.5
        }
        # Sample 10% of info messages
        else if this.level == "INFO" {
          root.sampled = random() > 0.9
        }
        # Sample 1% of debug messages
        else if this.level == "DEBUG" {
          root.sampled = random() > 0.99
        }
        # Default: don't sample
        else {
          root.sampled = false
        }

    # Remove sampled messages
    - mapping: |
        if this.sampled == true {
          root = deleted()
        } else {
          root = this
        }
```

## Message Ordering Strategies

### Pattern 1: Sequence Number Preservation

Maintain message ordering within batches:

```yaml
pipeline:
  processors:
    # Store batch sequencing information
    - mapping: |
        meta batch_id = this.batch_id
        meta batch_timestamp = this.collected_at
        
        # Number each event in the original order
        let sequence = 0
        range this.events | {
          this.original_sequence = $sequence
          $sequence = $sequence + 1
        }
        
        root = this

    - unarchive:
        format: json_array
        field: events

    - mapping: |
        root = this
        root.batch_id = meta("batch_id")
        root.batch_timestamp = meta("batch_timestamp")
        
        # Preserve original sequence for ordering
        root.sequence_number = this.original_sequence
        
        # Add sequence metadata for Kafka partitioning
        root.partition_key = meta("batch_id") + "-" + this.original_sequence.string()

output:
  kafka:
    topic: ordered-events
    # Use sequence-aware key to maintain order within partitions
    key: ${!json("partition_key")}
```

### Pattern 2: Priority-Based Ordering

Route messages with priority-aware ordering:

```yaml
pipeline:
  processors:
    # Add priority sequencing
    - mapping: |
        meta batch_id = this.batch_id
        meta high_priority_seq = 0
        meta normal_priority_seq = 0
        
        # Separate sequencing for different priorities
        range this.events | {
          if this.level == "CRITICAL" || this.level == "ERROR" {
            this.priority_sequence = meta("high_priority_seq")
            meta high_priority_seq = meta("high_priority_seq") + 1
          } else {
            this.priority_sequence = meta("normal_priority_seq") 
            meta normal_priority_seq = meta("normal_priority_seq") + 1
          }
        }
        
        root = this

    - unarchive:
        format: json_array
        field: events

    - mapping: |
        root = this
        root.batch_id = meta("batch_id")
        
        # Create priority-aware partition key
        root.priority_level = match {
          this.level == "CRITICAL" || this.level == "ERROR" => "high"
          _ => "normal"
        }
        
        root.partition_key = root.priority_level + "-" + this.priority_sequence.string()

output:
  switch:
    cases:
      # High priority: Dedicated topic with ordering
      - check: this.priority_level == "high"
        output:
          kafka:
            topic: high-priority-events
            key: ${!json("partition_key")}
            # Single partition for strict ordering of critical events
            partition: 0

      # Normal priority: Multiple partitions for throughput
      - output:
          kafka:
            topic: normal-events
            key: ${!json("partition_key")}
            # Distribute across partitions for parallel processing
            partition: ${!json("partition_key").hash("xxhash64").mod(10)}
```

## Performance Optimization Strategies

### Batch Size Optimization Based on Content

Dynamically adjust batch sizes based on event characteristics:

```yaml
pipeline:
  processors:
    # Analyze batch composition for optimization
    - mapping: |
        let total_events = this.events.length()
        let critical_events = 0
        let large_events = 0
        let total_size_bytes = 0
        
        range this.events | {
          if this.level == "CRITICAL" {
            $critical_events = $critical_events + 1
          }
          
          let event_size = this.encode("json").bytes().length()
          $total_size_bytes = $total_size_bytes + $event_size
          
          if $event_size > 1024 {  # 1KB threshold
            $large_events = $large_events + 1
          }
        }
        
        meta critical_ratio = $critical_events.number() / $total_events.number()
        meta large_event_ratio = $large_events.number() / $total_events.number()
        meta avg_event_size = $total_size_bytes / $total_events
        
        root = this

output:
  switch:
    cases:
      # Batch with high critical ratio: small batches, frequent delivery
      - check: meta("critical_ratio") > 0.1
        output:
          kafka:
            topic: events-mixed-critical
            batching:
              count: 50      # Small batches when critical events present
              period: 5s

      # Batch with large events: size-based batching
      - check: meta("large_event_ratio") > 0.3
        output:
          kafka:
            topic: events-large
            batching:
              byte_size: 1048576  # 1MB batches for large events
              period: 30s

      # Standard events: normal batching
      - output:
          kafka:
            topic: events-standard
            batching:
              count: 1000
              period: 60s
```

### Throughput vs. Latency Optimization

Balance throughput and latency based on business requirements:

```yaml
# Configuration for different performance profiles
output:
  switch:
    cases:
      # Ultra-low latency for critical systems
      - check: this.service == "payment" || this.service == "auth"
        output:
          kafka:
            topic: critical-services
            batching:
              count: 1      # No batching - immediate delivery
              period: 1s

      # High throughput for analytics  
      - check: this.destination_type == "analytics"
        output:
          kafka:
            topic: analytics-events
            batching:
              count: 10000   # Large batches
              period: 300s   # 5-minute intervals
              compression: gzip

      # Balanced for monitoring
      - check: this.destination_type == "monitoring"
        output:
          kafka:
            topic: monitoring-events
            batching:
              count: 500     # Medium batches
              period: 30s    # Balanced interval
              compression: lz4

      # Default: optimized for cost
      - output:
          s3:
            bucket: ${DEFAULT_BUCKET}
            batching:
              count: 50000   # Very large batches
              period: 1800s  # 30-minute intervals
              compression: gzip
```

## Summary

You've mastered advanced splitting patterns for production systems. Key takeaways:

✅ **Split-and-Rebatch:** Split for decisions, rebatch for delivery efficiency  
✅ **Dynamic Filtering:** Adjust filtering based on content, load, and environment  
✅ **Message Ordering:** Preserve sequence when business logic requires it  
✅ **Performance Tuning:** Balance latency, throughput, and resource usage  
✅ **Business Logic:** Apply sophisticated routing based on multiple criteria

**Next:** [Step 5: Production Considerations](./step-5-production-considerations) to learn memory management, error handling, and edge deployment strategies.

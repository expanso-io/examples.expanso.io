---
title: "Step 2: Split CSV Batches"
sidebar_label: "Step 2: Split CSV Batches"  
sidebar_position: 5
description: Process CSV batch files line-by-line with header handling, field parsing, and typed data conversion
keywords: [csv-processing, file-input, batch-processing, line-scanner, field-parsing, financial-data]
---

# Step 2: Split CSV Batches

**Transform CSV batch files into individual structured messages with proper data typing and validation.** CSV splitting is essential for processing financial transactions, audit logs, and bulk data imports where each row needs individual processing.

## Learning Objectives

By completing this step, you'll understand:

✅ **File Input Processing** - Use file input with line-based scanning  
✅ **CSV Parsing Patterns** - Parse CSV lines into structured JSON objects  
✅ **Data Type Conversion** - Convert string fields to proper data types  
✅ **Header Row Handling** - Skip headers and validate CSV structure  
✅ **Field Validation** - Ensure data quality and handle malformed records

## The Problem: Financial Transaction Batches

Financial systems often deliver transaction data as CSV batch files containing thousands of individual transactions. Each file might represent hourly, daily, or real-time transaction batches:

```csv title="Input: transactions-batch-20251020.csv"
transaction_id,timestamp,amount,currency,country,customer_id,merchant_id,description
txn-001,2025-10-20T10:00:00Z,150.00,USD,US,cust-123,merch-456,Coffee Shop Purchase
txn-002,2025-10-20T10:01:00Z,12500.00,EUR,DE,cust-789,merch-101,Equipment Purchase
txn-003,2025-10-20T10:02:00Z,75.50,GBP,UK,cust-456,merch-789,Online Service
txn-004,2025-10-20T10:03:00Z,25.99,USD,US,cust-101,merch-456,Book Purchase
txn-005,2025-10-20T10:04:00Z,8750.00,JPY,JP,cust-202,merch-345,Software License
```

**Processing challenges:**
- Can't route transactions based on amount thresholds (fraud detection)
- Can't apply country-specific compliance rules per transaction  
- Can't track individual transaction metrics and audit trails
- Can't implement real-time alerting for high-value transactions

**Business impact:**
- Fraud detection delayed until entire batch is processed
- Regulatory compliance requirements for individual transaction processing not met
- Unable to implement per-transaction encryption or PII handling
- Real-time monitoring and alerting impossible with batch processing

## The Solution: Line-by-Line CSV Processing

CSV splitting requires a different approach than JSON arrays. Instead of using `unarchive`, we use file input with line scanning and custom field parsing:

### Core Pattern: File → Lines → Structured Messages

```yaml
input:
  file:
    paths:
      - /var/data/transactions-*.csv
    scanner:
      lines: {}     # Split file by lines
    codec: lines    # Each line becomes a message

pipeline:
  processors:
    # Skip header row
    - mapping: |
        if this.string().has_prefix("transaction_id") {
          root = deleted()  # Remove header from processing
        } else {
          root = this       # Keep data rows
        }
    
    # Parse CSV line into structured data
    - mapping: |
        let fields = this.string().split(",")
        
        root.transaction_id = $fields.index(0)
        root.timestamp = $fields.index(1) 
        root.amount = $fields.index(2).number()
        root.currency = $fields.index(3)
        root.country = $fields.index(4)
        root.customer_id = $fields.index(5)
        root.merchant_id = $fields.index(6)
        root.description = $fields.index(7)
```

## Implementation: Complete Transaction Processor

Let's build a production-ready CSV transaction processor with validation, compliance, and routing:

```yaml title="transaction-csv-processor.yaml"
name: transaction-csv-processor
description: Process CSV transaction batches into individual structured messages
type: pipeline
namespace: production

config:
  input:
    file:
      paths:
        - /var/data/incoming/transactions-*.csv
        - /var/data/incoming/payments-*.csv
      scanner:
        lines:
          max_buffer_size: 1000000  # 1MB line buffer
      codec: lines
      delete_on_finish: false  # Keep files for audit
      
  pipeline:
    processors:
      # Step 1: Skip empty lines and comments
      - mapping: |
          let line = this.string().trim()
          
          # Skip empty lines
          if $line == "" {
            root = deleted()
          }
          # Skip comment lines
          else if $line.has_prefix("#") || $line.has_prefix("//") {
            root = deleted()
          }
          else {
            root = this
          }

      # Step 2: Detect and skip header row
      - mapping: |
          let line = this.string()
          
          # Skip header row (case insensitive)
          if $line.lowercase().has_prefix("transaction_id") ||
             $line.lowercase().has_prefix("txn_id") ||
             $line.lowercase().has_prefix("id,") {
            root = deleted()
          } else {
            root = this
          }

      # Step 3: Validate CSV structure
      - mapping: |
          let line = this.string()
          let fields = $line.split(",")
          let field_count = $fields.length()
          
          # Validate field count (expecting 8 fields)
          if $field_count < 8 {
            throw("Invalid CSV: expected 8 fields, got " + $field_count.string() + 
                  " in line: " + $line)
          }
          
          if $field_count > 8 {
            # Handle descriptions with commas (join extra fields)
            let description_parts = $fields.slice(7, $field_count)
            let description = $description_parts.join(",")
            
            # Rebuild fields array with proper description
            root.fields = $fields.slice(0, 7).append($description)
          } else {
            root.fields = $fields
          }

      # Step 4: Parse CSV fields into structured JSON
      - mapping: |
          let fields = this.fields
          
          root.transaction_id = $fields.index(0).trim()
          root.timestamp = $fields.index(1).trim()
          root.amount = $fields.index(2).trim().number()
          root.currency = $fields.index(3).trim().uppercase()
          root.country = $fields.index(4).trim().uppercase() 
          root.customer_id = $fields.index(5).trim()
          root.merchant_id = $fields.index(6).trim()
          root.description = $fields.index(7).trim()
          
          # Add processing metadata
          root.processed_at = now()
          root.batch_id = uuid_v4()
          root.file_source = file_source()
          
      # Step 5: Data validation and enrichment
      - mapping: |
          # Validate required fields
          if this.transaction_id == "" {
            throw("transaction_id cannot be empty")
          }
          
          if this.amount <= 0 {
            throw("amount must be positive, got: " + this.amount.string())
          }
          
          # Validate currency code
          let valid_currencies = ["USD", "EUR", "GBP", "JPY", "CAD", "AUD"]
          if !$valid_currencies.contains(this.currency) {
            throw("Invalid currency: " + this.currency)
          }
          
          # Parse and validate timestamp
          root.timestamp_parsed = this.timestamp.parse_timestamp("2006-01-02T15:04:05Z")
          
          # Classify transaction
          root.transaction_type = match {
            this.amount >= 10000 => "large"
            this.amount >= 1000 => "medium"  
            _ => "small"
          }
          
          # Add geographic classification
          root.region = match {
            this.country == "US" || this.country == "CA" => "north_america"
            this.country == "GB" || this.country == "DE" || this.country == "FR" => "europe"
            this.country == "JP" || this.country == "KR" || this.country == "CN" => "asia_pacific"
            _ => "other"
          }
          
          # Flag for compliance requirements
          root.requires_kyc = this.amount >= 10000
          root.requires_pci_audit = true
          root.is_international = this.country != "US"
          
          # Calculate fees (example business logic)
          root.processing_fee = match {
            this.transaction_type == "large" => this.amount * 0.001  # 0.1%
            this.transaction_type == "medium" => this.amount * 0.0015 # 0.15%
            _ => this.amount * 0.002  # 0.2%
          }
          
          root.net_amount = this.amount - this.processing_fee

  output:
    switch:
      cases:
        # Large transactions: Fraud detection + audit trail
        - check: this.transaction_type == "large"
          output:
            broker:
              pattern: fan_out
              outputs:
                # Immediate fraud detection
                - http_client:
                    url: ${FRAUD_DETECTION_URL}/analyze
                    verb: POST
                    timeout: 10s
                    headers:
                      Authorization: Bearer ${FRAUD_API_KEY}
                      X-Priority: high
                    batching:
                      count: 1
                      period: 1s

                # Audit trail storage
                - kafka:
                    addresses: ["${KAFKA_BROKERS}"]
                    topic: transaction-audit
                    key: ${!json("transaction_id")}
                    partition: 0  # All large transactions to partition 0
                    compression: snappy
                    
                # Compliance reporting
                - file:
                    path: /var/audit/large-transactions-${!timestamp_date()}.jsonl
                    codec: lines

        # International transactions: Compliance processing
        - check: this.is_international == true
          output:
            kafka:
              addresses: ["${KAFKA_BROKERS}"] 
              topic: international-transactions
              key: ${!json("country")}
              headers:
                compliance_required: "true"
                processing_region: ${!json("region")}
              batching:
                count: 100
                period: 30s

        # Medium transactions: Standard processing with batching
        - check: this.transaction_type == "medium"
          output:
            kafka:
              addresses: ["${KAFKA_BROKERS}"]
              topic: medium-transactions
              key: ${!json("merchant_id")}
              batching:
                count: 500
                period: 60s
                compression: lz4

        # Small domestic transactions: Efficient batched storage
        - output:
            s3:
              bucket: ${S3_TRANSACTIONS_BUCKET}
              path: transactions/${!json("country")}/${!timestamp_date()}/${!timestamp_hour()}/
              batching:
                count: 10000
                period: 300s  # 5 minutes
              compression: gzip
              content_type: application/x-ndjson
```

## Testing the CSV Processor

Create comprehensive test files and validate processing:

```bash
# Create test CSV files
mkdir -p /var/data/incoming

# Test 1: Standard transaction batch
cat > /var/data/incoming/transactions-test-001.csv << 'EOF'
transaction_id,timestamp,amount,currency,country,customer_id,merchant_id,description
txn-001,2025-10-20T10:00:00Z,150.00,USD,US,cust-123,merch-456,Coffee Shop Purchase
txn-002,2025-10-20T10:01:00Z,12500.00,EUR,DE,cust-789,merch-101,Equipment Purchase  
txn-003,2025-10-20T10:02:00Z,75.50,GBP,UK,cust-456,merch-789,Online Service
txn-004,2025-10-20T10:03:00Z,25000.00,USD,US,cust-101,merch-456,Large Equipment
EOF

# Test 2: CSV with problematic data (commas in description)
cat > /var/data/incoming/transactions-test-002.csv << 'EOF'
transaction_id,timestamp,amount,currency,country,customer_id,merchant_id,description
txn-005,2025-10-20T11:00:00Z,89.99,USD,US,cust-202,merch-345,"Restaurant Purchase, Table 5, Party of 4"
txn-006,2025-10-20T11:01:00Z,15000.00,JPY,JP,cust-303,merch-678,Software License
EOF

# Test 3: CSV with validation errors
cat > /var/data/incoming/transactions-test-003.csv << 'EOF'
transaction_id,timestamp,amount,currency,country,customer_id,merchant_id,description
# This file contains test cases for validation
txn-007,2025-10-20T12:00:00Z,-50.00,USD,US,cust-404,merch-567,Negative Amount Test
,2025-10-20T12:01:00Z,100.00,USD,US,cust-505,merch-678,Empty ID Test
txn-008,invalid-timestamp,200.00,XXX,ZZ,cust-606,merch-789,Invalid Currency/Country
EOF

# Deploy and test the processor
expanso pipeline deploy transaction-csv-processor.yaml

# Monitor processing results
tail -f /var/log/expanso/transaction-csv-processor.log

# Check different output destinations
echo "Large transactions in audit log:"
tail /var/audit/large-transactions-$(date +%Y-%m-%d).jsonl

echo "Kafka topic contents:"
kafka-console-consumer.sh \
  --bootstrap-server ${KAFKA_BROKERS} \
  --topic international-transactions \
  --from-beginning --max-messages 10
```

## Advanced CSV Processing Patterns

### Pattern 1: Multi-Format CSV Detection

Handle CSV files with different schemas automatically:

```yaml
pipeline:
  processors:
    # Detect CSV format based on header
    - mapping: |
        let line = this.string().lowercase()
        
        # Determine CSV format
        root.csv_format = match {
          $line.contains("transaction_id") && $line.contains("amount") => "financial"
          $line.contains("user_id") && $line.contains("event") => "analytics" 
          $line.contains("device_id") && $line.contains("timestamp") => "iot"
          _ => "unknown"
        }
        
        # Store format in metadata for later processors
        meta csv_format = root.csv_format
        
        # Skip header row
        if root.csv_format != "unknown" {
          root = deleted()
        } else {
          root = this
        }

    # Parse based on detected format
    - switch:
        cases:
          - check: meta("csv_format") == "financial"
            processors:
              - mapping: |
                  let fields = this.string().split(",")
                  root.transaction_id = $fields.index(0)
                  root.amount = $fields.index(2).number()
                  # ... financial format parsing

          - check: meta("csv_format") == "analytics"  
            processors:
              - mapping: |
                  let fields = this.string().split(",")
                  root.user_id = $fields.index(0)
                  root.event_name = $fields.index(1)
                  # ... analytics format parsing
```

### Pattern 2: CSV with Quoted Fields and Escaping

Handle properly escaped CSV data:

```yaml
pipeline:
  processors:
    - mapping: |
        let line = this.string()
        
        # Custom CSV parser for quoted fields
        let parse_csv_line = |line| {
          let fields = []
          let current_field = ""
          let in_quotes = false
          let i = 0
          
          # Parse character by character
          while $i < $line.length() {
            let char = $line.slice($i, $i + 1)
            
            if $char == "\"" {
              $in_quotes = !$in_quotes
            } else if $char == "," && !$in_quotes {
              $fields = $fields.append($current_field.trim())
              $current_field = ""
            } else {
              $current_field = $current_field + $char  
            }
            
            $i = $i + 1
          }
          
          # Add final field
          $fields = $fields.append($current_field.trim())
          $fields
        }
        
        root.parsed_fields = $parse_csv_line($line)
```

### Pattern 3: CSV with Variable Column Count

Handle CSV files where not all rows have the same number of columns:

```yaml
pipeline:
  processors:
    - mapping: |
        let fields = this.string().split(",")
        let field_count = $fields.length()
        
        # Define expected field mappings
        root.transaction_id = $fields.index(0).or("")
        root.timestamp = $fields.index(1).or("")
        root.amount = $fields.index(2).or("0").number()
        root.currency = $fields.index(3).or("USD")
        root.country = $fields.index(4).or("US")
        root.customer_id = $fields.index(5).or("")
        root.merchant_id = $fields.index(6).or("")
        
        # Handle optional fields
        root.description = match {
          $field_count >= 8 => $fields.index(7)
          _ => ""
        }
        
        root.category = match {
          $field_count >= 9 => $fields.index(8)
          _ => "uncategorized"
        }
        
        root.reference = match {
          $field_count >= 10 => $fields.index(9) 
          _ => ""
        }
        
        # Track completeness
        root.field_completeness = $field_count / 10.0
```

## Compliance and Security for Financial Data

### PCI-DSS Compliance for Payment Data

When processing payment card transactions:

```yaml
pipeline:
  processors:
    # PCI-DSS field handling
    - mapping: |
        # Parse standard fields
        let fields = this.string().split(",")
        
        root.transaction_id = $fields.index(0)
        root.timestamp = $fields.index(1)
        root.amount = $fields.index(2).number()
        
        # Handle PCI-sensitive data
        let card_number = $fields.index(8).or("")
        
        # Never store full card numbers - mask immediately
        root.card_last_four = match {
          $card_number.length() >= 4 => $card_number.slice(-4, -1)
          _ => "****"
        }
        
        # Hash the full card number for fraud detection
        root.card_hash = $card_number.hash("sha256").string()
        
        # Remove the original card number field
        # (it only existed in the CSV, not in our output)
        
        # Add PCI compliance metadata
        root.pci_compliant_processing = true
        root.card_data_masked = true
        root.processing_audit_id = uuid_v4()

    # Validate PCI compliance
    - mapping: |
        # Ensure no card numbers in output
        let json_str = this.encode("json")
        
        # Check for credit card patterns (4 digits repeated)
        if $json_str.re_match("\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}") {
          throw("PCI violation: Full card number detected in output")
        }
        
        root = this
```

### Anti-Money Laundering (AML) Flagging

Implement AML transaction monitoring:

```yaml
pipeline:
  processors:
    # AML risk assessment
    - mapping: |
        # Standard CSV parsing
        let fields = this.string().split(",")
        root.transaction_id = $fields.index(0)
        root.amount = $fields.index(2).number()
        root.country = $fields.index(4).uppercase()
        root.customer_id = $fields.index(5)
        
        # AML risk scoring
        root.aml_risk_score = 0
        
        # High-value transaction risk
        if this.amount >= 10000 {
          root.aml_risk_score = root.aml_risk_score + 25
        }
        
        # High-risk country check
        let high_risk_countries = ["XX", "YY", "ZZ"]  # Real list from compliance
        if $high_risk_countries.contains(this.country) {
          root.aml_risk_score = root.aml_risk_score + 50
        }
        
        # Round dollar amounts (possible structuring)
        if this.amount % 1000 == 0 && this.amount > 5000 {
          root.aml_risk_score = root.aml_risk_score + 15
        }
        
        # Overall risk classification
        root.aml_risk_level = match {
          root.aml_risk_score >= 75 => "high"
          root.aml_risk_score >= 50 => "medium"
          root.aml_risk_score >= 25 => "low"
          _ => "minimal"
        }
        
        root.requires_aml_review = root.aml_risk_level == "high"
```

## Performance Optimization for Large CSV Files

### Memory-Efficient File Processing

Handle very large CSV files without memory exhaustion:

```yaml
config:
  input:
    file:
      paths:
        - /var/data/large-batches/*.csv
      scanner:
        lines:
          max_buffer_size: 65536  # 64KB line buffer
          max_scan_tokens: 1000   # Process 1000 lines at a time
      codec: lines
      
  pipeline:
    processors:
      # Lightweight processing to reduce memory usage
      - mapping: |
          let fields = this.string().split(",")
          
          # Only extract essential fields
          root = {
            "id": $fields.index(0),
            "amount": $fields.index(2).number(),
            "country": $fields.index(4),
            "timestamp": now()  # Processing timestamp
          }

  output:
    # Stream directly to output without buffering
    kafka:
      addresses: ["${KAFKA_BROKERS}"]
      topic: transactions
      batching:
        count: 100    # Small batches
        period: 1s    # Frequent flushing
```

### Parallel Processing for Multiple Files

Process multiple CSV files concurrently:

```yaml
# Use multiple pipeline instances with file pattern matching
config:
  input:
    file:
      paths:
        # Each instance processes files with different patterns
        - /var/data/batch-${WORKER_ID}-*.csv
      scanner:
        lines: {}
      codec: lines

  # Add worker identification to messages
  pipeline:
    processors:
      - mapping: |
          # Parse CSV normally
          let fields = this.string().split(",")
          root.transaction_id = $fields.index(0)
          root.amount = $fields.index(2).number()
          
          # Add worker metadata
          root.processed_by_worker = env("WORKER_ID").or("default")
          root.processing_node = hostname()
```

## Troubleshooting Common CSV Issues

### Issue: Header Row Being Processed as Data

**Symptom:** First transaction appears with transaction_id of "transaction_id"

**Solution:**
```yaml
processors:
  - mapping: |
      let line = this.string().lowercase()
      
      # Multiple header detection patterns
      if $line.has_prefix("transaction_id") ||
         $line.has_prefix("id,") ||
         $line.has_prefix("txn_id") ||
         $line.contains("amount") && $line.contains("currency") {
        root = deleted()  # Skip header
      } else {
        root = this
      }
```

### Issue: Commas in Description Fields Breaking Parsing

**Symptom:** Fields shifted because description contains commas

**Solution:**
```yaml
processors:
  - mapping: |
      let line = this.string()
      let fields = $line.split(",")
      
      # If more fields than expected, description has commas
      if $fields.length() > 8 {
        # Reconstruct description from remaining fields
        let description_parts = $fields.slice(7, $fields.length())
        let description = $description_parts.join(",")
        
        # Rebuild proper field array
        root.fields = $fields.slice(0, 7).append($description)
      } else {
        root.fields = $fields
      }
```

### Issue: Number Parsing Failures for Amounts

**Symptom:** Amount field parsing throws "not a number" errors

**Solution:**
```yaml
processors:
  - mapping: |
      let amount_str = $fields.index(2).trim()
      
      # Clean common formatting issues
      $amount_str = $amount_str.replace_all("$", "")    # Remove currency symbols
      $amount_str = $amount_str.replace_all(",", "")    # Remove thousands separators
      $amount_str = $amount_str.replace_all(" ", "")    # Remove spaces
      
      # Validate before parsing
      if $amount_str.re_match("^[0-9]+\\.?[0-9]*$") {
        root.amount = $amount_str.number()
      } else {
        throw("Invalid amount format: " + $amount_str)
      }
```

### Issue: Timestamp Parsing Failures

**Symptom:** Timestamp field cannot be parsed into proper date format

**Solution:**
```yaml
processors:
  - mapping: |
      let timestamp_str = $fields.index(1).trim()
      
      # Try multiple timestamp formats
      root.timestamp_parsed = match {
        # ISO 8601 format
        $timestamp_str.re_match("\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z?") => 
          $timestamp_str.parse_timestamp("2006-01-02T15:04:05Z")
        
        # US date format  
        $timestamp_str.re_match("\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}:\\d{2}") =>
          $timestamp_str.parse_timestamp("01/02/2006 15:04:05")
        
        # European date format
        $timestamp_str.re_match("\\d{2}\\.\\d{2}\\.\\d{4} \\d{2}:\\d{2}:\\d{2}") =>
          $timestamp_str.parse_timestamp("02.01.2006 15:04:05")
        
        # Unix timestamp
        $timestamp_str.re_match("^\\d{10}$") =>
          $timestamp_str.number().timestamp_unix()
        
        # Default: use current time and log warning
        _ => {
          # Log the parsing issue
          meta parsing_warning = "Could not parse timestamp: " + $timestamp_str
          now()
        }
      }
```

## Summary

You've mastered CSV batch processing with line-by-line splitting. Key takeaways:

✅ **File Input Pattern:** Use file input with line scanner, not unarchive  
✅ **Header Handling:** Always skip header rows with proper detection  
✅ **Field Parsing:** Split lines and convert to proper data types  
✅ **Data Validation:** Validate structure, types, and business rules  
✅ **Error Handling:** Handle malformed data gracefully with detailed errors

**Next:** [Step 3: Split Nested Structures](./step-3-split-nested-structures) to learn complex JSON hierarchy processing.

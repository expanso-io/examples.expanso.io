---
title: Configure Broker Foundation for Fan-Out
sidebar_label: Step 1 - Configure Broker
sidebar_position: 4
description: Set up the broker output foundation for concurrent multi-destination routing
keywords: [broker, fan-out, concurrent routing, configuration, foundation]
---

# Step 1: Configure Broker Foundation for Fan-Out

**Transform single-output pipelines into multi-destination powerhouses**. This step teaches you how to configure the broker output foundation that enables concurrent routing to multiple destinations without blocking or duplication overhead.

## Understanding Broker Fan-Out Patterns

The broker output is Expanso's solution for sophisticated data routing. Unlike simple outputs that send to one destination, the broker can implement multiple routing patterns:

### Fan-Out Pattern vs Alternatives

**Fan-Out (`fan_out`)**: Send each message to ALL outputs concurrently
- **Use case:** Logging to multiple systems (Kafka + S3 + Elasticsearch)
- **Behavior:** Concurrent delivery, independent failure handling
- **Performance:** Optimal throughput, no blocking between destinations

**Round-Robin (`round_robin`)**: Distribute messages across outputs in sequence  
- **Use case:** Load balancing across multiple identical services
- **Behavior:** Sequential distribution (message 1 → output A, message 2 → output B)
- **Performance:** Even distribution, shared throughput

**Try (`try`)**: Attempt outputs in order until one succeeds
- **Use case:** Primary/failover scenarios (try cloud, fallback to local)
- **Behavior:** Sequential attempts, stops at first success
- **Performance:** Resilience focus, higher latency

For multi-destination routing, **fan-out is the right choice** because we want every destination to receive every message.

## Core Configuration Structure

Every broker fan-out configuration follows this structure:

```yaml
output:
  broker:
    pattern: fan_out    # Enable concurrent routing
    outputs:           # Array of destination configurations
      - destination_1   # First output (e.g., Kafka)
      - destination_2   # Second output (e.g., S3)  
      - destination_N   # Nth output (e.g., Elasticsearch)
```

The broker handles:
- **Message duplication**: Efficiently copies messages to all destinations
- **Concurrent delivery**: Sends to all outputs simultaneously, not sequentially
- **Independent error handling**: Per-output retry logic and failure isolation
- **Resource management**: Shared connection pools and memory optimization

## Step-by-Step Implementation

### Starting Point: Shell Pipeline

From the setup step, you have this working single-output pipeline:

```yaml title="Current shell-fan-out-pipeline.yaml"
output:
  broker:
    pattern: fan_out
    outputs:
      - file:
          path: /tmp/expanso-test-events-${!timestamp_unix_date("2006-01-02")}.jsonl
          codec: lines
```

### Add Broker Logging and Monitoring

Before adding multiple destinations, enhance the broker with proper observability:

```yaml title="Enhanced broker configuration"
output:
  broker:
    pattern: fan_out
    
    # Global broker settings that apply to all outputs
    batching:
      # Overall broker memory limits
      count: 1000
      period: 30s
    
    # Logging and debugging
    debug: false
    
    outputs:
      # Keep existing file output for comparison
      - file:
          path: /tmp/expanso-test-events-${!timestamp_unix_date("2006-01-02")}.jsonl
          codec: lines
          
          # Per-output configuration
          batching:
            count: 10
            period: 5s
```

**Key concepts:**

- **Global vs Per-Output Settings**: Broker-level settings provide defaults; output-level settings override for specific destinations
- **Resource Isolation**: Each output gets its own batching buffers and retry state
- **Debug Mode**: Set `debug: true` during development to see detailed routing logs

### Add Multiple File Destinations

Before cloud integrations, demonstrate fan-out with multiple local files:

```yaml title="Multi-file fan-out test"
output:
  broker:
    pattern: fan_out
    outputs:
      # Raw events for debugging
      - file:
          path: /tmp/raw-events-${!timestamp_unix_date("2006-01-02")}.jsonl
          codec: lines
          
      # Processed events for analysis  
      - file:
          path: /tmp/processed-events-${!timestamp_unix_date("2006-01-02")}.jsonl
          codec: lines
          
          # Add metadata to demonstrate per-output processing
          processors:
            - mapping: |
                root = this
                root.output_destination = "processed_file"
                root.output_timestamp = now()
      
      # Error events for troubleshooting
      - file:
          path: /tmp/error-events-${!timestamp_unix_date("2006-01-02")}.jsonl  
          codec: lines
          
          # Only write events that might have issues
          processors:
            - mapping: |
                # Flag events missing critical fields
                root = this
                root.has_error = false
                if !this.exists("sensor_id") {
                  root.has_error = true
                  root.error_reason = "missing sensor_id"
                }
                if this.temperature.type() != "number" {
                  root.has_error = true  
                  root.error_reason = "invalid temperature type"
                }
```

**Test the multi-file fan-out:**

```bash title="Test multi-file routing"
# Deploy updated configuration
expanso job deploy shell-fan-out-pipeline.yaml

# Generate test data  
./generate-test-data.sh http://localhost:8080/events 5

# Verify all three files receive data
echo "Raw events:"
wc -l /tmp/raw-events-*.jsonl

echo "Processed events:"  
wc -l /tmp/processed-events-*.jsonl

echo "Error events:"
wc -l /tmp/error-events-*.jsonl

# Check that processed events have additional metadata
echo "Sample processed event:"
tail -1 /tmp/processed-events-*.jsonl | jq '.output_destination, .output_timestamp'
```

**Expected output:**
```
Raw events:
5 /tmp/raw-events-2025-01-20.jsonl

Processed events:
5 /tmp/processed-events-2025-01-20.jsonl  

Error events:
0 /tmp/error-events-2025-01-20.jsonl

Sample processed event:
"processed_file"
"2025-01-20T10:00:01.234Z"
```

### Configure Independent Batching Strategies

Different destinations have different performance characteristics. Configure batching to optimize each:

```yaml title="Optimized batching per destination type"
output:
  broker:
    pattern: fan_out
    outputs:
      # Real-time file (small batches, low latency)
      - file:
          path: /tmp/realtime-${!timestamp_unix_date("2006-01-02")}.jsonl
          codec: lines
          batching:
            count: 5          # Small batches  
            period: 1s        # Fast delivery
            
      # Archive file (large batches, high efficiency)
      - file:
          path: /tmp/archive-${!timestamp_unix_date("2006-01-02")}.jsonl
          codec: lines  
          batching:
            count: 100        # Large batches
            period: 30s       # Less frequent delivery
            
      # Analytics file (medium batches, balanced approach)
      - file:
          path: /tmp/analytics-${!timestamp_unix_date("2006-01-02")}.jsonl
          codec: lines
          batching:
            count: 25         # Medium batches
            period: 10s       # Moderate delivery frequency
```

**Understanding batching trade-offs:**

**Small batches (real-time destinations):**
- ✅ Lower latency for immediate processing
- ✅ Faster error detection and recovery
- ❌ Higher overhead per message
- ❌ More frequent I/O operations

**Large batches (archive destinations):**  
- ✅ Better compression ratios
- ✅ Lower per-message overhead
- ✅ More efficient bulk operations
- ❌ Higher latency for individual messages
- ❌ More memory usage while batching

**Medium batches (analytics destinations):**
- ✅ Balanced latency and efficiency
- ✅ Good for near-real-time processing
- ✅ Reasonable memory usage

### Add Error Handling and Retries

Configure per-output error handling to ensure resilience:

```yaml title="Error handling configuration"  
output:
  broker:
    pattern: fan_out
    outputs:
      # Reliable destination (aggressive retries)
      - file:
          path: /tmp/critical-${!timestamp_unix_date("2006-01-02")}.jsonl
          codec: lines
          
          # Aggressive retry for critical data
          max_retries: 10
          backoff:
            initial_interval: 100ms
            max_interval: 10s
            multiplier: 2.0
            
      # Best-effort destination (limited retries)
      - file:
          path: /tmp/besteffort-${!timestamp_unix_date("2006-01-02")}.jsonl  
          codec: lines
          
          # Limited retry for non-critical data
          max_retries: 3
          backoff:
            initial_interval: 1s
            max_interval: 5s
            
      # No-retry destination (fail fast)
      - file:
          path: /tmp/failfast-${!timestamp_unix_date("2006-01-02")}.jsonl
          codec: lines
          
          # Fail fast for debugging/development
          max_retries: 0
```

**Test error handling behavior:**

```bash title="Test error scenarios"
# Create a permission error to test retry behavior
sudo touch /tmp/permission-test-${date +%Y-%m-%d}.jsonl
sudo chmod 000 /tmp/permission-test-${date +%Y-%m-%d}.jsonl

# Update one file output to write to protected file
# (demonstrates retry behavior in logs)
```

## Production-Ready Broker Configuration

Combine all concepts into a production-ready broker foundation:

```yaml title="production-broker-foundation.yaml"
name: production-fan-out-foundation
description: Production-ready broker foundation for multi-destination routing
type: pipeline
namespace: production
labels:
  environment: production
  pattern: fan-out
  tier: foundation

config:
  input:
    http_server:
      address: 0.0.0.0:8080
      path: /events
      timeout: 5s
      rate_limit: "10000/s"  # Higher limit for production
      
      # Production security settings
      cors:
        enabled: false  # Disable in production
        
  pipeline:
    processors:
      # Enhanced edge metadata
      - mapping: |
          root = this
          root.edge_node_id = env("NODE_ID").or("unknown")
          root.edge_location = env("NODE_LOCATION").or("unknown")
          root.processing_timestamp = now()
          root.pipeline_version = "foundation-v1"
          
          # Add routing metadata for observability  
          root.routing = {
            "pattern": "fan_out",
            "destinations_count": 3,
            "expected_destinations": ["realtime", "archive", "analytics"]
          }

      # Production validation
      - mapping: |
          # Strict validation for production
          if !this.exists("event_id") {
            throw("missing required field: event_id")
          }
          if !this.exists("sensor_id") { 
            throw("missing required field: sensor_id")
          }
          if !this.exists("timestamp") {
            throw("missing required field: timestamp")
          }
          
          # Validate timestamp format
          if !this.timestamp.parse_timestamp().type() != "number" {
            throw("invalid timestamp format: " + this.timestamp)
          }
          
          root = this

  output:
    broker:
      pattern: fan_out
      
      # Global broker configuration
      batching:
        count: 1000    # High-throughput default
        period: 30s    # Reasonable latency default
        
      # Enable detailed logging in production
      debug: false
      
      outputs:
        # Real-time processing destination
        - file:
            path: /var/expanso/data/realtime/events-${!timestamp_unix_date("2006-01-02")}-${!timestamp_unix_hour()}.jsonl
            codec: lines
            
            # Optimized for low latency
            batching:
              count: 50
              period: 2s
              
            # Aggressive retry for real-time criticality  
            max_retries: 5
            backoff:
              initial_interval: 200ms
              max_interval: 5s
              
            # Add real-time metadata
            processors:
              - mapping: |
                  root = this
                  root.destination_type = "realtime"
                  root.delivered_at = now()

        # Archive destination  
        - file:
            path: /var/expanso/archive/events-${!timestamp_unix_date("2006/01/02")}/hour-${!timestamp_unix_hour()}/batch-${!count("archive_batches")}.jsonl
            codec: lines
            
            # Optimized for efficiency
            batching:
              count: 1000
              period: 15m
              
            # Patient retry for archive reliability
            max_retries: 10  
            backoff:
              initial_interval: 1s
              max_interval: 60s
              
            # Add archive metadata
            processors:
              - mapping: |
                  root = this
                  root.destination_type = "archive"
                  root.archived_at = now()
                  
        # Analytics destination
        - file:
            path: /var/expanso/analytics/events-${!timestamp_unix_date("2006-01-02")}.jsonl
            codec: lines
            
            # Balanced approach
            batching:
              count: 200
              period: 5m
              
            # Moderate retry for analytics
            max_retries: 5
            backoff:
              initial_interval: 500ms
              max_interval: 30s
              
            # Add analytics metadata and processing
            processors:
              - mapping: |
                  root = this
                  root.destination_type = "analytics"
                  root.processed_at = now()
                  
                  # Add analytics-specific fields
                  root.analytics = {
                    "hour_partition": this.timestamp.parse_timestamp().ts_hour(),
                    "day_of_week": this.timestamp.parse_timestamp().ts_weekday(),
                    "is_weekend": this.timestamp.parse_timestamp().ts_weekday() > 5
                  }
```

## Testing and Verification

### Deploy and Test Production Foundation

```bash title="Deploy production foundation"
# Deploy the production foundation
expanso job deploy production-broker-foundation.yaml

# Wait for deployment
sleep 10

# Verify status
expanso job status production-fan-out-foundation
```

### Generate Load Test Data

```bash title="Generate comprehensive test data"
# Create load test script
cat > load-test-fan-out.sh << 'EOF'
#!/bin/bash

ENDPOINT=${1:-http://localhost:8080/events}
DURATION=${2:-60}  # seconds
RATE=${3:-10}      # messages per second

echo "Load testing fan-out broker for ${DURATION}s at ${RATE} msg/s..."

END_TIME=$(($(date +%s) + DURATION))

MESSAGE_COUNT=0
while [ $(date +%s) -lt $END_TIME ]; do
  TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
  TEMP=$(echo "scale=1; 15 + $RANDOM % 30" | bc)
  HUMIDITY=$(echo "scale=1; 20 + $RANDOM % 60" | bc)
  
  EVENT_JSON=$(cat <<JSON
{
  "event_id": "load-test-$(date +%s%N | cut -b1-13)",
  "sensor_id": "sensor-$(($RANDOM % 20 + 1))",
  "timestamp": "$TIMESTAMP",
  "temperature": $TEMP,
  "humidity": $HUMIDITY,
  "device_type": "environmental_sensor",
  "test_metadata": {
    "load_test": true,
    "message_number": $MESSAGE_COUNT,
    "target_rate": $RATE
  }
}
JSON
)

  curl -s -X POST "$ENDPOINT" \
    -H "Content-Type: application/json" \
    -d "$EVENT_JSON" >/dev/null
  
  MESSAGE_COUNT=$((MESSAGE_COUNT + 1))
  
  # Rate limiting
  sleep $(echo "scale=3; 1.0 / $RATE" | bc)
done

echo "Load test complete: $MESSAGE_COUNT messages sent"
EOF

chmod +x load-test-fan-out.sh

# Run 2-minute load test at 5 msg/s
./load-test-fan-out.sh http://localhost:8080/events 120 5
```

### Verify Fan-Out Distribution

```bash title="Verify all destinations receive data"
# Check message counts across all destinations
echo "Checking fan-out distribution..."

REALTIME_COUNT=$(find /var/expanso/data/realtime/ -name "*.jsonl" -exec wc -l {} + 2>/dev/null | tail -1 | awk '{print $1}')
ARCHIVE_COUNT=$(find /var/expanso/archive/ -name "*.jsonl" -exec wc -l {} + 2>/dev/null | tail -1 | awk '{print $1}')  
ANALYTICS_COUNT=$(find /var/expanso/analytics/ -name "*.jsonl" -exec wc -l {} + 2>/dev/null | tail -1 | awk '{print $1}')

echo "Message distribution:"
echo "  Realtime: $REALTIME_COUNT messages"
echo "  Archive: $ARCHIVE_COUNT messages"  
echo "  Analytics: $ANALYTICS_COUNT messages"

# Verify counts are equal (fan-out should send to ALL destinations)
if [ "$REALTIME_COUNT" = "$ARCHIVE_COUNT" ] && [ "$ARCHIVE_COUNT" = "$ANALYTICS_COUNT" ]; then
  echo "✅ Fan-out working correctly - all destinations have equal message counts"
else
  echo "❌ Fan-out issue - message counts don't match"
fi

# Check metadata differences
echo ""
echo "Sample from each destination:"
echo "Realtime metadata:"
find /var/expanso/data/realtime/ -name "*.jsonl" -exec tail -1 {} \; | jq '.destination_type, .delivered_at' | head -2

echo "Archive metadata:"  
find /var/expanso/archive/ -name "*.jsonl" -exec tail -1 {} \; | jq '.destination_type, .archived_at' | head -2

echo "Analytics metadata:"
find /var/expanso/analytics/ -name "*.jsonl" -exec tail -1 {} \; | jq '.destination_type, .analytics' | head -4
```

## Common Issues and Solutions

### Issue: Messages Not Reaching All Destinations

**Symptom:** Some destinations receive fewer messages than others in fan-out pattern.

**Diagnosis:**
```bash
# Check for per-output errors in logs
expanso job logs production-fan-out-foundation | grep -i error

# Check disk space for file outputs
df -h /var/expanso/

# Verify permissions on output directories
ls -la /var/expanso/data/ /var/expanso/archive/ /var/expanso/analytics/
```

**Solutions:**

1. **Resource exhaustion:** Increase memory limits or reduce batch sizes
```yaml
batching:
  count: 100  # Reduce from 1000
  period: 10s # Reduce from 30s
```

2. **Disk space issues:** Enable log rotation or increase storage
```bash
# Add log rotation
cat > /etc/logrotate.d/expanso-fan-out << 'EOF'
/var/expanso/**/*.jsonl {
    daily
    rotate 7
    compress
    missingok
    notifempty
}
EOF
```

3. **Permission problems:** Fix directory permissions
```bash
sudo chown -R expanso:expanso /var/expanso/
sudo chmod -R 755 /var/expanso/
```

### Issue: High Memory Usage with Large Batches

**Symptom:** Edge node memory consumption grows over time.

**Solutions:**

1. **Reduce global batch limits:**
```yaml
output:
  broker:
    pattern: fan_out
    batching:
      count: 500    # Reduce from 1000
      period: 15s   # Reduce from 30s
```

2. **Limit concurrent batches per output:**
```yaml
outputs:
  - file:
      # ... other config
      max_in_flight: 2  # Limit concurrent batches
```

3. **Monitor memory usage:**
```bash
# Create memory monitoring script
watch -n 5 'ps aux | grep expanso | grep -v grep; free -h'
```

### Issue: Uneven Batch Timing

**Symptom:** Some destinations receive batches much more frequently than others.

**Root Cause:** Different batching configurations combined with varying message rates.

**Solutions:**

1. **Normalize batch timing across destinations:**
```yaml
# Use consistent timing with different counts
outputs:
  - file:  # Real-time
      batching:
        count: 50
        period: 10s
  - file:  # Archive  
      batching:
        count: 500
        period: 10s
  - file:  # Analytics
      batching:
        count: 100
        period: 10s
```

2. **Use timeout-based batching for consistent delivery:**
```yaml
outputs:
  - file:
      batching:
        period: 30s      # Consistent timing
        byte_size: 1MB   # Size-based alternative to count
```

## Key Concepts Recap

**Fan-Out Pattern Benefits:**
- ✅ **Concurrent delivery**: No blocking between destinations
- ✅ **Independent failure handling**: Per-output retry and error handling  
- ✅ **Flexible configuration**: Different batching and processing per destination
- ✅ **Resource efficiency**: Shared message copying and connection pooling

**Batching Strategy Guidelines:**
- **Real-time destinations**: Small batches (50-200), short periods (1-5s)
- **Archive destinations**: Large batches (1000+), longer periods (15-60m)
- **Analytics destinations**: Medium batches (200-500), moderate periods (5-15m)

**Error Handling Best Practices:**
- **Critical data**: Aggressive retries with exponential backoff
- **Best-effort data**: Limited retries to prevent resource exhaustion  
- **Debug/development**: Fast failure for immediate feedback

## Next Steps

✅ **Broker foundation configured!** You now have a robust, production-ready fan-out foundation that can scale to handle multiple cloud destinations.

**Ready for cloud integration?** Continue with adding real-world destinations:

<div style={{display: 'flex', gap: '1.5rem', marginTop: '2rem', marginBottom: '3rem', flexWrap: 'wrap', justifyContent: 'flex-start'}}>
  <a href="./step-2-add-kafka" className="button button--primary button--lg" style={{display: 'inline-flex', alignItems: 'center', justifyContent: 'center', textDecoration: 'none', borderRadius: '8px', padding: '1rem 2rem', fontWeight: '600', minWidth: '240px', boxShadow: '0 2px 8px rgba(0,0,0,0.15)', cursor: 'pointer', transition: 'all 0.2s ease'}}>
    Step 2: Add Kafka Streaming
  </a>
  <a href="./troubleshooting" className="button button--secondary button--lg" style={{display: 'inline-flex', alignItems: 'center', justifyContent: 'center', textDecoration: 'none', borderRadius: '8px', padding: '1rem 2rem', fontWeight: '600', minWidth: '240px', boxShadow: '0 2px 8px rgba(0,0,0,0.15)', cursor: 'pointer', transition: 'all 0.2s ease'}}>
    Troubleshooting Guide
  </a>
</div>

---

**Next:** [Add Kafka for real-time streaming](./step-2-add-kafka)

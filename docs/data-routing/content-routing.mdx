---
title: How to Route Data Based on Content
sidebar_label: Content Routing
sidebar_position: 3
description: Use conditional logic to send data to different destinations
keywords: [routing, switch, conditional, content-based, priority]
---

import CodeBlock from '@theme/CodeBlock';
import pipelineYaml from '!!raw-loader!../../examples/data-routing/content-routing.yaml';


# How to Route Data Based on Content

In production data pipelines, not all data deserves the same treatment. Critical alerts need immediate delivery to on-call systems, while routine metrics can be batched to long-term storage. Geographic data should route to regional endpoints, and high-priority orders need expedited processing while standard requests follow the normal path.

The switch output with conditional routing enables this by evaluating message content and directing each message to the appropriate destination. Unlike fan-out patterns that send every message to every destination, content-based routing makes intelligent decisions about where each message should go based on its properties, values, and context.

## Problem Statement

Consider a multi-tenant application processing events from IoT devices across different regions and severity levels. Your routing requirements might include:

- **Critical alerts** (severity: critical) â†’ PagerDuty API for immediate escalation
- **Regional data** (region: us-east) â†’ US-based Kafka cluster for low-latency processing
- **Regional data** (region: eu-west) â†’ EU Kafka cluster for GDPR compliance
- **High-volume telemetry** â†’ Local storage to avoid bandwidth costs
- **Everything else** â†’ Default cloud archive for long-term retention

Without content-based routing, you would either need to create separate pipelines for each data type (duplicating configuration and increasing maintenance burden) or send everything everywhere and filter at the destination (wasting bandwidth and destination resources). The switch output solves this by routing intelligently at the edge based on message content.

## Prerequisites

Before starting this guide, ensure you have:

- An Expanso edge node running and connected to the orchestrator
- Access credentials for your destination services

For background on Bloblang expressions used in routing conditions, see the [Bloblang Guide](https://docs.expanso.io/guides/bloblang).

## Solution Overview

The switch output evaluates messages against a series of conditional checks. When a message matches a condition, it's sent to the corresponding output. This happens at the edge, before any data leaves your network.

```
                   â”Œâ”€â†’ [severity=critical] â†’ PagerDuty API
                   â”œâ”€â†’ [region=us-east] â†’ US Kafka
Input â†’ Switch â”€â”€â”€â”€â”¼â”€â†’ [region=eu-west] â†’ EU Kafka
                   â”œâ”€â†’ [volume>1000] â†’ Local Storage
                   â””â”€â†’ [default] â†’ Cloud Archive
```

Key characteristics of content-based routing:

- **Conditional evaluation**: Each message is checked against conditions in order
- **First match wins**: When a condition matches, that output is used (no further checks)
- **Default fallback**: A final case with no condition catches everything else
- **Edge processing**: Routing decisions happen at the edge, reducing cloud bandwidth
- **Per-route configuration**: Each destination has independent batching, retry, and error handling

## Step-by-Step Implementation

### Step 1: Route by Severity Level

Let's start with a common pattern: routing log messages based on severity. Critical errors go to PagerDuty, warnings to Slack, and everything else to Elasticsearch.

Create a new pipeline configuration file called `severity-routing-pipeline.yaml`:

<CodeBlock language="yaml" title="severity-routing-pipeline.yaml" showLineNumbers>
  {pipelineYaml}
</CodeBlock>

<a
  href="/files/data-routing/content-routing.yaml"
  download
  className="button button--primary button--lg margin-top--md"
>
  ðŸ“¥ Download Pipeline
</a>

---


This configuration:

- Sends CRITICAL/FATAL errors immediately to PagerDuty with fast retries
- Batches warnings to Slack (avoiding rate limits)
- Routes all other logs to Elasticsearch for searchability
- Uses first-match semantics: critical errors never reach Slack or Elasticsearch

**Key Pattern:** The final case has no `check` condition, making it a default fallback that catches everything not matched by earlier cases.

### Step 2: Route by Geographic Region

Routing by region is critical for compliance (GDPR requires EU data stay in EU) and performance (low-latency local processing). Let's add geographic routing:

```yaml title="region-routing-pipeline.yaml"
name: region-based-routing
description: Route data to region-specific destinations for compliance and performance
type: pipeline
namespace: production

config:
  input:
    http_server:
      address: 0.0.0.0:8080
      path: /events
      timeout: 5s

  pipeline:
    processors:
      # Enrich with region metadata
      - mapping: |
          root = this

          # Extract region from event or environment
          root.region = this.region.or(env("EDGE_REGION")).or("unknown")

          # Validate region
          let valid_regions = ["us-east", "us-west", "eu-west", "ap-south"]
          if !valid_regions.contains(root.region) {
            root.region = "unknown"
          }

  output:
    switch:
      cases:
        # Case 1: EU data must stay in EU (GDPR compliance)
        - check: this.region == "eu-west"
          output:
            broker:
              pattern: fan_out
              outputs:
                # EU Kafka cluster
                - kafka:
                    addresses:
                      - eu-kafka-1.example.com:9092
                      - eu-kafka-2.example.com:9092
                    topic: eu-events
                    compression: snappy
                    batching:
                      count: 100
                      period: 1s

                    # EU-specific compliance settings
                    metadata:
                      include_prefixes:
                        - gdpr_

                # EU S3 bucket for archive
                - aws_s3:
                    bucket: eu-data-archive
                    region: eu-west-1
                    path: events/${!timestamp_unix_date("2006/01/02")}/events_${!count("files")}.jsonl
                    content_encoding: gzip
                    storage_class: INTELLIGENT_TIERING
                    batching:
                      count: 1000
                      period: 5m

        # Case 2: US East data to east cluster
        - check: this.region == "us-east"
          output:
            kafka:
              addresses:
                - us-east-kafka-1.example.com:9092
                - us-east-kafka-2.example.com:9092
              topic: us-east-events
              compression: snappy
              batching:
                count: 100
                period: 1s

        # Case 3: US West data to west cluster
        - check: this.region == "us-west"
          output:
            kafka:
              addresses:
                - us-west-kafka-1.example.com:9092
                - us-west-kafka-2.example.com:9092
              topic: us-west-events
              compression: snappy
              batching:
                count: 100
                period: 1s

        # Case 4: Asia-Pacific data to AP cluster
        - check: this.region == "ap-south"
          output:
            kafka:
              addresses:
                - ap-kafka-1.example.com:9092
                - ap-kafka-2.example.com:9092
              topic: ap-events
              compression: snappy
              batching:
                count: 100
                period: 1s

        # Default: Unknown region to error handling
        - output:
            broker:
              pattern: fan_out
              outputs:
                # Log to file for investigation
                - file:
                    path: /var/log/expanso/unknown-region-${!timestamp_unix()}.jsonl
                    codec: lines

                # Also send to central logging
                - http_client:
                    url: ${CENTRAL_LOGGING_URL}/unknown-region
                    verb: POST
                    headers:
                      Content-Type: application/json
```

This pattern:

- Enforces GDPR compliance by keeping EU data in EU-based systems
- Routes to the closest regional Kafka cluster for low latency
- Handles unknown regions by logging locally and alerting central systems
- Combines switch routing with fan-out (EU data goes to both Kafka and S3)

**Compliance Benefit:** Processing and routing decisions happen at the edge. Your edge node in Frankfurt routes EU data to EU systems without ever sending it to US-based infrastructure.

### Step 3: Route by Event Type

Different event types often need different processing. Let's route based on event type:

```yaml title="event-type-routing-pipeline.yaml"
name: event-type-routing
description: Route events based on type to specialized processors
type: pipeline
namespace: production

config:
  input:
    http_server:
      address: 0.0.0.0:8080
      path: /events
      timeout: 5s

  output:
    switch:
      cases:
        # Case 1: User authentication events
        - check: this.event_type == "user.login" || this.event_type == "user.logout" || this.event_type == "user.password_reset"
          output:
            broker:
              pattern: fan_out
              outputs:
                # Send to security monitoring
                - http_client:
                    url: ${SECURITY_MONITORING_URL}/auth-events
                    verb: POST
                    batching:
                      count: 50
                      period: 5s

                # Archive to S3 for audit trail
                - aws_s3:
                    bucket: security-audit-logs
                    path: auth-events/${!timestamp_unix_date("2006/01/02")}/auth_${!count("files")}.jsonl
                    content_encoding: gzip
                    # Long retention for compliance
                    storage_class: GLACIER_IR
                    batching:
                      count: 500
                      period: 10m

        # Case 2: Payment transactions
        - check: this.event_type == "payment.authorized" || this.event_type == "payment.captured" || this.event_type == "payment.failed"
          output:
            broker:
              pattern: fan_out
              outputs:
                # Send to payment processor
                - kafka:
                    addresses: [${KAFKA_BROKERS}]
                    topic: payment-events
                    # Exactly-once semantics for financial data
                    idempotent_write: true
                    ack_replicas: true
                    max_in_flight: 1
                    batching:
                      count: 100
                      period: 1s

                # Real-time fraud detection
                - http_client:
                    url: ${FRAUD_DETECTION_URL}/analyze
                    verb: POST
                    timeout: 2s
                    retry_as_batch: false
                    max_retries: 2

        # Case 3: High-volume telemetry
        - check: this.event_type.has_prefix("telemetry.")
          output:
            # Keep high-volume data local to save bandwidth
            file:
              path: /var/expanso/telemetry/telemetry-${!timestamp_unix_date("2006-01-02")}.jsonl
              codec: lines
              batching:
                count: 10000
                period: 1m

        # Case 4: Analytics events
        - check: this.event_type.has_prefix("analytics.")
          output:
            http_client:
              url: ${ANALYTICS_API_URL}/events
              verb: POST
              batching:
                count: 500
                period: 30s
              gzip_compression: true
              max_retries: 3

        # Default: Standard event processing
        - output:
            kafka:
              addresses: [${KAFKA_BROKERS}]
              topic: general-events
              batching:
                count: 200
                period: 5s
              compression: snappy
```

**Key Pattern:** This configuration uses `has_prefix()` to match event type families (e.g., all `telemetry.*` events), avoiding the need to list every specific type.

### Step 4: Priority Queue Routing

Let's implement a priority queue system where high-priority messages get expedited processing:

```yaml title="priority-routing-pipeline.yaml"
name: priority-queue-routing
description: Route messages to different queues based on priority
type: pipeline
namespace: production

config:
  input:
    http_server:
      address: 0.0.0.0:8080
      path: /messages
      timeout: 5s

  pipeline:
    processors:
      # Determine priority (default to normal)
      - mapping: |
          root = this

          # Set priority based on multiple factors
          root.priority = if this.priority.exists() {
            this.priority.lowercase()
          } else if this.user_tier.or("") == "premium" {
            "high"
          } else if this.message_type.or("") == "alert" {
            "critical"
          } else {
            "normal"
          }

          # Add priority score for metrics
          root.priority_score = match root.priority {
            "critical" => 100
            "high" => 75
            "normal" => 50
            "low" => 25
            _ => 0
          }

  output:
    switch:
      cases:
        # Case 1: Critical priority - immediate processing, no batching
        - check: this.priority == "critical"
          output:
            kafka:
              addresses: [${KAFKA_BROKERS}]
              topic: critical-queue

              # No batching for critical messages
              batching:
                count: 1
                period: 0s

              # Aggressive retries
              max_in_flight: 1
              ack_replicas: true
              max_retries: 10

              # Monitoring metadata
              metadata:
                include_patterns:
                  - "^priority.*"

        # Case 2: High priority - small batches, fast delivery
        - check: this.priority == "high"
          output:
            kafka:
              addresses: [${KAFKA_BROKERS}]
              topic: high-priority-queue

              # Small batches
              batching:
                count: 10
                period: 1s

              max_retries: 5

        # Case 3: Normal priority - standard batching
        - check: this.priority == "normal"
          output:
            kafka:
              addresses: [${KAFKA_BROKERS}]
              topic: normal-queue

              # Standard batching
              batching:
                count: 100
                period: 5s

              compression: snappy

        # Case 4: Low priority - large batches, deferred delivery
        - check: this.priority == "low"
          output:
            kafka:
              addresses: [${KAFKA_BROKERS}]
              topic: low-priority-queue

              # Large batches to reduce overhead
              batching:
                count: 1000
                period: 1m

              compression: gzip

        # Default: Treat as normal priority
        - output:
            kafka:
              addresses: [${KAFKA_BROKERS}]
              topic: normal-queue
              batching:
                count: 100
                period: 5s
```

This priority routing:

- **Critical messages**: No batching, aggressive retries, immediate delivery
- **High priority**: Small batches (10 messages), 1-second window
- **Normal priority**: Standard batching (100 messages), 5-second window
- **Low priority**: Large batches (1000 messages), 1-minute window

**Performance Impact:** By batching low-priority messages more aggressively, you reduce overhead on Kafka while ensuring critical messages get through immediately.

## Production Considerations

### Default Cases and Fallback Handling

**Always include a default case.** Without one, messages that don't match any condition will cause errors. Your default case should handle unexpected data gracefully:

```yaml
# Good: Explicit default with monitoring
- output:
    broker:
      pattern: fan_out
      outputs:
        # Log locally for investigation
        - file:
            path: /var/log/expanso/unrouted-${!timestamp_unix()}.jsonl

        # Alert monitoring
        - http_client:
            url: ${MONITORING_URL}/unrouted-events
            verb: POST
```

**Monitor your default case.** If it receives significant traffic, your routing logic may be incomplete or your data schema has changed.

### Condition Evaluation Order Matters

The switch output evaluates conditions **in order** and uses the **first match**. Order your cases from most specific to least specific:

```yaml
# Correct: Specific to general
cases:
  # Most specific
  - check: this.region == "eu-west" && this.severity == "CRITICAL"
    output: eu_critical_kafka

  # Less specific
  - check: this.region == "eu-west"
    output: eu_general_kafka

  # Least specific (default)
  - output: general_kafka
```

```yaml
# Incorrect: General before specific
cases:
  # This matches all EU events
  - check: this.region == "eu-west"
    output: eu_general_kafka

  # This will NEVER match because the previous case already caught it
  - check: this.region == "eu-west" && this.severity == "CRITICAL"
    output: eu_critical_kafka  # Dead code!
```

### Performance Optimization

**Bloblang conditions are fast, but not free.** For high-throughput pipelines (>10,000 events/sec), optimize your conditions:

```yaml
# Slow: Multiple string operations
- check: |
    this.event_type.lowercase().trim().contains("payment") &&
    this.severity.uppercase() == "CRITICAL"

# Fast: Pre-process in pipeline processors (runs once)
pipeline:
  processors:
    - mapping: |
        root = this
        root.event_type_normalized = this.event_type.lowercase().trim()
        root.severity_normalized = this.severity.uppercase()

# Then use simple comparisons in switch
output:
  switch:
    cases:
      - check: this.event_type_normalized.contains("payment") && this.severity_normalized == "CRITICAL"
```

### Edge Context: Bandwidth and Latency

Content-based routing is particularly powerful at the edge. Consider bandwidth costs when designing your routing:

**Before (everything to cloud):**
```
High-volume telemetry: 1 GB/hour â†’ Cloud
Critical alerts: 10 MB/hour â†’ Cloud
General events: 500 MB/hour â†’ Cloud

Total bandwidth: 1.51 GB/hour = 36.24 GB/day
Monthly cost (at $0.09/GB): $98
```

**After (content-based routing):**
```
High-volume telemetry: 1 GB/hour â†’ Local storage (0 bandwidth)
Critical alerts: 10 MB/hour â†’ Cloud (immediate delivery)
General events: 500 MB/hour â†’ Cloud (batched)

Total bandwidth: 510 MB/hour = 12.24 GB/day
Monthly cost (at $0.09/GB): $33

Savings: $65/month (66% reduction)
```

**Routing Strategy for Edge:**

```yaml
# Keep high-volume, low-value data local
- check: this.event_type.has_prefix("telemetry.") && this.priority == "low"
  output:
    file:
      path: /var/expanso/local-data/telemetry.jsonl

# Send only critical data to cloud immediately
- check: this.severity == "CRITICAL"
  output:
    kafka:
      addresses: [cloud-kafka.example.com:9092]
      batching:
        count: 1  # No batching for critical

# Batch everything else aggressively
- output:
    kafka:
      addresses: [cloud-kafka.example.com:9092]
      batching:
        count: 1000
        period: 5m
```

### Nested Switch Outputs

For complex routing, you can nest switch outputs:

```yaml
output:
  switch:
    cases:
      # First level: Route by region
      - check: this.region == "eu-west"
        output:
          switch:
            # Second level: Route by event type within EU
            cases:
              - check: this.event_type == "payment"
                output:
                  kafka:
                    topic: eu-payments

              - check: this.event_type == "analytics"
                output:
                  kafka:
                    topic: eu-analytics

              - output:
                  kafka:
                    topic: eu-general
```

**Caution:** Deep nesting (>2 levels) becomes hard to maintain. Consider splitting into multiple pipelines instead.

## Verification

### Test Message Routing

Send test events to verify correct routing:

```bash title="Test severity-based routing"
# Test critical event
curl -X POST http://localhost:8080/events/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "event_id": "test-critical-001",
    "timestamp": "2025-10-20T10:00:00Z",
    "severity": "CRITICAL",
    "message": "Test critical alert",
    "region": "us-east"
  }'

# Test normal event
curl -X POST http://localhost:8080/events/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "event_id": "test-normal-001",
    "timestamp": "2025-10-20T10:00:00Z",
    "severity": "INFO",
    "message": "Test normal event",
    "region": "us-east"
  }'
```

### Verify Destination Delivery

**Check Kafka:**

```bash
# Verify critical event reached critical queue
kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic critical-queue \
  --from-beginning \
  | grep "test-critical-001"

# Verify normal event reached normal queue
kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic normal-queue \
  --from-beginning \
  | grep "test-normal-001"
```

**Check PagerDuty:**

```bash
# Query PagerDuty API for triggered incidents
curl -X GET "https://api.pagerduty.com/incidents" \
  -H "Authorization: Token ${PAGERDUTY_API_TOKEN}" \
  -H "Accept: application/vnd.pagerduty+json;version=2" \
  | jq '.incidents[] | select(.title | contains("test-critical-001"))'
```

## Troubleshooting

### Problem: Messages Not Routing to Expected Destination

**Symptom:** All messages go to the default route, even though they should match earlier conditions.

**Cause:** Condition syntax error or field name mismatch.

**Solution:** Check pipeline logs for condition errors and enable debug logging.

**Common issues:**

```yaml
# Wrong: Field doesn't exist
- check: this.severitty == "CRITICAL"  # Typo!

# Correct:
- check: this.severity == "CRITICAL"

# Wrong: Case sensitivity
- check: this.severity == "critical"  # Won't match "CRITICAL"

# Correct: Normalize first
pipeline:
  processors:
    - mapping: |
        root = this
        root.severity = this.severity.uppercase()

# Then check:
- check: this.severity == "CRITICAL"
```

### Problem: High Latency in Condition Evaluation

**Symptom:** Pipeline shows high latency in metrics.

**Cause:** Complex Bloblang conditions or inefficient string operations.

**Solution:** Pre-process complex conditions in pipeline processors instead of in switch conditions.

### Problem: Default Route Receiving Unexpected Traffic

**Symptom:** Many messages falling through to default route.

**Cause:** Data schema changed, or new event types not covered by routing logic.

**Solution:** Examine messages in the default route to identify new patterns, then add cases for them.

## See Also

**Related Guides:**
- [Send Data to Multiple Destinations](https://examples.expanso.io/data-routing/fan-out-pattern) - Fan-out pattern for broadcasting
- [Create Priority Queues](https://examples.expanso.io/data-routing/priority-queues) - Priority-based routing patterns

**Component Reference:**
- [Switch Output](https://docs.expanso.io/components/outputs/switch) - Complete switch configuration options
- [Bloblang Guide](https://docs.expanso.io/guides/bloblang) - Conditional expression syntax
- [Kafka Output](https://docs.expanso.io/components/outputs/kafka) - Kafka configuration
- [HTTP Client Output](https://docs.expanso.io/components/outputs/http_client) - HTTP output configuration

---
title: Step 4 - Production Monitoring & Observability
sidebar_label: Step 4 - Production Monitoring
sidebar_position: 6
description: Add comprehensive monitoring, alerting, and observability for circuit breaker systems
keywords: [monitoring, observability, metrics, alerting, production, circuit-breaker]
---

# Step 4: Production Monitoring & Observability

Production circuit breaker systems require comprehensive monitoring to detect failures early, understand system behavior, and ensure rapid recovery. Without proper observability, circuit breakers can mask underlying problems or fail silently, leaving you blind to system health.

In this step, you'll implement production-grade monitoring for circuit breaker systems using metrics, logging, alerting, and dashboards that provide complete visibility into circuit state, failure patterns, and recovery behavior.

## Understanding Circuit Breaker Observability Requirements

### Key Metrics to Monitor

**Circuit Breaker State Metrics:**
- **Circuit state transitions:** Open → Half-Open → Closed
- **Failure rates:** Percentage of requests failing over time
- **Recovery times:** How long circuits stay open before recovery
- **Success/failure counts:** Total attempts vs successful completions

**Performance Metrics:**
- **Latency distributions:** Response times for successful requests
- **Timeout frequencies:** How often requests time out
- **Retry patterns:** Number of retries before success/failure
- **Throughput impact:** Requests per second before/during/after failures

**Fallback Metrics:**
- **Fallback activation rates:** How often each fallback level is used
- **Data loss tracking:** Events dropped vs processed
- **Buffer utilization:** Local storage usage during outages
- **Recovery success rates:** Percentage of buffered events successfully recovered

### Alerting Strategies

**Immediate Alerts (< 1 minute):**
- Circuit breaker opened (critical services)
- Multiple circuits open simultaneously
- Fallback systems failing
- Data loss threshold exceeded

**Warning Alerts (< 5 minutes):**
- Elevated failure rates
- Increased latency patterns
- Buffer capacity warnings
- Recovery process delays

**Trend Alerts (< 30 minutes):**
- Degraded service patterns
- Resource consumption trends
- Circuit flapping behavior
- SLA threshold approaching

## Implementation: Production Monitoring Pipeline

Let's implement a comprehensive monitoring system for circuit breaker operations.

### Pipeline Configuration with Monitoring

Create `monitored-circuit-breaker.yaml`:

```yaml title="monitored-circuit-breaker.yaml"
name: monitored-circuit-breaker
description: Circuit breaker system with comprehensive production monitoring
type: pipeline
namespace: circuit-breaker-production

config:
  input:
    http_server:
      address: 0.0.0.0:8092
      path: /monitored-events
      timeout: 30s

  pipeline:
    processors:
      # Step 1: Initialize monitoring context
      - mapping: |
          root = this
          
          # Add monitoring metadata
          root.monitoring = {
            "request_id": uuid_v4(),
            "pipeline_instance": env("HOSTNAME") || "unknown",
            "processing_start": now(),
            "circuit_breaker_version": "v1.0",
            "monitoring_enabled": true
          }
          
          # Validate required fields
          if !this.event_id.exists() {
            throw("missing required field: event_id")
          }

      # Step 2: Pre-processing metrics
      - metric:
          type: counter
          name: circuit_breaker_requests_total
          labels:
            pipeline: monitored-circuit-breaker
            event_type: ${!this.event_type || "unknown"}
            instance: ${!env("HOSTNAME") || "unknown"}

      # Step 3: Primary service call with detailed monitoring
      - try:
          # Record attempt
          - metric:
              type: counter  
              name: circuit_breaker_attempts_total
              labels:
                service: primary_service
                attempt_type: initial

          # Time the request
          - mapping: |
              root = this
              root.primary_service_start = now()

          # Primary HTTP call
          - http:
              url: ${PRIMARY_SERVICE_URL:-http://localhost:8081}/process
              verb: POST
              timeout: 5s
              retries: 3
              retry_period: 2s
              
              headers:
                X-Request-ID: ${!this.monitoring.request_id}
                X-Circuit-Breaker-Monitor: "enabled"

          # Record success metrics
          - mapping: |
              root = this
              root.primary_service_duration = (now() - this.primary_service_start).seconds()
              root.primary_service_success = true

          - metric:
              type: counter
              name: circuit_breaker_success_total
              labels:
                service: primary_service
                
          - metric:
              type: histogram
              name: circuit_breaker_duration_seconds
              value: ${!this.primary_service_duration}
              labels:
                service: primary_service

          # Record latency bucket for SLA tracking
          - switch:
              cases:
                - check: this.primary_service_duration < 1.0
                  processors:
                    - metric:
                        type: counter
                        name: circuit_breaker_sla_bucket_total
                        labels:
                          service: primary_service
                          bucket: under_1s

                - check: this.primary_service_duration < 3.0
                  processors:
                    - metric:
                        type: counter
                        name: circuit_breaker_sla_bucket_total
                        labels:
                          service: primary_service
                          bucket: 1s_to_3s

                - processors:
                    - metric:
                        type: counter
                        name: circuit_breaker_sla_bucket_total
                        labels:
                          service: primary_service
                          bucket: over_3s

        # Handle primary service failure
        catch:
          - mapping: |
              root = this
              root.primary_service_success = false
              root.primary_service_error = error().string()
              
              # Classify error type
              error_msg = error().string()
              if error_msg.contains("timeout") {
                root.primary_error_type = "timeout"
              } else if error_msg.contains("connection") {
                root.primary_error_type = "connection"
              } else if error_msg.contains("5") && error_msg.contains("server") {
                root.primary_error_type = "server_error"
              } else {
                root.primary_error_type = "unknown"
              }

          # Record failure metrics
          - metric:
              type: counter
              name: circuit_breaker_failures_total
              labels:
                service: primary_service
                error_type: ${!this.primary_error_type}

          - metric:
              type: counter
              name: circuit_breaker_opened_total
              labels:
                service: primary_service
                trigger: ${!this.primary_error_type}

          # Log circuit breaker activation
          - log:
              level: WARN
              message: "Circuit breaker opened for primary service - Request ID: ${!this.monitoring.request_id}, Error: ${!this.primary_error_type}"

      # Step 4: Secondary service with monitoring (if primary failed)
      - switch:
          cases:
            - check: this.primary_service_success != true
              processors:
                - try:
                    # Record secondary attempt
                    - metric:
                        type: counter
                        name: circuit_breaker_attempts_total
                        labels:
                          service: secondary_service
                          attempt_type: fallback

                    - mapping: |
                        root = this
                        root.secondary_service_start = now()

                    # Secondary HTTP call
                    - http:
                        url: ${SECONDARY_SERVICE_URL:-http://localhost:8082}/process
                        verb: POST
                        timeout: 8s
                        retries: 2
                        retry_period: 3s

                    # Record secondary success
                    - mapping: |
                        root = this
                        root.secondary_service_duration = (now() - this.secondary_service_start).seconds()
                        root.secondary_service_success = true
                        root.final_destination = "secondary_service"

                    - metric:
                        type: counter
                        name: circuit_breaker_success_total
                        labels:
                          service: secondary_service

                    - metric:
                        type: histogram
                        name: circuit_breaker_duration_seconds
                        value: ${!this.secondary_service_duration}
                        labels:
                          service: secondary_service

                    - log:
                        level: INFO
                        message: "Successfully failed over to secondary service - Request ID: ${!this.monitoring.request_id}"

                  # Secondary service also failed
                  catch:
                    - mapping: |
                        root = this
                        root.secondary_service_success = false
                        root.secondary_service_error = error().string()
                        root.both_services_failed = true

                    - metric:
                        type: counter
                        name: circuit_breaker_failures_total
                        labels:
                          service: secondary_service

                    - metric:
                        type: counter
                        name: circuit_breaker_cascade_failure_total
                        labels:
                          primary_error: ${!this.primary_error_type}

                    - log:
                        level: ERROR
                        message: "CRITICAL: Both primary and secondary services failed - Request ID: ${!this.monitoring.request_id}"

      # Step 5: Add comprehensive monitoring summary
      - mapping: |
          root = this
          root.processing_end = now()
          root.total_processing_time = (this.processing_end - this.monitoring.processing_start).seconds()
          
          # Calculate success status
          if this.primary_service_success == true {
            root.processing_result = "success_primary"
            root.success = true
          } else if this.secondary_service_success == true {
            root.processing_result = "success_secondary" 
            root.success = true
          } else {
            root.processing_result = "failure_both"
            root.success = false
          }
          
          # Add monitoring summary
          root.monitoring_summary = {
            "total_duration": this.total_processing_time,
            "primary_attempted": true,
            "primary_success": this.primary_service_success || false,
            "secondary_attempted": this.primary_service_success != true,
            "secondary_success": this.secondary_service_success || false,
            "circuit_breaker_activated": this.primary_service_success != true,
            "cascade_failure": this.both_services_failed || false
          }

      # Step 6: Record final metrics
      - metric:
          type: counter
          name: circuit_breaker_processing_total
          labels:
            result: ${!this.processing_result}
            
      - metric:
          type: histogram
          name: circuit_breaker_total_duration_seconds
          value: ${!this.total_processing_time}

      # Step 7: Health check for service recovery monitoring
      - switch:
          cases:
            # If primary failed, schedule health check
            - check: this.primary_service_success != true
              processors:
                - mapping: |
                    root = this
                    root.health_check_scheduled = true
                    root.health_check_service = "primary_service"

  # Output routing with monitoring
  output:
    switch:
      cases:
        # Successful processing
        - check: this.success == true
          output:
            processors:
              - metric:
                  type: counter
                  name: circuit_breaker_output_total
                  labels:
                    destination: success_queue
                    
            stdout:
              codec: lines

        # Failed processing - route to DLQ with detailed metrics
        - output:
            processors:
              - metric:
                  type: counter
                  name: circuit_breaker_output_total
                  labels:
                    destination: dead_letter_queue
                    
              - metric:
                  type: counter
                  name: circuit_breaker_data_loss_total
                  labels:
                    event_type: ${!this.event_type || "unknown"}

              - log:
                  level: ERROR
                  message: "Event sent to DLQ due to circuit breaker failures - Request ID: ${!this.monitoring.request_id}, Summary: ${!this.monitoring_summary.format_json()}"

            file:
              path: /tmp/circuit-breaker-dlq/${!timestamp_unix_date("2006-01-02")}/failed-events.jsonl
```

### Health Check and Recovery Monitoring

Create a separate pipeline to monitor service health and track recovery:

Create `circuit-breaker-health-monitor.yaml`:

```yaml title="circuit-breaker-health-monitor.yaml"
name: circuit-breaker-health-monitor
description: Continuous health monitoring for circuit breaker protected services
type: pipeline
namespace: circuit-breaker-production

config:
  input:
    # Health check on a schedule
    generate:
      interval: 30s  # Check every 30 seconds
      mapping: |
        root = {
          "health_check_id": uuid_v4(),
          "timestamp": now(),
          "check_type": "scheduled"
        }

  pipeline:
    processors:
      # Check primary service health
      - try:
          - http:
              url: ${PRIMARY_SERVICE_URL:-http://localhost:8081}/health
              verb: GET
              timeout: 2s
              retries: 1

          - mapping: |
              root = this
              root.primary_service_health = "healthy"

          - metric:
              type: gauge
              name: circuit_breaker_service_health
              value: 1
              labels:
                service: primary_service

        catch:
          - mapping: |
              root = this
              root.primary_service_health = "unhealthy"
              root.primary_service_error = error().string()

          - metric:
              type: gauge
              name: circuit_breaker_service_health
              value: 0
              labels:
                service: primary_service

      # Check secondary service health
      - try:
          - http:
              url: ${SECONDARY_SERVICE_URL:-http://localhost:8082}/health
              verb: GET
              timeout: 2s
              retries: 1

          - mapping: |
              root = this
              root.secondary_service_health = "healthy"

          - metric:
              type: gauge
              name: circuit_breaker_service_health
              value: 1
              labels:
                service: secondary_service

        catch:
          - mapping: |
              root = this
              root.secondary_service_health = "unhealthy"
              root.secondary_service_error = error().string()

          - metric:
              type: gauge
              name: circuit_breaker_service_health
              value: 0
              labels:
                service: secondary_service

      # Calculate overall system health
      - mapping: |
          root = this
          
          if this.primary_service_health == "healthy" && this.secondary_service_health == "healthy" {
            root.system_health = "fully_operational"
            root.system_health_score = 1.0
          } else if this.primary_service_health == "healthy" || this.secondary_service_health == "healthy" {
            root.system_health = "degraded"
            root.system_health_score = 0.5
          } else {
            root.system_health = "critical"
            root.system_health_score = 0.0
          }

      - metric:
          type: gauge
          name: circuit_breaker_system_health_score
          value: ${!this.system_health_score}

      # Alert on health changes
      - switch:
          cases:
            - check: this.system_health == "critical"
              processors:
                - log:
                    level: FATAL
                    message: "CRITICAL: Both primary and secondary services are unhealthy"

                - metric:
                    type: counter
                    name: circuit_breaker_critical_alerts_total

            - check: this.system_health == "degraded" 
              processors:
                - log:
                    level: WARN
                    message: "DEGRADED: One service is unhealthy - Primary: ${!this.primary_service_health}, Secondary: ${!this.secondary_service_health}"

  output:
    stdout:
      codec: lines
```

### Circuit Breaker Metrics Dashboard

Create a metrics collection pipeline for dashboard visualization:

Create `circuit-breaker-metrics-collector.yaml`:

```yaml title="circuit-breaker-metrics-collector.yaml"
name: circuit-breaker-metrics-collector
description: Collect and aggregate circuit breaker metrics for dashboards
type: pipeline
namespace: circuit-breaker-production

config:
  input:
    # Collect metrics every minute
    generate:
      interval: 1m
      mapping: |
        root = {
          "collection_id": uuid_v4(),
          "timestamp": now(),
          "collection_period": "1m"
        }

  pipeline:
    processors:
      # Calculate derived metrics from basic counters
      - mapping: |
          root = this
          
          # In production, these would come from actual metrics store
          # Simulating metrics calculation
          
          current_time = now()
          
          # Circuit breaker state metrics
          root.metrics = {
            "total_requests": 1000,
            "successful_requests": 850,
            "failed_requests": 150,
            "circuit_opens": 5,
            "fallback_activations": 45,
            "recovery_events": 4
          }
          
          # Calculate rates and percentages
          total = this.metrics.total_requests
          if total > 0 {
            root.calculated_metrics = {
              "success_rate": (this.metrics.successful_requests / total) * 100,
              "failure_rate": (this.metrics.failed_requests / total) * 100,
              "fallback_rate": (this.metrics.fallback_activations / total) * 100,
              "circuit_open_rate": (this.metrics.circuit_opens / total) * 100,
              "mean_time_to_recovery": 45.5  # seconds
            }
          }

      # Generate SLA compliance metrics
      - mapping: |
          root = this
          
          # SLA targets
          sla_success_rate_target = 99.5  # 99.5% success rate
          sla_latency_target = 2.0        # 2 second response time
          
          success_rate = this.calculated_metrics.success_rate
          
          root.sla_compliance = {
            "success_rate_target": sla_success_rate_target,
            "current_success_rate": success_rate,
            "success_rate_compliance": success_rate >= sla_success_rate_target,
            "latency_target": sla_latency_target,
            "current_p95_latency": 1.8,  # Would come from actual metrics
            "latency_compliance": true,
            "overall_compliance": success_rate >= sla_success_rate_target
          }

      # Record dashboard metrics
      - metric:
          type: gauge
          name: circuit_breaker_dashboard_success_rate
          value: ${!this.calculated_metrics.success_rate}

      - metric:
          type: gauge
          name: circuit_breaker_dashboard_failure_rate
          value: ${!this.calculated_metrics.failure_rate}

      - metric:
          type: gauge
          name: circuit_breaker_sla_compliance
          value: ${!this.sla_compliance.overall_compliance}

      # Generate alerts for SLA violations
      - switch:
          cases:
            - check: this.sla_compliance.success_rate_compliance != true
              processors:
                - log:
                    level: ERROR
                    message: "SLA VIOLATION: Success rate ${!this.calculated_metrics.success_rate}% below target ${!this.sla_compliance.success_rate_target}%"

                - metric:
                    type: counter
                    name: circuit_breaker_sla_violations_total
                    labels:
                      type: success_rate

      # Log comprehensive metrics summary
      - log:
          level: INFO
          message: "Circuit Breaker Metrics - Success Rate: ${!this.calculated_metrics.success_rate}%, Fallback Rate: ${!this.calculated_metrics.fallback_rate}%, Circuit Opens: ${!this.metrics.circuit_opens}"

  output:
    # Send metrics to monitoring system (simulate with stdout)
    stdout:
      codec: lines
```

## Advanced Monitoring Patterns

### Pattern 1: Anomaly Detection for Circuit Breakers

Implement statistical anomaly detection to predict circuit breaker failures:

```yaml title="circuit-breaker-anomaly-detection.yaml"
name: circuit-breaker-anomaly-detection
description: Detect anomalies in circuit breaker behavior patterns
type: pipeline
namespace: circuit-breaker-production

config:
  cache_resources:
    - label: metrics_history
      memory:
        cap: 1000
        default_ttl: 1h

  input:
    generate:
      interval: 10s
      mapping: |
        root = {
          "check_timestamp": now(),
          "check_type": "anomaly_detection"
        }

  pipeline:
    processors:
      # Get recent metrics (simulate)
      - mapping: |
          root = this
          
          # In production, would query actual metrics
          current_metrics = {
            "requests_per_second": 50 + (rand() % 20 - 10),  # 40-60 rps with noise
            "error_rate": 2.0 + (rand() % 5),                # 2-7% error rate 
            "avg_latency": 150 + (rand() % 50),              # 150-200ms latency
            "circuit_opens_per_minute": rand() % 3           # 0-2 opens per minute
          }
          
          root.current = current_metrics

      # Get historical baseline from cache
      - cache:
          resource: metrics_history
          operator: get
          key: baseline_metrics

      # Calculate anomaly scores
      - mapping: |
          root = this
          
          # Use cached baseline or initialize
          if this.cached_data.exists() {
            baseline = this.cached_data.parse_json()
          } else {
            baseline = {
              "avg_requests_per_second": 50,
              "avg_error_rate": 2.5,
              "avg_latency": 175,
              "avg_circuit_opens": 0.5
            }
          }
          
          # Calculate deviations from baseline
          rps_deviation = abs(this.current.requests_per_second - baseline.avg_requests_per_second) / baseline.avg_requests_per_second
          error_deviation = abs(this.current.error_rate - baseline.avg_error_rate) / baseline.avg_error_rate
          latency_deviation = abs(this.current.avg_latency - baseline.avg_latency) / baseline.avg_latency
          
          root.anomaly_scores = {
            "rps_deviation": rps_deviation,
            "error_deviation": error_deviation, 
            "latency_deviation": latency_deviation,
            "max_deviation": max([rps_deviation, error_deviation, latency_deviation])
          }
          
          # Overall anomaly score (0-1, higher = more anomalous)
          root.anomaly_score = this.anomaly_scores.max_deviation

      # Alert on anomalies
      - switch:
          cases:
            - check: this.anomaly_score > 0.5
              processors:
                - log:
                    level: WARN
                    message: "ANOMALY DETECTED: Circuit breaker metrics anomaly score ${!this.anomaly_score} - Investigate potential issues"

                - metric:
                    type: counter
                    name: circuit_breaker_anomalies_detected_total
                    labels:
                      severity: ${!this.anomaly_score > 0.8 ? "high" : "medium"}

      # Update baseline (exponential moving average)
      - mapping: |
          root = this
          alpha = 0.1  # Smoothing factor
          
          # Update baseline with current metrics
          if this.cached_data.exists() {
            baseline = this.cached_data.parse_json()
          } else {
            baseline = {
              "avg_requests_per_second": 50,
              "avg_error_rate": 2.5, 
              "avg_latency": 175,
              "avg_circuit_opens": 0.5
            }
          }
          
          root.new_baseline = {
            "avg_requests_per_second": baseline.avg_requests_per_second * (1 - alpha) + this.current.requests_per_second * alpha,
            "avg_error_rate": baseline.avg_error_rate * (1 - alpha) + this.current.error_rate * alpha,
            "avg_latency": baseline.avg_latency * (1 - alpha) + this.current.avg_latency * alpha,
            "avg_circuit_opens": baseline.avg_circuit_opens * (1 - alpha) + this.current.circuit_opens_per_minute * alpha
          }

      # Store updated baseline
      - cache:
          resource: metrics_history
          operator: set
          key: baseline_metrics
          value: ${!this.new_baseline.format_json()}

  output:
    stdout:
      codec: lines
```

### Pattern 2: Circuit Breaker State Machine Tracking

Track detailed circuit breaker state transitions:

```yaml title="circuit-breaker-state-tracker.yaml"
name: circuit-breaker-state-tracker
description: Track detailed circuit breaker state transitions and patterns
type: pipeline
namespace: circuit-breaker-production

config:
  cache_resources:
    - label: circuit_states
      memory:
        cap: 100
        default_ttl: 30m

  input:
    # Listen for circuit breaker state change events
    http_server:
      address: 0.0.0.0:8093
      path: /circuit-state-change

  pipeline:
    processors:
      # Validate state change event
      - mapping: |
          root = this
          
          if !this.circuit_id.exists() {
            throw("missing circuit_id")
          }
          
          if !this.new_state.exists() || !["closed", "open", "half_open"].contains(this.new_state) {
            throw("invalid new_state: must be closed, open, or half_open")
          }
          
          root.state_change_timestamp = now()

      # Get previous state from cache
      - cache:
          resource: circuit_states
          operator: get
          key: circuit_${!this.circuit_id}

      # Analyze state transition
      - mapping: |
          root = this
          
          if this.cached_data.exists() {
            previous = this.cached_data.parse_json()
            root.previous_state = previous.state
            root.previous_timestamp = previous.timestamp
            root.time_in_previous_state = (now() - previous.timestamp.parse_timestamp()).seconds()
          } else {
            root.previous_state = "unknown"
            root.time_in_previous_state = null
          }
          
          root.transition = root.previous_state + "_to_" + this.new_state
          
          # Calculate transition validity
          valid_transitions = [
            "closed_to_open",
            "open_to_half_open", 
            "half_open_to_closed",
            "half_open_to_open",
            "unknown_to_closed",
            "unknown_to_open"
          ]
          
          root.valid_transition = valid_transitions.contains(this.transition)

      # Record state transition metrics
      - metric:
          type: counter
          name: circuit_breaker_state_transitions_total
          labels:
            circuit_id: ${!this.circuit_id}
            transition: ${!this.transition}
            valid: ${!this.valid_transition}

      # Record time in state metrics
      - switch:
          cases:
            - check: this.time_in_previous_state != null
              processors:
                - metric:
                    type: histogram
                    name: circuit_breaker_state_duration_seconds
                    value: ${!this.time_in_previous_state}
                    labels:
                      circuit_id: ${!this.circuit_id}
                      state: ${!this.previous_state}

      # Alert on concerning patterns
      - switch:
          cases:
            # Circuit flapping (too frequent state changes)
            - check: this.time_in_previous_state != null && this.time_in_previous_state < 30
              processors:
                - log:
                    level: WARN
                    message: "CIRCUIT FLAPPING: Circuit ${!this.circuit_id} changed state after only ${!this.time_in_previous_state} seconds"

                - metric:
                    type: counter
                    name: circuit_breaker_flapping_total
                    labels:
                      circuit_id: ${!this.circuit_id}

            # Invalid state transition
            - check: this.valid_transition != true
              processors:
                - log:
                    level: ERROR
                    message: "INVALID TRANSITION: Circuit ${!this.circuit_id} invalid transition ${!this.transition}"

            # Circuit stuck open (concerning pattern)  
            - check: this.previous_state == "open" && this.time_in_previous_state > 300
              processors:
                - log:
                    level: ERROR
                    message: "CIRCUIT STUCK: Circuit ${!this.circuit_id} has been open for ${!this.time_in_previous_state} seconds"

      # Update state cache
      - cache:
          resource: circuit_states
          operator: set
          key: circuit_${!this.circuit_id}
          value: |
            {
              "state": "${!this.new_state}",
              "timestamp": "${!this.state_change_timestamp}",
              "previous_state": "${!this.previous_state}",
              "transition": "${!this.transition}"
            }

  output:
    stdout:
      codec: lines
```

## Production Deployment and Testing

Deploy the complete monitoring system:

```bash
# Deploy all monitoring pipelines
expanso apply -f monitored-circuit-breaker.yaml
expanso apply -f circuit-breaker-health-monitor.yaml  
expanso apply -f circuit-breaker-metrics-collector.yaml
expanso apply -f circuit-breaker-anomaly-detection.yaml
expanso apply -f circuit-breaker-state-tracker.yaml

# Verify deployments
expanso get pipelines -n circuit-breaker-production
```

Test the monitoring system:

```bash
# Generate test events to create metrics
for i in {1..10}; do
  curl -X POST http://localhost:8092/monitored-events \
    -H "Content-Type: application/json" \
    -d "{
      \"event_id\": \"test_$i\",
      \"event_type\": \"test\",
      \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"
    }"
  sleep 2
done

# Check monitoring logs
expanso logs pipeline monitored-circuit-breaker --tail=20
expanso logs pipeline circuit-breaker-health-monitor --tail=10
expanso logs pipeline circuit-breaker-metrics-collector --tail=5

# Test state tracking
curl -X POST http://localhost:8093/circuit-state-change \
  -H "Content-Type: application/json" \
  -d '{
    "circuit_id": "primary_service_circuit",
    "new_state": "open",
    "reason": "timeout_threshold_exceeded"
  }'
```

## Troubleshooting Production Monitoring

### Issue: Missing Metrics

**Symptoms:**
- Dashboards show no data
- Metrics not being recorded
- Monitoring alerts not firing

**Diagnosis:**
```bash
# Check if metrics pipelines are running
expanso get pipeline -l app=circuit-breaker-monitoring

# Check pipeline logs for errors
expanso logs pipeline monitored-circuit-breaker | grep -i error

# Verify metric processor configuration
expanso get pipeline monitored-circuit-breaker -o yaml | grep -A5 metric
```

**Solutions:**

1. **Verify metric processor syntax:**
```yaml
- metric:
    type: counter          # Ensure valid type
    name: valid_metric_name # No spaces or special chars
    labels:
      service: ${!this.service_name}  # Ensure variable exists
```

2. **Add metric debugging:**
```yaml
- log:
    level: DEBUG
    message: "Recording metric: ${!this.metric_name} = ${!this.metric_value}"
    
- metric:
    type: counter
    name: ${!this.metric_name}
```

### Issue: Alert Fatigue

**Symptoms:**
- Too many alerts being generated
- Important alerts getting lost in noise
- Alert storm during outages

**Solutions:**

1. **Implement alert throttling:**
```yaml
# Add rate limiting to alerts
cache_resources:
  - label: alert_throttle
    memory:
      cap: 1000
      default_ttl: 5m

processors:
  - cache:
      resource: alert_throttle
      operator: get
      key: alert_${!this.alert_type}

  - switch:
      cases:
        - check: this.cached_data.exists()
          processors:
            - log:
                level: DEBUG
                message: "Alert throttled: ${!this.alert_type}"
            drop: {}

        # Send alert and cache to throttle
        - processors:
            - log:
                level: WARN  
                message: "ALERT: ${!this.alert_message}"

            - cache:
                resource: alert_throttle
                operator: set
                key: alert_${!this.alert_type}
                value: "sent"
```

2. **Implement alert severity levels:**
```yaml
- switch:
    cases:
      - check: this.severity == "critical"
        processors:
          - log:
              level: FATAL
              message: "CRITICAL ALERT: ${!this.message}"

      - check: this.severity == "warning"  
        processors:
          - log:
              level: WARN
              message: "WARNING: ${!this.message}"
```

## What You've Learned

You've successfully implemented production-grade monitoring that:

✅ **Tracks comprehensive circuit breaker metrics** including state transitions, failure rates, and recovery times
✅ **Provides real-time health monitoring** with automated service health checks
✅ **Implements anomaly detection** to predict failures before they occur
✅ **Includes SLA compliance tracking** with automatic violation alerting  
✅ **Monitors state machine behavior** to detect circuit flapping and stuck states
✅ **Provides actionable alerts** with proper severity levels and throttling
✅ **Supports dashboard visualization** with aggregated metrics and KPIs

This monitoring system gives you complete visibility into circuit breaker behavior, enabling proactive issue detection and faster incident resolution.

---

**Next:** [Complete Circuit Breaker Pipeline](./complete-circuit-breakers) - Deploy the complete production-ready circuit breaker system with all components integrated

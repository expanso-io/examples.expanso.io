---
title: Circuit Breaker Troubleshooting Guide
sidebar_label: Troubleshooting
sidebar_position: 8
description: Comprehensive troubleshooting guide for circuit breaker issues, common problems, and solutions
keywords: [troubleshooting, debugging, circuit-breaker, problems, solutions]
---

# Circuit Breaker Troubleshooting Guide

This comprehensive troubleshooting guide covers common circuit breaker issues, diagnostic techniques, and solutions for production problems. Use this guide to quickly identify and resolve circuit breaker failures.

## Quick Diagnostics

### Circuit Breaker Health Check

Run this diagnostic script to check overall circuit breaker health:

```bash
#!/bin/bash
# circuit-breaker-health-check.sh

echo "=== Circuit Breaker Health Check ==="

# Check pipeline status
echo "1. Pipeline Status:"
expanso get pipeline circuit-breakers-complete 2>/dev/null || echo "❌ Pipeline not found"

# Check recent logs for errors
echo -e "\n2. Recent Error Logs:"
expanso logs pipeline circuit-breakers-complete --tail=50 2>/dev/null | grep -i error | tail -5

# Check if services are responding
echo -e "\n3. Service Health:"
curl -s --max-time 3 "${PRIMARY_API_URL:-http://localhost:8081}/health" > /dev/null && echo "✅ Primary API responding" || echo "❌ Primary API not responding"
curl -s --max-time 3 "${SECONDARY_API_URL:-http://localhost:8082}/health" > /dev/null && echo "✅ Secondary API responding" || echo "❌ Secondary API not responding"

# Check local buffers
echo -e "\n4. Local Buffer Status:"
if [ -d "/tmp/circuit-breaker-buffers" ]; then
  BUFFER_SIZE=$(du -sh /tmp/circuit-breaker-buffers 2>/dev/null | cut -f1)
  echo "Local buffer size: ${BUFFER_SIZE}"
  BUFFER_FILES=$(find /tmp/circuit-breaker-buffers -name "*.jsonl" 2>/dev/null | wc -l)
  echo "Buffered files: ${BUFFER_FILES}"
else
  echo "No local buffers found"
fi

# Check metrics
echo -e "\n5. Recent Metrics:"
echo "Use: expanso logs pipeline circuit-breakers-complete | grep metric"

echo -e "\nHealth check complete. See individual sections below for detailed troubleshooting."
```

---

## Configuration Issues

### Issue: Circuit Breaker Never Opens

**Symptoms:**
- Services are clearly failing but circuit breaker doesn't activate
- Requests continue to wait for full timeout period
- No fallback behavior triggered
- High resource consumption during failures

**Diagnosis:**

```bash
# Check circuit breaker configuration
expanso get pipeline circuit-breakers-complete -o yaml | grep -A10 -B5 -E "(retry|timeout|fallback)"

# Check if try/catch blocks are properly configured
expanso get pipeline circuit-breakers-complete -o yaml | grep -A5 -E "(try|catch)"

# Monitor request flow
expanso logs pipeline circuit-breakers-complete --tail=100 | grep -E "(attempt|retry|timeout|fallback)"
```

**Solutions:**

1. **Ensure finite retry configuration:**
```yaml
# WRONG: Infinite retries prevent circuit breaker
retries: -1  

# RIGHT: Finite retries allow circuit breaker to open
retries: 3
timeout: 5s
retry_period: 2s
```

2. **Check fallback configuration:**
```yaml
# WRONG: Missing fallback output
output:
  stdout:
    codec: lines

# RIGHT: Fallback output configured
output:
  fallback:
    - stdout:
        codec: lines
    - file:
        path: /tmp/fallback.jsonl
```

3. **Remove error-consuming try/catch blocks:**
```yaml
# WRONG: Catch block prevents fallback
- try:
    - http:
        url: ${API_URL}
  catch:
    - mapping: |
        root.failed = true  # Error consumed, no fallback

# RIGHT: Let errors bubble up to trigger fallback
- http:
    url: ${API_URL}
```

### Issue: Circuit Breaker Opens Too Frequently

**Symptoms:**
- Most requests go to fallback even when services are healthy
- Circuit breaker state flaps between open and closed
- Legitimate traffic being rejected

**Diagnosis:**

```bash
# Check service response times
time curl "${PRIMARY_API_URL:-http://localhost:8081}/health"

# Check timeout configuration
expanso get pipeline circuit-breakers-complete -o yaml | grep timeout

# Monitor circuit state changes
expanso logs pipeline circuit-breakers-complete | grep -E "(circuit|breaker|open|close)"
```

**Solutions:**

1. **Increase timeout values:**
```yaml
# Conservative timeouts for unstable networks
http:
  timeout: 10s      # was: 3s
  retries: 5        # was: 2
  retry_period: 3s  # was: 1s
```

2. **Add jitter to prevent thundering herd:**
```yaml
http:
  retry_period: 2s
  max_retry_backoff: 30s  # Exponential backoff
```

3. **Implement health checks before circuit activation:**
```yaml
processors:
  # Quick health check before main request
  - try:
      - http:
          url: ${API_URL}/health
          timeout: 1s
          retries: 1
          
      - mapping: |
          root.service_healthy = true
          
    catch:
      - mapping: |
          root.service_healthy = false

  # Skip main request if health check fails
  - switch:
      cases:
        - check: this.service_healthy == true
          processors:
            - http:
                url: ${API_URL}/main-endpoint
```

### Issue: Circuit Breaker Stuck Open

**Symptoms:**
- Circuit breaker never recovers after service comes back online
- All requests continue going to fallback
- No automatic recovery happening

**Diagnosis:**

```bash
# Check if services are actually healthy
curl -v "${PRIMARY_API_URL:-http://localhost:8081}/health"
curl -v "${SECONDARY_API_URL:-http://localhost:8082}/health"

# Check if health monitoring is running
expanso get pipeline circuit-breaker-monitor
expanso logs pipeline circuit-breaker-monitor --tail=20

# Look for recovery attempts
expanso logs pipeline circuit-breakers-complete | grep -i recovery
```

**Solutions:**

1. **Implement periodic health checks:**
```yaml
# Add health check pipeline
name: circuit-breaker-health-monitor
config:
  input:
    generate:
      interval: 30s  # Check every 30 seconds
      
  pipeline:
    processors:
      - try:
          - http:
              url: ${PRIMARY_API_URL}/health
              timeout: 2s
              
          - log:
              level: INFO
              message: "Service recovered: marking healthy"
              
        catch:
          - log:
              level: WARN
              message: "Service still unhealthy"
```

2. **Add manual circuit reset endpoint:**
```yaml
input:
  http_server:
    address: 0.0.0.0:8099
    path: /admin/reset-circuit
    
pipeline:
  processors:
    - mapping: |
        # Reset circuit breaker state
        root.circuit_reset = true
        root.timestamp = now()
        
    - log:
        level: INFO
        message: "Circuit breaker manually reset"
```

---

## Performance Issues

### Issue: High CPU Usage During Failures

**Symptoms:**
- CPU usage spikes when services fail
- System becomes unresponsive
- Many pending requests consuming resources

**Diagnosis:**

```bash
# Check system resource usage
top -p $(pgrep expanso)
htop

# Check pending request count
expanso describe pipeline circuit-breakers-complete | grep -E "(pending|queued|backlog)"

# Look for tight retry loops
expanso logs pipeline circuit-breakers-complete | grep retry | tail -20
```

**Solutions:**

1. **Add aggressive rate limiting:**
```yaml
http:
  rate_limit: "20/s"  # Limit concurrent requests
  timeout: 3s         # Fail fast
  retry_period: 5s    # Don't retry too quickly
```

2. **Implement backpressure:**
```yaml
input:
  http_server:
    timeout: 10s      # Timeout client connections
    
pipeline:
  processors:
    # Check system resources before processing
    - mapping: |
        # Get CPU usage (would use actual metrics in production)
        cpu_usage = 75  # Simulate 75% CPU
        if cpu_usage > 90 {
          throw("system overloaded, rejecting request")
        }
```

3. **Use circuit breaker pools:**
```yaml
# Separate smaller connection pools for different services
http:
  max_idle_connections: 5     # was: 50
  max_connections_per_host: 10 # was: 100
```

### Issue: Memory Leaks in Circuit Breaker

**Symptoms:**
- Memory usage continuously increases
- Out of memory errors
- Pipeline restarts frequently

**Diagnosis:**

```bash
# Monitor memory usage over time
while true; do
  ps aux | grep expanso | grep -v grep
  sleep 60
done

# Check for large object accumulation
expanso logs pipeline circuit-breakers-complete | grep -E "(cache|buffer|memory)"

# Look for connection leaks
netstat -an | grep ESTABLISHED | wc -l
```

**Solutions:**

1. **Implement connection lifecycle management:**
```yaml
http:
  max_idle_connections: 10
  max_connections_per_host: 20
  # Add connection timeouts
  idle_conn_timeout: 30s
  connection_max_lifetime: 5m
```

2. **Add cache size limits:**
```yaml
cache_resources:
  - label: circuit_cache
    memory:
      cap: 1000           # Limit cache entries
      default_ttl: 5m     # Expire entries
      cleanup_interval: 1m # Regular cleanup
```

3. **Implement buffer rotation:**
```yaml
# Rotate buffers to prevent unlimited growth
file:
  path: /tmp/buffers/buffer-${!timestamp_unix_date("2006-01-02-15")}.jsonl  # Hourly rotation
```

---

## Output and Routing Issues

### Issue: Fallback Chain Not Executing

**Symptoms:**
- Requests fail at first fallback level and don't try subsequent levels
- Only first fallback destination receives data
- Expected fallback behavior not happening

**Diagnosis:**

```bash
# Check fallback configuration structure
expanso get pipeline circuit-breakers-complete -o yaml | grep -A20 fallback

# Look for fallback execution in logs  
expanso logs pipeline circuit-breakers-complete | grep -E "(fallback|level)"

# Test each fallback destination manually
curl -X POST "http://localhost:9200/test" -d '{"test": true}'  # Elasticsearch
curl -X POST "http://localhost:9092/test" -d '{"test": true}'  # Kafka
```

**Solutions:**

1. **Verify fallback configuration syntax:**
```yaml
output:
  fallback:  # Must be at output level
    - # Level 1: Primary destination
      elasticsearch:
        urls: [${ELASTICSEARCH_URL}]
        
    - # Level 2: Secondary destination  
      kafka:
        addresses: [${KAFKA_BROKERS}]
        
    - # Level 3: Local buffer
      file:
        path: /tmp/buffer.jsonl
```

2. **Add fallback-specific error handling:**
```yaml
output:
  fallback:
    - processors:
        - mapping: |
            root.fallback_attempt = 1
            
        elasticsearch:
          urls: [${ELASTICSEARCH_URL}]
          
    - processors:
        - mapping: |
            root.fallback_attempt = 2
            
        kafka:
          addresses: [${KAFKA_BROKERS}]
```

3. **Test fallback destinations individually:**
```bash
# Test Elasticsearch  
curl -X POST "${ELASTICSEARCH_URL}/test/_doc/1" -d '{"test": true}'

# Test Kafka
echo '{"test": true}' | kafka-console-producer --broker-list ${KAFKA_BROKERS} --topic test

# Test local file
echo '{"test": true}' > /tmp/test-buffer.jsonl
```

### Issue: Data Loss in Fallback Chain

**Symptoms:**
- Events not appearing in any destination
- Missing data in final destinations
- Silent failures with no error logs

**Diagnosis:**

```bash
# Count events at each stage
echo "Checking event counts..."

# Check input
expanso logs pipeline circuit-breakers-complete | grep "input" | wc -l

# Check successful outputs  
expanso logs pipeline circuit-breakers-complete | grep "success" | wc -l

# Check fallback activations
expanso logs pipeline circuit-breakers-complete | grep "fallback" | wc -l

# Check dead letter queue
ls -la /tmp/circuit-breaker-dlq/ 2>/dev/null
```

**Solutions:**

1. **Add comprehensive logging at each fallback level:**
```yaml
output:
  fallback:
    - processors:
        - log:
            level: INFO
            message: "Attempting primary destination: ${!this.event_id}"
            
        elasticsearch:
          urls: [${ELASTICSEARCH_URL}]
          
    - processors:
        - log:
            level: WARN
            message: "Primary failed, attempting secondary: ${!this.event_id}"
            
        kafka:
          addresses: [${KAFKA_BROKERS}]
          
    - processors:
        - log:
            level: ERROR
            message: "All destinations failed, buffering locally: ${!this.event_id}"
            
        file:
          path: /tmp/critical-buffer.jsonl
```

2. **Implement event tracking:**
```yaml
processors:
  - mapping: |
      root = this
      root.tracking = {
        "event_id": this.event_id || uuid_v4(),
        "pipeline_instance": env("HOSTNAME"),
        "processing_timestamp": now()
      }
```

3. **Add final safety net:**
```yaml
output:
  fallback:
    # ... other levels ...
    
    # Final safety net - should never be reached
    - processors:
        - log:
            level: FATAL
            message: "CRITICAL: All fallback levels failed for event: ${!this.event_id}"
            
        - metric:
            type: counter
            name: events_dropped_total
            
      # Always succeed to prevent pipeline failure
      stdout:
        codec: lines
```

---

## Database Issues

### Issue: Database Connection Pool Exhaustion

**Symptoms:**
- "too many connections" errors
- Database queries hanging
- Circuit breaker not protecting database

**Diagnosis:**

```bash
# Check database connection count
docker exec your-db-container psql -U user -d dbname -c "SELECT count(*) FROM pg_stat_activity WHERE state = 'active';"

# Check connection pool configuration
expanso get pipeline circuit-breakers-complete -o yaml | grep -A5 max_open_connections

# Monitor connection lifecycle
expanso logs pipeline circuit-breakers-complete | grep -E "(connection|pool)"
```

**Solutions:**

1. **Optimize connection pool settings:**
```yaml
sql_select:
  max_open_connections: 20      # Limit total connections
  max_idle_connections: 10      # Maintain warm connections
  connection_max_lifetime: 10m  # Recycle connections
  connection_max_idle_time: 2m  # Close idle connections
```

2. **Add connection health checks:**
```yaml
processors:
  - try:
      # Test connection before use
      - sql_select:
          driver: postgres
          data_source_name: ${DB_CONNECTION_STRING}
          query: "SELECT 1"
          timeout: 1s
          
    catch:
      - log:
          level: WARN
          message: "Database connection failed, using fallback"
```

3. **Implement connection circuit breaker:**
```yaml
cache_resources:
  - label: db_circuit_state
    memory:
      cap: 100
      default_ttl: 5m

processors:
  # Check if database circuit is open
  - cache:
      resource: db_circuit_state
      operator: get
      key: db_circuit_open

  - switch:
      cases:
        - check: this.cached_data.exists()
          processors:
            - log:
                level: WARN
                message: "Database circuit open, skipping database call"
            - mapping: |
                root.database_skipped = true
```

### Issue: Database Query Timeouts

**Symptoms:**
- Frequent query timeout errors
- Database appears slow or unresponsive
- Circuit breaker activating on database calls

**Diagnosis:**

```bash
# Check database performance
docker exec your-db-container psql -U user -d dbname -c "
  SELECT query, mean_exec_time, calls 
  FROM pg_stat_statements 
  ORDER BY mean_exec_time DESC 
  LIMIT 10;"

# Check for long-running queries
docker exec your-db-container psql -U user -d dbname -c "
  SELECT pid, state, query_start, query 
  FROM pg_stat_activity 
  WHERE state = 'active' AND query_start < now() - interval '30 seconds';"
```

**Solutions:**

1. **Add database indexes:**
```sql
-- Add indexes for frequently queried columns
CREATE INDEX CONCURRENTLY idx_users_user_id ON users(user_id);
CREATE INDEX CONCURRENTLY idx_events_timestamp ON events(timestamp);
```

2. **Optimize queries:**
```yaml
sql_select:
  query: |
    SELECT user_name, user_tier 
    FROM users 
    WHERE user_id = $1
    LIMIT 1  -- Add LIMIT for safety
```

3. **Implement query caching:**
```yaml
cache_resources:
  - label: query_cache
    memory:
      cap: 5000
      default_ttl: 2m

processors:
  # Check cache first
  - cache:
      resource: query_cache
      operator: get
      key: user_${!this.user_id}
      
  - switch:
      cases:
        # Cache hit
        - check: this.cached_data.exists()
          processors:
            - mapping: |
                root.user = this.cached_data.parse_json()
                root.from_cache = true
                
        # Cache miss - query database
        - processors:
            - sql_select:
                # ... database query ...
                
            # Cache result  
            - cache:
                resource: query_cache
                operator: set
                key: user_${!this.user_id}
                value: ${!this.user.format_json()}
```

---

## Monitoring and Alerting Issues

### Issue: Missing Metrics

**Symptoms:**
- Dashboard shows no circuit breaker metrics
- Alerts not triggering
- No visibility into circuit breaker state

**Diagnosis:**

```bash
# Check if metrics processors are configured
expanso get pipeline circuit-breakers-complete -o yaml | grep -A5 metric

# Look for metric-related errors
expanso logs pipeline circuit-breakers-complete | grep -E "(metric|error)"

# Test metrics endpoint
curl "${METRICS_ENDPOINT:-http://localhost:9090}/api/v1/query?query=circuit_breaker_requests_total"
```

**Solutions:**

1. **Verify metric processor configuration:**
```yaml
processors:
  - metric:
      type: counter               # Valid types: counter, gauge, histogram
      name: circuit_breaker_requests_total  # No spaces
      labels:
        status: ${!this.status}   # Ensure variable exists
```

2. **Add metric validation:**
```yaml
processors:
  - mapping: |
      # Ensure required fields exist for metrics
      if !this.status.exists() {
        root.status = "unknown"
      }
      
  - metric:
      type: counter
      name: circuit_breaker_requests_total
      labels:
        status: ${!this.status}
```

3. **Debug metrics with logging:**
```yaml
processors:
  - log:
      level: DEBUG
      message: "Recording metric - name: circuit_breaker_requests_total, status: ${!this.status}"
      
  - metric:
      type: counter
      name: circuit_breaker_requests_total
      labels:
        status: ${!this.status}
```

### Issue: Alert Fatigue

**Symptoms:**
- Too many alerts being generated
- Important alerts getting lost in noise
- Alert channels overwhelmed

**Solutions:**

1. **Implement alert throttling:**
```yaml
cache_resources:
  - label: alert_throttle
    memory:
      cap: 1000
      default_ttl: 5m  # 5-minute throttle window

processors:
  # Check if alert was recently sent
  - cache:
      resource: alert_throttle
      operator: get
      key: alert_${!this.alert_type}_${!this.service}

  - switch:
      cases:
        - check: this.cached_data.exists()
          processors:
            - log:
                level: DEBUG
                message: "Alert throttled: ${!this.alert_type}"
            drop: {}

        # Send alert and cache to prevent duplicates
        - processors:
            - log:
                level: ERROR
                message: "ALERT: ${!this.alert_message}"

            - cache:
                resource: alert_throttle
                operator: set
                key: alert_${!this.alert_type}_${!this.service}
                value: "sent"
```

2. **Implement alert severity levels:**
```yaml
processors:
  - mapping: |
      # Classify alert severity
      if this.error_rate > 50 || this.all_circuits_open {
        root.alert_severity = "critical"
      } else if this.error_rate > 20 || this.degraded_service {
        root.alert_severity = "warning"  
      } else {
        root.alert_severity = "info"
      }

  - switch:
      cases:
        # Only page for critical alerts
        - check: this.alert_severity == "critical"
          processors:
            - http:
                url: ${PAGER_WEBHOOK}
                verb: POST

        # Send to Slack for warnings
        - check: this.alert_severity == "warning"
          processors:
            - http:
                url: ${SLACK_WEBHOOK}
                verb: POST
```

---

## Recovery and Maintenance

### Issue: Buffer Recovery Not Working

**Symptoms:**
- Events stuck in local buffers
- Recovery process not running
- Duplicate processing after recovery

**Diagnosis:**

```bash
# Check for buffered events
find /tmp -name "*buffer*.jsonl" -exec wc -l {} \; | sort -nr

# Check if recovery pipeline is running
expanso get pipeline buffer-recovery-processor

# Look for recovery logs
expanso logs pipeline buffer-recovery-processor | grep -E "(recovery|duplicate)"
```

**Solutions:**

1. **Implement proper buffer recovery:**
```yaml
name: buffer-recovery-processor
config:
  input:
    file:
      paths: ["/tmp/circuit-breaker-buffers/**/*.jsonl"]
      scanner:
        lines: {}
      delete_on_finish: true  # Remove processed files

  pipeline:
    processors:
      # Parse buffered event
      - mapping: |
          root = content().parse_json()
          root.recovered_at = now()
          root.recovery_attempt = true

      # Check if services are healthy before retry
      - try:
          - http:
              url: ${PRIMARY_API_URL}/health
              timeout: 2s
              
          - mapping: |
              root.service_healthy = true
              
        catch:
          - mapping: |
              root.service_healthy = false

      # Only process if services are healthy
      - switch:
          cases:
            - check: this.service_healthy == true
              processors:
                # Retry original processing
                - http:
                    url: ${PRIMARY_API_URL}/process
                    timeout: 10s
                    retries: 2

  output:
    stdout:
      codec: lines
```

2. **Add duplicate detection:**
```yaml
cache_resources:
  - label: processed_events
    memory:
      cap: 10000
      default_ttl: 1h

processors:
  # Check if event already processed
  - cache:
      resource: processed_events
      operator: get
      key: ${!this.event_id}

  - switch:
      cases:
        - check: this.cached_data.exists()
          processors:
            - log:
                level: INFO
                message: "Skipping duplicate event: ${!this.event_id}"
            drop: {}

  # Mark as processed
  - cache:
      resource: processed_events
      operator: set
      key: ${!this.event_id}
      value: "processed"
```

### Issue: Circuit Breaker State Inconsistency

**Symptoms:**
- Different pipeline instances showing different circuit states
- Circuit state not synchronized
- Inconsistent fallback behavior

**Solutions:**

1. **Use shared state storage:**
```yaml
cache_resources:
  - label: shared_circuit_state
    redis:
      url: ${REDIS_URL}
      default_ttl: 10m

processors:
  # Check shared circuit state
  - cache:
      resource: shared_circuit_state
      operator: get
      key: circuit_state_${!this.service_name}

  - mapping: |
      # Use shared state if available
      if this.cached_data.exists() {
        shared_state = this.cached_data.parse_json()
        root.circuit_open = shared_state.open
        root.circuit_last_failure = shared_state.last_failure
      } else {
        root.circuit_open = false
      }
```

2. **Implement circuit state synchronization:**
```yaml
processors:
  # Update shared circuit state on changes
  - switch:
      cases:
        - check: this.circuit_state_changed == true
          processors:
            - cache:
                resource: shared_circuit_state
                operator: set
                key: circuit_state_${!this.service_name}
                value: |
                  {
                    "open": ${!this.circuit_open},
                    "last_failure": "${!now()}",
                    "instance": "${!env("HOSTNAME")}"
                  }
```

---

## Getting Help

If you're still experiencing issues after trying these solutions:

### 1. Enable Debug Logging

```yaml
processors:
  - log:
      level: DEBUG
      message: "Circuit breaker debug - Event: ${!this.event_id}, State: ${!this.circuit_state}, Attempt: ${!this.attempt_number}"
```

### 2. Collect Diagnostic Information

```bash
#!/bin/bash
# collect-diagnostics.sh

echo "Collecting circuit breaker diagnostics..."

# Pipeline configuration
expanso get pipeline circuit-breakers-complete -o yaml > pipeline-config.yaml

# Recent logs
expanso logs pipeline circuit-breakers-complete --tail=500 > pipeline-logs.txt

# System resources
top -b -n1 > system-resources.txt

# Network connectivity
curl -v "${PRIMARY_API_URL}/health" > connectivity-test.txt 2>&1

echo "Diagnostic files created: pipeline-config.yaml, pipeline-logs.txt, system-resources.txt, connectivity-test.txt"
```

### 3. Minimal Reproduction Case

Create a minimal test case that reproduces the issue:

```yaml
name: circuit-breaker-debug
config:
  input:
    generate:
      interval: 10s
      mapping: |
        root = {"test_id": count("test"), "timestamp": now()}

  pipeline:
    processors:
      - log:
          level: INFO
          message: "Processing test event: ${!this.test_id}"

      # Minimal circuit breaker test
      - try:
          - http:
              url: "http://httpbin.org/delay/2"  # Reliable test endpoint
              timeout: 1s  # Will timeout to test circuit breaker

        catch:
          - log:
              level: WARN
              message: "Circuit breaker activated for test: ${!this.test_id}"

  output:
    stdout:
      codec: lines
```

### 4. Community Resources

- **Documentation:** [Expanso Circuit Breaker Guide](https://docs.expanso.io/patterns/circuit-breaker)
- **Community Forum:** Share your diagnostics and get help from the community
- **GitHub Issues:** Report bugs with minimal reproduction cases
- **Support:** Contact support with diagnostic files for production issues

---

Remember to always test circuit breaker changes in a non-production environment first, and have rollback plans ready when deploying fixes to production systems.

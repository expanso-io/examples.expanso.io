---
title: Enterprise Migration
sidebar_label: Overview
sidebar_position: 1
description: Replace legacy ETL tools with edge-native data pipelines
keywords: [enterprise, migration, etl, datastage, informatica, db2, mainframe]
---

# Enterprise Migration

Replace legacy ETL infrastructure with modern, edge-native data pipelines.

## The Challenge

Enterprise data teams face a common problem:
- **Legacy ETL tools** (DataStage, Informatica, SSIS) with expensive licensing
- **Mainframe databases** (DB2, IMS, VSAM) that need cloud migration
- **Compliance requirements** that demand data stays on-premise during transformation
- **Skills gap** as ETL specialists retire

## The Solution

Expanso pipelines run **at the edge**—on or near your data centers—providing:

✅ **No license fees** - Flat-rate pricing, unlimited nodes  
✅ **Data sovereignty** - Transform before data leaves your network  
✅ **Modern tooling** - YAML configs, GitOps deployment, API-first  
✅ **Gradual migration** - Replace one job at a time, not big bang  

## Examples

### [DB2 to BigQuery Migration](./db2-to-bigquery/)
Replace DataStage ETL with edge-native processing for migrating financial transactions.

**What you'll learn:**
- Replace DataStage Lookup/Transformer stages
- Add automatic lineage tracking
- Mask sensitive data at the edge
- Deploy to nodes near your DB2 servers

**Time:** 45-60 minutes

---

### [Nightly Backup Pipeline](./nightly-backup/) *(coming soon)*
Secure, incremental backups from enterprise databases to cloud storage.

**What you'll learn:**
- Incremental extraction patterns
- Compression and encryption at rest
- Multi-cloud destination support
- Recovery verification

---

### [Cross-Border GDPR Routing](../data-security/cross-border-gdpr/) 
Route data to region-appropriate destinations based on customer location.

**What you'll learn:**
- Geographic data classification
- Region-specific transformation rules
- Audit logging for compliance

## Migration Patterns

### Pattern 1: Shadow Mode
Run Expanso alongside existing ETL to validate output before cutover.

```
DB2 → DataStage → BigQuery (production)
   └→ Expanso   → BigQuery_test (validation)
```

### Pattern 2: Gradual Replacement
Migrate one job at a time, starting with lowest-risk pipelines.

```
Week 1: Migrate reporting extracts
Week 2: Migrate dimension loads  
Week 3: Migrate fact tables
Week 4: Decommission DataStage server
```

### Pattern 3: Edge-First New Development
Keep legacy jobs running, build all new pipelines on Expanso.

```
Legacy jobs → DataStage (maintenance mode)
New jobs    → Expanso (active development)
```

## Getting Started

1. **Choose an example** that matches your migration scenario
2. **Set up a test environment** with sample data
3. **Run shadow mode** to validate against existing ETL
4. **Deploy to production** when confident

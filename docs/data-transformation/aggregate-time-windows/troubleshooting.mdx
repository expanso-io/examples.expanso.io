---
title: Troubleshooting Guide
sidebar_label: Troubleshooting
sidebar_position: 10
description: Comprehensive troubleshooting guide for time-windowed aggregation issues and solutions
keywords: [troubleshooting, debugging, performance, errors, solutions]
---

# Troubleshooting Time-Windowed Aggregation

**Comprehensive solutions for common issues in time-windowed aggregation pipelines.** This guide covers 25+ issues across deployment, performance, data quality, and production operations with step-by-step diagnostic procedures and solutions.

## Quick Diagnosis Tools

### System Health Check

Run this comprehensive health check to quickly identify issues:

```bash
#!/bin/bash
# Aggregate Time Windows Health Check Script

echo "üîç Time-Windowed Aggregation Health Check"
echo "========================================"

# 1. Check Kubernetes deployment
echo "üì¶ Kubernetes Deployment Status:"
kubectl get pods -l app=time-window-aggregation -o wide
kubectl get hpa aggregation-hpa

# 2. Check cache health
echo -e "\nüóÑÔ∏è  Cache Health:"
curl -s http://localhost:8080/metrics | grep -E "(cache_items|cache_hit_rate|cache_evictions)"

# 3. Check processing rates
echo -e "\n‚ö° Processing Rates:"
curl -s http://localhost:8080/metrics | grep -E "(events_processed|aggregations_output|processing_duration)"

# 4. Check error rates  
echo -e "\n‚ùå Error Rates:"
kubectl logs -l app=time-window-aggregation --tail=100 | grep -i error | wc -l
echo "Recent errors found in logs"

# 5. Check resource usage
echo -e "\nüíæ Resource Usage:"
kubectl top pods -l app=time-window-aggregation

# 6. Check downstream connectivity
echo -e "\nüîó Connectivity:"
kubectl exec deployment/time-window-aggregation -- curl -s -o /dev/null -w "%{http_code}" "$ANALYTICS_ENDPOINT/health"
echo " Analytics endpoint health"

echo -e "\n‚úÖ Health check completed!"
```

### Quick Performance Check

```bash
# Quick performance diagnostic
echo "‚è±Ô∏è Performance Quick Check:"

# Check current throughput
CURRENT_RATE=$(curl -s http://localhost:8080/metrics | grep "custom_events" | awk '{print $2}')
echo "Current processing rate: $CURRENT_RATE events/sec"

# Check latency
LATENCY=$(curl -s http://localhost:8080/metrics | grep "processor_duration" | tail -1 | awk '{print $2}')
echo "Processing latency: ${LATENCY}s"

# Check queue depth
QUEUE_DEPTH=$(kubectl exec deployment/time-window-aggregation -- curl -s http://localhost:9092/metrics | grep consumer_lag | awk '{print $2}')
echo "Consumer lag: $QUEUE_DEPTH events"

# Resource utilization
kubectl top pods -l app=time-window-aggregation | tail -n +2 | awk '{cpu+=$2; mem+=$3} END {print "Avg CPU:", cpu/NR "m, Avg Memory:", mem/NR "Mi"}'
```

---

## Performance Issues

### Issue: High Memory Usage

**Symptoms:**
- Memory usage continuously growing
- Pods being OOM killed
- Slow aggregation performance
- Cache evictions increasing

**Diagnosis:**
```bash
# Check memory usage patterns
kubectl top pods -l app=time-window-aggregation
kubectl describe pod <aggregation-pod-name> | grep -A5 "Resource Requests"

# Monitor memory over time
watch "kubectl top pods -l app=time-window-aggregation"

# Check cache memory usage
curl http://localhost:8080/metrics | grep -E "(cache_memory|cache_items|go_memstats)"

# Check for memory leaks
kubectl exec aggregation-pod-1 -- cat /proc/meminfo | grep -E "(MemTotal|MemAvailable|MemFree)"
```

**Solutions:**

**1. Reduce Cache Size**
```yaml
resources:
  caches:
    tumbling_cache:
      memory:
        max_items: 50000     # Reduced from 100000
        default_ttl: "90s"   # Reduced from 120s
```

**2. Implement Horizontal Scaling**
```bash
# Scale out instead of up
kubectl scale deployment time-window-aggregation --replicas=10

# Update HPA for more aggressive scaling
kubectl patch hpa aggregation-hpa -p '{
  "spec": {
    "metrics": [{
      "type": "Resource",
      "resource": {
        "name": "memory",
        "target": {
          "type": "Utilization", 
          "averageUtilization": 60
        }
      }
    }]
  }
}'
```

**3. Optimize Data Structures**
```yaml
# Use more compact data representations
- mapping: |
    # Compress event data before caching
    root = {
      "id": this.sensor_id,
      "ts": this.timestamp,
      "temp": this.temperature,
      "loc": this.location
    }
    
    # Remove unnecessary fields
    deleted("original_payload")
    deleted("debug_info")
```

**4. Implement Memory Monitoring**
```yaml
# Add memory circuit breaker
- switch:
    - check: process_resident_memory_bytes > 3000000000  # 3GB
      processors:
        - mapping: |
            error("Memory protection: Rejecting events due to high memory usage")
    - processors:
        # Normal processing pipeline
```

### Issue: High Processing Latency

**Symptoms:**
- End-to-end processing time > 10 seconds
- Dashboard updates delayed
- Consumer lag increasing
- Batch timeouts occurring

**Diagnosis:**
```bash
# Check processing pipeline latency
curl http://localhost:8080/metrics | grep processor_duration_seconds

# Check Kafka consumer lag
kubectl exec kafka-0 -- kafka-consumer-groups.sh \
  --bootstrap-server kafka:9092 \
  --describe --group aggregation-consumer

# Check cache operation latency
curl http://localhost:8080/metrics | grep cache_operation_duration

# Check network latency to downstream services
kubectl exec deployment/time-window-aggregation -- \
  curl -w "Connect: %{time_connect}s, Total: %{time_total}s\n" \
  -o /dev/null -s "$ANALYTICS_ENDPOINT/health"
```

**Solutions:**

**1. Optimize Batch Processing**
```yaml
# Increase batch sizes for efficiency
pipeline:
  processors:
    - group_by:
        - key: ${! this.group_key }
          value: ${! this }
          size: 5000  # Increased from 1000

# Optimize output batching
output:
  http_client:
    batching:
      count: 1000     # Increased from 500
      period: "5s"    # Reduced from 10s
```

**2. Implement Parallel Processing**
```yaml
# Add parallel processing for high-throughput
- parallel:
    cap: 8  # Process up to 8 windows simultaneously
    processors:
      - cache:
          resource: aggregation_cache
      - group_by:
          - key: ${! this.group_key }
            value: ${! this }
```

**3. Optimize Cache Performance**
```yaml
# Use Redis clustering for better performance
resources:
  caches:
    production_cache:
      redis:
        # Connection pooling
        pool_size: 20
        max_active: 50
        
        # Cluster mode for horizontal scaling
        cluster_addresses: 
          - "redis-cluster-0:6379"
          - "redis-cluster-1:6379" 
          - "redis-cluster-2:6379"
```

**4. Add Processing Circuit Breaker**
```yaml
# Fail fast if processing takes too long
- switch:
    - check: processing_time > duration("30s")
      processors:
        - mapping: |
            error("Processing timeout: Abandoning slow aggregation")
    - processors:
        # Normal aggregation pipeline
```

### Issue: Low Throughput

**Symptoms:**
- Processing < 10,000 events/second
- CPU usage < 50%
- Consumer not keeping up with producer
- Events backing up in Kafka

**Diagnosis:**
```bash
# Check current throughput
THROUGHPUT=$(curl -s http://localhost:8080/metrics | grep rate | grep custom_events | awk '{print $2}')
echo "Current throughput: $THROUGHPUT events/sec"

# Check resource utilization
kubectl top pods -l app=time-window-aggregation

# Check Kafka partition assignment
kubectl exec kafka-0 -- kafka-consumer-groups.sh \
  --bootstrap-server kafka:9092 \
  --describe --group aggregation-consumer

# Check for bottlenecks in pipeline
curl http://localhost:8080/metrics | grep processor | grep duration
```

**Solutions:**

**1. Increase Consumer Parallelism**
```yaml
input:
  kafka:
    # Increase consumer parallelism
    consumer:
      max_poll_records: 20000    # Increased from 10000
      fetch_min_bytes: 2097152   # 2MB (increased from 1MB)
      
    # Use more partitions
    partitions: 12  # Match number of consumer instances
```

**2. Optimize Pipeline Processing**
```yaml
# Reduce unnecessary processing
- mapping: |
    # Skip complex calculations for stable sensors
    if this.temperature_stddev < 1.0 {
      root.temperature_trend = "stable"
      # Skip detailed trend analysis
    } else {
      # Full trend calculation for volatile sensors
      # ... complex calculations
    }
```

**3. Scale Out Processing**
```bash
# Scale deployment for higher throughput
kubectl scale deployment time-window-aggregation --replicas=12

# Update resource requests for better scheduling
kubectl patch deployment time-window-aggregation -p '{
  "spec": {
    "template": {
      "spec": {
        "containers": [{
          "name": "aggregation-pipeline",
          "resources": {
            "requests": {
              "cpu": "1000m",
              "memory": "2Gi"
            },
            "limits": {
              "cpu": "2000m", 
              "memory": "4Gi"
            }
          }
        }]
      }
    }
  }
}'
```

---

## Data Quality Issues

### Issue: Missing Events in Aggregations

**Symptoms:**
- Event counts lower than expected
- Completeness ratios < 0.8
- Gaps in time series data
- Inconsistent aggregation results

**Diagnosis:**
```bash
# Check event loss at different stages
echo "Input events (last 5 minutes):"
kubectl logs -l app=time-window-aggregation --since=5m | grep "input_received" | wc -l

echo "Cached events:"
curl http://localhost:8080/metrics | grep cache_items

echo "Output aggregations:"
curl http://localhost:8080/metrics | grep output_sent

# Check for validation errors
kubectl logs -l app=time-window-aggregation | grep -i "validation\|error" | tail -10

# Check timestamp parsing issues
kubectl logs -l app=time-window-aggregation | grep -i "parse\|timestamp" | tail -10
```

**Solutions:**

**1. Improve Input Validation**
```yaml
# More robust validation with better error handling
- try:
    processors:
      - json: {}
      - mapping: |
          # Graceful handling of missing fields
          root = this
          
          # Provide defaults for optional fields
          root.location = this.get("location").or("unknown")
          root.facility = this.get("facility").or("default_facility")
          
          # Validate required fields exist
          if !this.exists("sensor_id") {
            root.sensor_id = "unknown_sensor_" + uuid_v4()
            root.data_quality_issue = "missing_sensor_id"
          }
          
          if !this.exists("timestamp") {
            root.timestamp = now().ts_format("2006-01-02T15:04:05.000Z")
            root.data_quality_issue = "missing_timestamp"
          }
          
  catch:
    # Log and continue instead of dropping events
    - mapping: |
        root = {
          "sensor_id": "error_placeholder",
          "temperature": 0,
          "timestamp": now().ts_format("2006-01-02T15:04:05.000Z"),
          "data_quality_issue": "validation_error",
          "original_error": error(),
          "original_payload": this
        }
```

**2. Handle Late-Arriving Events**
```yaml
# Increase grace periods for late events
resources:
  caches:
    aggregation_cache:
      memory:
        default_ttl: "180s"  # 3-minute grace period (increased from 120s)

# Track late events for analysis
- mapping: |
    let event_time = this.timestamp.parse_timestamp("2006-01-02T15:04:05.000Z")
    let current_time = now()
    let lateness_seconds = (current_time - event_time).seconds()
    
    root = this
    root.event_lateness_seconds = lateness_seconds
    
    if lateness_seconds > 120 {  # 2 minutes late
      root.late_event = true
      root.late_event_category = match {
        lateness_seconds > 3600 => "very_late",   # > 1 hour
        lateness_seconds > 600 => "moderately_late", # > 10 minutes 
        _ => "slightly_late"
      }
    }
```

**3. Implement Event Deduplication**
```yaml
# Add deduplication to prevent double-counting
- cache:
    resource: dedup_cache
    key: ${! this.sensor_id + "|" + this.timestamp }
    value: "processed"
    
# Skip if already seen
- switch:
    - check: this.exists("cache_duplicate")
      processors:
        - mapping: |
            error("Duplicate event filtered: " + this.sensor_id + " at " + this.timestamp)
    - processors:
        # Normal processing
```

### Issue: Incorrect Aggregation Values

**Symptoms:**
- Temperature averages seem wrong
- Event counts don't add up
- Inconsistent values across hierarchy levels
- Mathematical errors in calculations

**Diagnosis:**
```bash
# Check aggregation accuracy
echo "Sensor-level aggregations:"
jq 'select(.aggregation_level == "sensor") | {sensor_id, temperature_avg, event_count}' aggregation-output.jsonl | head -5

echo "Location-level aggregations:"
jq 'select(.aggregation_level == "location") | {location, sensor_count, temperature_avg}' aggregation-output.jsonl | head -5

# Verify mathematical consistency
echo "Manual verification:"
jq -r 'select(.aggregation_level == "sensor") | .temperature_avg' aggregation-output.jsonl | awk '{sum+=$1; count++} END {print "Manual average:", sum/count}'
jq -r 'select(.aggregation_level == "global") | .temperature_avg' aggregation-output.jsonl | head -1
```

**Solutions:**

**1. Add Calculation Validation**
```yaml
# Validate aggregation results
- mapping: |
    # Validate temperature averages are reasonable
    if this.temperature_avg < -50 || this.temperature_avg > 150 {
      error("Unrealistic temperature average: " + this.temperature_avg.string())
    }
    
    # Validate event counts are positive
    if this.event_count < 0 {
      error("Negative event count: " + this.event_count.string())
    }
    
    # Validate completeness ratios
    if this.completeness_ratio > 2.0 {
      error("Completeness ratio > 2.0: " + this.completeness_ratio.string())
    }
    
    root = this
```

**2. Improve Numeric Precision**
```yaml
# Use higher precision for calculations
- mapping: |
    let events = this
    let temperatures = events.map_each(e -> e.temperature)
    
    # Use higher precision arithmetic
    root.temperature_avg = (temperatures.sum() / temperatures.length().float()).round(3)
    root.temperature_variance = temperatures.map_each(t -> (t - root.temperature_avg).pow(2)).sum() / temperatures.length().float()
    root.temperature_stddev = root.temperature_variance.sqrt().round(3)
    
    # Add calculation metadata for debugging
    root.calculation_metadata = {
      "input_count": temperatures.length(),
      "temperature_sum": temperatures.sum(),
      "calculation_method": "precise_arithmetic"
    }
```

**3. Debug Aggregation Logic**
```yaml
# Add comprehensive debugging information
- mapping: |
    root = this
    root.debug_aggregation = {
      "input_events": if this.is_array() { this.length() } else { 1 },
      "group_key": this.group_key,
      "time_bucket": this.time_bucket,
      "processing_timestamp": now().ts_format("2006-01-02T15:04:05.000Z")
    }
    
    # Log unusual aggregations for investigation
    if this.exists("event_count") && this.event_count > 1000 {
      root.unusual_aggregation = true
      root.investigation_needed = "high_event_count"
    }
```

### Issue: Clock Skew and Timestamp Issues

**Symptoms:**
- Events appearing in wrong time buckets
- Future timestamps in events
- Inconsistent time bucket distribution
- Sessions not forming correctly

**Diagnosis:**
```bash
# Check for timestamp issues
kubectl logs -l app=time-window-aggregation | grep -i "timestamp\|parse" | tail -10

# Check time bucket distribution
jq -r '.time_bucket' aggregation-output.jsonl | sort | uniq -c | head -10

# Check for future timestamps
jq '.timestamp, .processing_timestamp' aggregation-output.jsonl | paste - - | awk '$1 > $2 {print "Future timestamp:", $1}' | head -5

# Check system clock on pods
kubectl exec deployment/time-window-aggregation -- date
```

**Solutions:**

**1. Robust Timestamp Parsing**
```yaml
# Handle multiple timestamp formats gracefully
- mapping: |
    root = this
    
    # Comprehensive timestamp parsing
    let parsed_timestamp = match {
      # ISO 8601 with milliseconds: 2025-01-15T10:30:45.123Z
      this.timestamp.type() == "string" && this.timestamp.contains("T") && this.timestamp.contains("Z") =>
        this.timestamp.parse_timestamp("2006-01-02T15:04:05.000Z"),
        
      # ISO 8601 without milliseconds: 2025-01-15T10:30:45Z  
      this.timestamp.type() == "string" && this.timestamp.contains("T") =>
        this.timestamp.parse_timestamp("2006-01-02T15:04:05Z"),
        
      # Unix timestamp (seconds)
      this.timestamp.type() == "int" && this.timestamp > 1000000000 && this.timestamp < 2000000000 =>
        this.timestamp.timestamp_unix(),
        
      # Unix timestamp (milliseconds)
      this.timestamp.type() == "int" && this.timestamp > 1000000000000 =>
        (this.timestamp / 1000).timestamp_unix(),
        
      _ => {
        # Default to current time if unparseable
        root.timestamp_parse_error = "unparseable_format"
        now()
      }
    }
    
    # Validate timestamp is reasonable
    let current_time = now()
    let age_hours = (current_time - parsed_timestamp).hours()
    
    if age_hours > 24 {
      root.timestamp_warning = "event_older_than_24_hours"
    }
    
    if age_hours < -1 {
      root.timestamp_warning = "future_event"
      # Use current time for future events
      parsed_timestamp = current_time
    }
    
    root.parsed_timestamp = parsed_timestamp
```

**2. Clock Skew Compensation**
```yaml
# Implement clock skew detection and correction
- mapping: |
    let server_time = now()
    let event_time = this.parsed_timestamp
    let skew_seconds = (event_time - server_time).seconds()
    
    # Detect significant clock skew
    if skew_seconds.abs() > 300 {  # 5 minutes
      root.clock_skew_detected = true
      root.skew_seconds = skew_seconds
      
      # Apply skew correction for devices with known consistent skew
      let device_skew = get_device_clock_skew(this.sensor_id)  # Would lookup from cache
      if device_skew != null {
        root.corrected_timestamp = (event_time - duration(device_skew.string() + "s")).ts_format("2006-01-02T15:04:05.000Z")
      } else {
        # Use server time for events with extreme skew
        root.corrected_timestamp = server_time.ts_format("2006-01-02T15:04:05.000Z")
      }
    } else {
      root.corrected_timestamp = this.timestamp
    }
```

**3. Time Zone Handling**
```yaml
# Normalize time zones for global deployments
- mapping: |
    # Convert all timestamps to UTC
    let utc_timestamp = match {
      # Handle timezone offsets: 2025-01-15T10:30:45-05:00
      this.timestamp.contains("+") || this.timestamp.ends_with("-") =>
        this.timestamp.parse_timestamp("2006-01-02T15:04:05-07:00").ts_format("2006-01-02T15:04:05Z"),
        
      # Already UTC: 2025-01-15T10:30:45Z
      this.timestamp.contains("Z") =>
        this.timestamp,
        
      # Assume UTC if no timezone specified
      _ =>
        this.timestamp + "Z"
    }
    
    root = this
    root.timestamp = utc_timestamp
    root.timezone_normalized = true
```

---

## Infrastructure Issues

### Issue: Kafka Consumer Lag

**Symptoms:**
- Consumer lag increasing continuously
- Processing delays growing
- Events backing up in Kafka topics
- Real-time dashboards showing stale data

**Diagnosis:**
```bash
# Check consumer lag across all partitions
kubectl exec kafka-0 -- kafka-consumer-groups.sh \
  --bootstrap-server kafka:9092 \
  --describe --group aggregation-consumer

# Check partition assignment and distribution
kubectl exec kafka-0 -- kafka-topics.sh \
  --describe --topic sensor-events \
  --bootstrap-server kafka:9092

# Monitor consumer performance
kubectl logs -l app=time-window-aggregation | grep -E "(consumer|lag|offset)" | tail -10
```

**Solutions:**

**1. Scale Consumer Instances**
```bash
# Scale to match number of Kafka partitions
PARTITION_COUNT=$(kubectl exec kafka-0 -- kafka-topics.sh \
  --describe --topic sensor-events \
  --bootstrap-server kafka:9092 | grep "PartitionCount" | awk '{print $2}')

kubectl scale deployment time-window-aggregation --replicas=$PARTITION_COUNT
```

**2. Optimize Consumer Configuration**
```yaml
input:
  kafka:
    consumer:
      # Increase batch sizes for higher throughput
      max_poll_records: 50000
      fetch_min_bytes: 5242880    # 5MB
      fetch_max_wait_ms: 500      # Reduced wait time
      
      # Optimize session management
      session_timeout_ms: 30000
      heartbeat_interval_ms: 3000
      max_poll_interval_ms: 300000  # 5 minutes
      
    # Partition assignment strategy
    partition_assignment_strategy: "RoundRobin"
```

**3. Implement Backpressure Handling**
```yaml
# Add circuit breaker for high lag situations
- switch:
    - check: kafka_consumer_lag() > 1000000  # 1M events behind
      processors:
        # Skip complex processing during catch-up
        - mapping: |
            root = {
              "sensor_id": this.sensor_id,
              "temperature_avg": this.temperature,
              "time_bucket": this.time_bucket,
              "catch_up_mode": true,
              "event_count": 1
            }
    - processors:
        # Normal full processing
```

### Issue: Redis Connection Failures

**Symptoms:**
- Cache operations failing
- Fallback to memory cache
- Inconsistent aggregation results
- Connection timeout errors

**Diagnosis:**
```bash
# Check Redis connectivity
kubectl exec deployment/time-window-aggregation -- \
  redis-cli -h $REDIS_HOST ping

# Check Redis cluster status  
kubectl exec redis-0 -- redis-cli cluster nodes

# Monitor cache metrics
curl http://localhost:8080/metrics | grep -E "(cache_hit|cache_miss|cache_error)"

# Check Redis resource usage
kubectl exec redis-0 -- redis-cli info memory
```

**Solutions:**

**1. Implement Redis High Availability**
```yaml
resources:
  caches:
    production_cache:
      redis:
        # Sentinel configuration for HA
        sentinel:
          master_name: "aggregation-master"
          addresses:
            - "redis-sentinel-0:26379"
            - "redis-sentinel-1:26379"
            - "redis-sentinel-2:26379"
          
        # Connection pooling for reliability
        pool_size: 20
        max_retries: 3
        retry_delay: "100ms"
        
        # Health checking
        health_check_interval: "30s"
```

**2. Add Cache Fallback Strategy**
```yaml
# Graceful fallback from Redis to memory
- try:
    processors:
      # Primary: Redis distributed cache
      - cache:
          resource: redis_cache
          key: ${! this.group_key }
          value: ${! this }
  catch:
    # Fallback: In-memory cache
    - cache:
        resource: memory_cache
        key: ${! this.group_key }
        value: ${! this }
        
    # Log cache fallback for monitoring
    - mapping: |
        root.cache_fallback_event = {
          "timestamp": now().ts_format("2006-01-02T15:04:05.000Z"),
          "reason": "redis_unavailable",
          "fallback_cache": "memory"
        }
```

**3. Optimize Redis Configuration**
```yaml
# Redis optimization for aggregation workloads
redis_config: |
  # Memory management
  maxmemory 8gb
  maxmemory-policy allkeys-lru
  
  # Persistence optimization
  save 900 1
  stop-writes-on-bgsave-error no
  
  # Network optimization
  tcp-keepalive 300
  timeout 0
  
  # Performance tuning
  hash-max-ziplist-entries 512
  list-max-ziplist-size -2
```

### Issue: Output Circuit Breaker Tripping

**Symptoms:**
- Frequent fallback to file output
- Analytics platform connectivity issues
- Circuit breaker open state
- Backup files accumulating

**Diagnosis:**
```bash
# Check circuit breaker status
curl http://localhost:8080/metrics | grep circuit_breaker

# Test downstream connectivity
kubectl exec deployment/time-window-aggregation -- \
  curl -v --max-time 10 "$ANALYTICS_ENDPOINT/health"

# Check output error logs
kubectl logs -l app=time-window-aggregation | grep -E "(circuit|output|fallback)" | tail -20

# Monitor backup file accumulation
kubectl exec deployment/time-window-aggregation -- ls -la /var/buffer/
```

**Solutions:**

**1. Adjust Circuit Breaker Sensitivity**
```yaml
output:
  circuit_breaker:
    failure_threshold: 10       # More tolerant (increased from 5)
    success_threshold: 5        # Require more successes to close
    timeout: "60s"             # Longer timeout before retry
    half_open_max_calls: 20    # More test calls when half-open
```

**2. Improve Downstream Resilience**
```yaml
output:
  circuit_breaker:
    outputs:
      - try:
          - http_client:
              url: "${ANALYTICS_ENDPOINT}/aggregations"
              # More conservative timeouts
              timeout: "30s"
              max_retries: 5
              
              # Better retry strategy
              backoff:
                initial_interval: "1s"
                max_interval: "30s"
                multiplier: 2.0
                jitter: 0.1
                
        catch:
          # Enhanced error logging
          - mapping: |
              root = {
                "error_type": "analytics_endpoint_failure",
                "error_message": error(),
                "endpoint": "${ANALYTICS_ENDPOINT}",
                "retry_count": this.get("retry_count").or(0),
                "timestamp": now().ts_format("2006-01-02T15:04:05.000Z")
              }
```

**3. Implement Circuit Breaker Monitoring**
```yaml
# Add circuit breaker state tracking
metrics:
  mapping: |
    # Track circuit breaker state
    if this.exists("circuit_breaker_state") {
      root.custom_circuit_breaker_state = match {
        this.circuit_breaker_state == "closed" => 0,
        this.circuit_breaker_state == "half_open" => 1,
        this.circuit_breaker_state == "open" => 2,
        _ => 3
      }
    }
    
    # Track fallback usage
    if this.exists("output_method") {
      root.custom_fallback_used = if this.output_method == "fallback" { 1 } else { 0 }
    }
```

---

## Scaling and Deployment Issues

### Issue: Auto-Scaling Not Working

**Symptoms:**
- HPA not scaling pods despite high load
- Resource requests not matching usage
- Custom metrics not being collected
- Scaling events not triggering

**Diagnosis:**
```bash
# Check HPA status and events
kubectl describe hpa aggregation-hpa
kubectl get events --sort-by=.metadata.creationTimestamp | grep HorizontalPodAutoscaler

# Check metrics server
kubectl top pods -l app=time-window-aggregation
kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods | jq '.items[0].containers[0].usage'

# Verify custom metrics
kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/custom_events_per_second"
```

**Solutions:**

**1. Fix Resource Requests**
```yaml
# Ensure resource requests are set correctly
spec:
  containers:
  - name: aggregation-pipeline
    resources:
      requests:
        cpu: "500m"      # Required for CPU-based HPA
        memory: "1Gi"    # Required for memory-based HPA
      limits:
        cpu: "2000m"
        memory: "4Gi"
```

**2. Configure Custom Metrics Properly**
```yaml
# Custom metrics HPA configuration
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: aggregation-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: time-window-aggregation
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Pods
    pods:
      metric:
        name: custom_events_per_second
      target:
        type: AverageValue
        averageValue: "10000"  # Scale when > 10K events/sec per pod
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

**3. Add Metrics Collection**
```yaml
# Ensure custom metrics are exposed properly
metrics:
  prometheus:
    use_histogram_timing: true
    
  mapping: |
    # Export metrics in format expected by HPA
    if this.exists("event_count") {
      root.custom_events_per_second = this.event_count / 60.0  # Events per minute to per second
    }
```

### Issue: Rolling Deployment Failures

**Symptoms:**
- New pods failing to start
- Old pods not terminating
- Service disruption during deployments
- Health check failures

**Diagnosis:**
```bash
# Check deployment status
kubectl rollout status deployment/time-window-aggregation
kubectl get replicasets -l app=time-window-aggregation

# Check pod startup issues
kubectl describe pods -l app=time-window-aggregation | grep -A10 Events

# Check readiness/liveness probe failures
kubectl get events --field-selector reason=Unhealthy
```

**Solutions:**

**1. Optimize Health Checks**
```yaml
# Improve readiness and liveness probes
spec:
  containers:
  - name: aggregation-pipeline
    readinessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 15    # Longer startup time
      periodSeconds: 5
      timeoutSeconds: 3
      successThreshold: 1
      failureThreshold: 3
      
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 60    # Much longer for liveness
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3
```

**2. Implement Graceful Shutdown**
```yaml
# Add graceful shutdown handling
spec:
  containers:
  - name: aggregation-pipeline
    lifecycle:
      preStop:
        exec:
          command: 
          - "/bin/sh"
          - "-c"
          - "sleep 15"  # Allow time for connections to drain
          
  terminationGracePeriodSeconds: 30
```

**3. Configure Rolling Update Strategy**
```yaml
# Optimize rolling update strategy
spec:
  strategy:
    rollingUpdate:
      maxUnavailable: 1          # Conservative approach
      maxSurge: 1                # Add one pod at a time
    type: RollingUpdate
```

---

## Quick Reference Commands

### Emergency Response Commands

```bash
# Emergency scale down (high resource usage)
kubectl scale deployment time-window-aggregation --replicas=3

# Emergency scale up (high lag)
kubectl scale deployment time-window-aggregation --replicas=10

# Restart all pods (cache issues)
kubectl rollout restart deployment/time-window-aggregation

# Check system health
kubectl get pods,svc,hpa,pvc -l app=time-window-aggregation

# Emergency fallback to file output
kubectl set env deployment/time-window-aggregation FALLBACK_MODE=true
```

### Diagnostic Commands

```bash
# Performance monitoring
watch "kubectl top pods -l app=time-window-aggregation"

# Error rate monitoring  
kubectl logs -l app=time-window-aggregation --tail=100 | grep -i error | wc -l

# Cache health check
curl http://localhost:8080/metrics | grep cache

# Throughput check
curl http://localhost:8080/metrics | grep custom_events
```

### Recovery Commands

```bash
# Recover from Redis issues
kubectl delete pod -l app=redis  # Restart Redis
kubectl rollout restart deployment/time-window-aggregation

# Recover from Kafka issues
kubectl exec kafka-0 -- kafka-topics.sh --bootstrap-server kafka:9092 --list

# Clear problematic cache data
kubectl exec redis-0 -- redis-cli FLUSHDB

# Replay buffered data
kubectl exec deployment/time-window-aggregation -- /scripts/replay-buffer.sh /var/buffer/
```

## Support Escalation

### When to Escalate

Escalate to engineering support when:

1. **Data Loss Detected**
   - Event counts dropping significantly
   - Aggregations missing for extended periods
   - Data quality scores below 0.5

2. **Performance Degradation**
   - Processing latency > 60 seconds sustained
   - Throughput < 1,000 events/second with normal load
   - Memory usage > 90% across all pods

3. **System Instability**
   - Pods crash-looping
   - Circuit breakers open for > 10 minutes
   - Multiple infrastructure components failing

### Escalation Information

When escalating, provide:

```bash
# Collect diagnostic information
kubectl get all -l app=time-window-aggregation -o yaml > aggregation-deployment.yaml
kubectl logs -l app=time-window-aggregation --tail=1000 > aggregation-logs.txt
curl http://localhost:8080/metrics > aggregation-metrics.txt

# System overview
kubectl cluster-info > cluster-info.txt
kubectl top nodes > node-resources.txt
kubectl get events --sort-by=.metadata.creationTimestamp > cluster-events.txt
```

Include:
- **Timeline:** When the issue started and key events
- **Impact:** Affected metrics, users, or business operations
- **Attempted Solutions:** What has been tried already
- **Diagnostic Data:** Logs, metrics, and configuration files

---

## Prevention Best Practices

### Monitoring Setup

Implement comprehensive monitoring to catch issues early:

```yaml
# Prometheus alerting rules
groups:
- name: aggregation-alerts
  rules:
  - alert: HighEventLoss
    expr: rate(input_received_total[5m]) - rate(output_sent_total[5m]) > 1000
    for: 2m
    annotations:
      summary: "High event loss detected"
      
  - alert: HighProcessingLatency  
    expr: histogram_quantile(0.95, processor_duration_seconds_bucket) > 10
    for: 5m
    annotations:
      summary: "Processing latency too high"
```

### Regular Maintenance

Schedule regular maintenance tasks:

```bash
#!/bin/bash
# Weekly maintenance script

# Clean up old buffer files
find /var/buffer -name "*.jsonl" -mtime +7 -delete

# Redis memory optimization
redis-cli MEMORY PURGE

# Check for memory leaks
kubectl top pods -l app=time-window-aggregation --sort-by=memory

# Validate configuration
expanso validate pipeline.yaml

# Performance baseline test
./load-test.sh --duration=5m --rate=10000
```

### Configuration Management

Use GitOps for configuration management:

- Version control all configuration changes
- Implement automated testing for configuration updates  
- Use staging environments for validation
- Monitor configuration drift with alerts

---

This troubleshooting guide covers the most common issues you'll encounter with time-windowed aggregation pipelines. For issues not covered here, check the [complete pipeline documentation](./complete-aggregation-pipeline) or reach out to the community for assistance.

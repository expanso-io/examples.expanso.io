---
title: Complete Aggregation Pipeline
sidebar_label: Complete Pipeline  
sidebar_position: 9
description: Production-ready time-windowed aggregation combining all techniques with deployment guide
keywords: [complete-pipeline, deployment, production-ready, aggregation, time-windows]
---

# Complete Aggregation Pipeline

**Download and deploy a production-ready time-windowed aggregation pipeline** that combines all techniques from Steps 1-5: tumbling windows, sliding windows, session windows, multi-level aggregation, and production optimization.

## Complete Solution Overview

This production-ready pipeline provides comprehensive time-windowed aggregation capabilities:

**Input:** High-frequency IoT sensor events (100,000+ events/second)
```json
{"sensor_id": "temp_001", "temperature": 72.3, "location": "warehouse_a", "timestamp": "2025-01-15T10:30:45.123Z"}
```

**Output:** Multi-level aggregated analytics with reliability and monitoring
```json
{
  "sensor_id": "temp_001", 
  "time_bucket": "2025-01-15T10:30:00Z",
  "aggregation_type": "tumbling_5min",
  "temperature_avg": 73.2,
  "temperature_trend": "increasing",
  "data_quality_score": 0.97,
  "processing_metadata": {
    "pipeline_instance": "aggregation-prod-1",
    "reliability_score": 0.99
  }
}
```

**Features:**
- âœ… **All window types:** Tumbling, sliding, session, and multi-level
- âœ… **Production reliability:** Circuit breakers, failover, monitoring
- âœ… **Auto-scaling:** Kubernetes HPA with custom metrics
- âœ… **Comprehensive monitoring:** Prometheus, Grafana, alerting
- âœ… **Zero-downtime deployments:** Rolling updates with health checks

## Complete Pipeline Configuration

<a
  href="/files/data-transformation/aggregate-time-windows-complete.yaml"
  download
  className="button button--primary button--lg margin-bottom--md"
>
  ðŸ“¥ Download Complete Pipeline
</a>

### Core Pipeline Features

```yaml title="aggregate-time-windows-complete.yaml"
# Complete production-ready time-windowed aggregation pipeline
# Combines all techniques: tumbling, sliding, session, multi-level, and production optimization

input:
  # High-performance Kafka input with consumer optimization
  kafka:
    addresses: ["${KAFKA_BROKER_1}", "${KAFKA_BROKER_2}", "${KAFKA_BROKER_3}"]
    topics: ["sensor-events"]
    consumer_group: "aggregation-${INSTANCE_ID}"
    
    # Production consumer settings
    consumer:
      max_poll_records: 10000
      session_timeout_ms: 30000
      fetch_min_bytes: 1048576

# Production resource configuration
resources:
  caches:
    # Distributed Redis cache for tumbling windows
    tumbling_cache:
      redis:
        url: "redis://${REDIS_HOST}:6379"
        default_ttl: "120s"
        sentinel:
          master_name: "aggregation-cache"
          addresses: ["${REDIS_SENTINEL_1}", "${REDIS_SENTINEL_2}"]
    
    # Session window cache for activity clustering  
    session_cache:
      memory:
        default_ttl: "3600s"
        max_items: 50000
        
  # Rate limiting for backpressure
  rate_limits:
    input_limit:
      count: 100000
      per: "1s"

pipeline:
  processors:
    # Production input validation
    - try:
        processors:
          - json: {}
          - mapping: |
              # Comprehensive field validation
              if !this.exists("sensor_id") { error("Missing sensor_id") }
              if !this.exists("timestamp") { error("Missing timestamp") }
              if !this.exists("temperature") { error("Missing temperature") }
              
              # Data quality validation
              if this.temperature < -50 || this.temperature > 100 {
                error("Temperature out of range: " + this.temperature.string())
              }
              
              root = this
              root.processing_instance = "${INSTANCE_ID}"
              root.ingestion_time = now().ts_format("2006-01-02T15:04:05.000Z")
              
        catch:
          # Comprehensive error handling
          - mapping: |
              root = {
                "error": error(),
                "original": this,
                "instance": "${INSTANCE_ID}",
                "timestamp": now().ts_format("2006-01-02T15:04:05.000Z")
              }
          - http_client:
              url: "${ERROR_ENDPOINT}/validation-errors"
              timeout: "2s"
          - mapping: 'deleted()'

    # Multi-window aggregation branch
    - branch:
        request_map: |
          # Create parallel processing for different window types
          root = [
            # Tumbling windows (1-minute)
            this.merge({
              "window_type": "tumbling",
              "window_duration": "1m",
              "group_key": this.sensor_id + "|tumbling_1m|" + 
                          this.timestamp.parse_timestamp("2006-01-02T15:04:05.000Z").ts_format("2006-01-02T15:04:00Z")
            }),
            
            # Sliding windows (5-minute with 1-minute slide)
            this.merge({
              "window_type": "sliding", 
              "window_duration": "5m",
              "slide_interval": "1m",
              "group_key": this.sensor_id + "|sliding_5m|" +
                          this.timestamp.parse_timestamp("2006-01-02T15:04:05.000Z").ts_format("2006-01-02T15:04:00Z")
            }),
            
            # Multi-level aggregation (sensor, location, global)
            this.merge({
              "window_type": "multi_level",
              "aggregation_level": "sensor",
              "group_key": this.sensor_id + "|sensor|" +
                          this.timestamp.parse_timestamp("2006-01-02T15:04:05.000Z").ts_format("2006-01-02T15:04:00Z")
            }),
            
            this.merge({
              "window_type": "multi_level", 
              "aggregation_level": "location",
              "group_key": this.get("location").or("unknown") + "|location|" +
                          this.timestamp.parse_timestamp("2006-01-02T15:04:05.000Z").ts_format("2006-01-02T15:04:00Z")
            })
          ]
          
        processors:
          # Cache by window type and group key
          - switch:
              - check: this.window_type == "tumbling" || this.window_type == "sliding"
                processors:
                  - cache:
                      resource: tumbling_cache
                      key: ${! this.group_key }
                      value: ${! this }
                      
              - processors:
                  - cache:
                      resource: session_cache
                      key: ${! this.group_key }
                      value: ${! this }

          # Group and aggregate
          - group_by:
              - key: ${! this.group_key }
                value: ${! this }
                size: 1000

          # Comprehensive aggregation logic
          - mapping: |
              let events = this.sort_by(e -> e.timestamp)
              let first_event = events[0]
              
              # Common metadata
              root.sensor_id = first_event.sensor_id
              root.window_type = first_event.window_type
              root.time_bucket = first_event.group_key.split("|").last()
              root.aggregation_timestamp = now().ts_format("2006-01-02T15:04:05.000Z")
              
              # Event statistics
              root.event_count = events.length()
              let temperatures = events.map_each(e -> e.temperature)
              root.temperature_avg = temperatures.mean().round(2)
              root.temperature_min = temperatures.min()
              root.temperature_max = temperatures.max()
              root.temperature_stddev = temperatures.stddev().round(2)
              
              # Window-specific calculations
              if first_event.window_type == "sliding" {
                # Trend analysis for sliding windows
                if temperatures.length() >= 3 {
                  let first_temp = temperatures[0]
                  let last_temp = temperatures[-1]
                  root.temperature_trend = match {
                    last_temp > first_temp + 1.0 => "increasing",
                    last_temp < first_temp - 1.0 => "decreasing",
                    _ => "stable"
                  }
                }
              } else if first_event.window_type == "multi_level" {
                # Multi-level specific metrics
                root.aggregation_level = first_event.aggregation_level
                if first_event.aggregation_level == "location" {
                  root.location = first_event.get("location").or("unknown")
                  root.sensor_count = events.map_each(e -> e.sensor_id).unique().length()
                }
              }
              
              # Data quality indicators
              root.data_quality_score = match {
                root.event_count >= 50 && root.temperature_stddev < 5.0 => 0.95,
                root.event_count >= 30 => 0.80,
                root.event_count >= 10 => 0.60,
                _ => 0.30
              }
              
              # Processing metadata
              root.processing_metadata = {
                "pipeline_instance": first_event.processing_instance,
                "processing_duration_ms": (now() - first_event.ingestion_time.parse_timestamp("2006-01-02T15:04:05.000Z")).milliseconds(),
                "data_quality_score": root.data_quality_score
              }

# Production output with circuit breaker and failover
output:
  circuit_breaker:
    failure_threshold: 5
    success_threshold: 3
    timeout: "30s"
    
    outputs:
      # Primary: Analytics platform
      - try:
          - http_client:
              url: "${ANALYTICS_ENDPOINT}/aggregations"
              verb: POST
              headers:
                Content-Type: application/json
                Authorization: "Bearer ${ANALYTICS_API_KEY}"
                
              batching:
                count: 500
                period: "10s"
                
              timeout: "15s"
              max_retries: 2
              
        catch:
          # Log failures
          - mapping: |
              root = {
                "error_type": "output_failure",
                "error": error(),
                "aggregation": this,
                "instance": "${INSTANCE_ID}"
              }
          - http_client:
              url: "${MONITORING_ENDPOINT}/errors"
              timeout: "2s"

      # Fallback outputs
      fallback:
        - file:
            path: "/var/buffer/aggregations-${!timestamp_unix()}.jsonl"
            codec: lines

# Production monitoring
metrics:
  prometheus:
    use_histogram_timing: true
    histogram_buckets: [.001, .01, .1, 1, 10, 60]
    static_labels:
      service: "time-window-aggregation"
      instance: "${INSTANCE_ID}"
      
  mapping: |
    root = this
    if this.exists("event_count") {
      root.custom_events = this.event_count
      root.custom_quality = this.data_quality_score
    }

# Health endpoint
http:
  address: "0.0.0.0:8080"
  health_check:
    enabled: true
    path: "/health"
  metrics_path: "/metrics"

logger:
  level: INFO
  json_format: true
```

## Deployment Guide

### Prerequisites

Ensure your environment meets production requirements:

```bash
# Check Kubernetes cluster
kubectl cluster-info

# Verify required resources
kubectl get nodes
kubectl get storageclass

# Check Kafka availability  
kubectl get pods -l app=kafka -A

# Verify Redis/Sentinel setup
kubectl get pods -l app=redis -A
```

### Quick Start Deployment

Deploy the complete solution in minutes:

```bash
# 1. Download deployment manifests
curl -o k8s-manifests.yaml https://github.com/expanso-io/examples/raw/main/deployments/aggregate-time-windows-k8s.yaml

# 2. Configure environment variables
export ANALYTICS_ENDPOINT="https://your-analytics-platform.com/api"
export ANALYTICS_API_KEY="your-api-key"
export REDIS_HOST="redis-cluster-primary"

# 3. Apply Kubernetes manifests
kubectl apply -f k8s-manifests.yaml

# 4. Verify deployment
kubectl get pods -l app=time-window-aggregation
kubectl logs -l app=time-window-aggregation --tail=20

# 5. Check health status
kubectl port-forward svc/aggregation-service 8080:80 &
curl http://localhost:8080/health
```

### Production Environment Configuration

#### Kubernetes Production Setup

```yaml title="k8s-production.yaml"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: time-window-aggregation
  labels:
    app: time-window-aggregation
    version: v1.0.0
spec:
  replicas: 5  # High availability
  strategy:
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: time-window-aggregation
  template:
    metadata:
      labels:
        app: time-window-aggregation
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: aggregation-pipeline
        image: expanso/pipeline:1.0.0
        ports:
        - containerPort: 8080
          name: http-metrics
          
        env:
        - name: INSTANCE_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: ANALYTICS_ENDPOINT
          valueFrom:
            secretKeyRef:
              name: analytics-config
              key: endpoint
        - name: ANALYTICS_API_KEY
          valueFrom:
            secretKeyRef:
              name: analytics-config
              key: api-key
              
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
            
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          
        volumeMounts:
        - name: buffer-storage
          mountPath: /var/buffer
        - name: config-volume
          mountPath: /etc/pipeline
          
      volumes:
      - name: buffer-storage
        persistentVolumeClaim:
          claimName: aggregation-buffer-pvc
      - name: config-volume
        configMap:
          name: pipeline-config

---
# Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: aggregation-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: time-window-aggregation
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: custom_events_per_second
      target:
        type: AverageValue
        averageValue: "10000"
```

#### Infrastructure Requirements

**Minimum Production Setup:**
```
Kubernetes Cluster:
- 5 nodes (4 vCPU, 16GB RAM each)
- 100GB persistent storage per node
- Load balancer with health checks

Redis Cluster:
- 3 Redis instances with Sentinel
- 8GB RAM per instance
- Persistent storage for AOF

Kafka Cluster:  
- 3 Kafka brokers
- 6 partitions per topic
- Replication factor: 3
```

**Scaling Targets:**
```
Normal Load (50K events/sec):
- 3-5 aggregation instances
- Redis: 16GB total memory
- Network: 100 Mbps

Peak Load (500K events/sec):
- 15-20 aggregation instances  
- Redis: 64GB total memory
- Network: 1 Gbps
```

### Monitoring Setup

#### Prometheus Configuration

```yaml title="prometheus-config.yaml"
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'time-window-aggregation'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        action: keep
        regex: time-window-aggregation
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true

rule_files:
  - "aggregation-alerts.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

#### Grafana Dashboard

```json title="grafana-dashboard.json"
{
  "dashboard": {
    "title": "Time Window Aggregation - Production",
    "panels": [
      {
        "title": "Events per Second",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(custom_events[1m])",
            "legendFormat": "{{instance}} events/sec"
          }
        ]
      },
      {
        "title": "Data Quality Score",
        "type": "singlestat",
        "targets": [
          {
            "expr": "avg(custom_quality)",
            "legendFormat": "Avg Quality"
          }
        ]
      },
      {
        "title": "Processing Latency",
        "type": "histogram",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, processor_duration_seconds_bucket)",
            "legendFormat": "95th percentile"
          }
        ]
      }
    ]
  }
}
```

### Performance Validation

#### Load Testing

```bash
#!/bin/bash
# Production load test
echo "ðŸš€ Starting production load test..."

# Generate high-volume test events
cat > load-test.py << 'EOF'
import json
import time
import requests
from concurrent.futures import ThreadPoolExecutor

def send_events(batch_id):
    events = []
    for i in range(1000):
        event = {
            "sensor_id": f"load_test_{batch_id:03d}_{i:03d}",
            "temperature": 70 + (i % 20),
            "location": f"warehouse_{batch_id % 5}",
            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S.000Z")
        }
        events.append(event)
    
    # Send batch to Kafka
    response = requests.post(
        "http://localhost:9092/events",
        json=events,
        timeout=5
    )
    return response.status_code

# Run load test: 100K events/sec for 5 minutes
with ThreadPoolExecutor(max_workers=100) as executor:
    for batch in range(3000):  # 300 batches * 1000 events = 300K events
        executor.submit(send_events, batch)
        time.sleep(0.1)  # 10 batches/sec * 1000 events = 10K events/sec

print("âœ… Load test completed")
EOF

python load-test.py
```

#### Performance Metrics

```bash
# Check pipeline performance
kubectl top pods -l app=time-window-aggregation

# Monitor processing rates
curl http://localhost:8080/metrics | grep custom_events

# Check data quality
curl http://localhost:8080/metrics | grep custom_quality

# Verify auto-scaling
kubectl get hpa aggregation-hpa
```

Expected performance targets:
```
Throughput: 100,000+ events/sec sustained
Latency: < 5 seconds end-to-end
CPU Usage: < 80% during normal operations
Memory Usage: < 85% during normal operations
Data Quality: > 95% score average
Availability: > 99.9% uptime
```

## Configuration Options

### Environment Variables

```bash
# Required configuration
export ANALYTICS_ENDPOINT="https://analytics.company.com/api/v1"
export ANALYTICS_API_KEY="your-analytics-api-key"
export REDIS_HOST="redis-cluster-primary.redis.svc.cluster.local"

# Optional configuration  
export KAFKA_BROKERS="kafka-1:9092,kafka-2:9092,kafka-3:9092"
export INSTANCE_ID="${HOSTNAME}"
export LOG_LEVEL="INFO"
export METRICS_INTERVAL="15s"

# Performance tuning
export MAX_EVENTS_PER_SECOND="50000"
export CACHE_TTL="120s"
export BATCH_SIZE="500"
export WORKER_THREADS="4"
```

### Custom Configuration

```yaml title="custom-config.yaml"
# Override default settings
pipeline_config:
  # Window configurations
  tumbling_window_duration: "5m"     # Default: 1m
  sliding_window_duration: "15m"     # Default: 5m  
  sliding_window_slide: "5m"         # Default: 1m
  session_gap_threshold: "10m"       # Default: 5m
  
  # Performance settings
  cache_max_items: 200000            # Default: 100000
  batch_processing_size: 1000        # Default: 500
  worker_concurrency: 8              # Default: 4
  
  # Data quality thresholds
  min_events_per_window: 10          # Default: 1
  max_temperature_range: 50          # Default: 100
  quality_score_threshold: 0.8       # Default: 0.6
```

## Production Checklist

### Pre-Deployment Validation

- [ ] **Environment Setup**
  - [ ] Kubernetes cluster configured with sufficient resources
  - [ ] Redis cluster deployed with high availability
  - [ ] Kafka cluster configured with replication
  - [ ] Monitoring stack (Prometheus + Grafana) deployed

- [ ] **Configuration Validation**  
  - [ ] Environment variables set correctly
  - [ ] API keys and secrets configured
  - [ ] Network connectivity verified
  - [ ] Pipeline configuration validated

- [ ] **Performance Testing**
  - [ ] Load testing completed successfully
  - [ ] Auto-scaling validated under load
  - [ ] Circuit breaker testing completed
  - [ ] Failover scenarios tested

### Post-Deployment Verification

- [ ] **Health Checks**
  - [ ] All pods running and ready
  - [ ] Health endpoints returning 200 OK
  - [ ] Metrics being collected successfully
  - [ ] Logs structured and accessible

- [ ] **Data Flow Validation**
  - [ ] Events flowing through pipeline
  - [ ] Aggregations being generated correctly
  - [ ] Output reaching analytics platform
  - [ ] Data quality scores meeting targets

- [ ] **Monitoring and Alerting**
  - [ ] Dashboards displaying metrics
  - [ ] Alerts configured and firing appropriately  
  - [ ] On-call procedures documented
  - [ ] Runbook available for operations team

## Getting Help

### Documentation Resources

- [**Troubleshooting Guide**](./troubleshooting) - Common issues and solutions
- [**Interactive Explorer**](./explorer) - Visual pipeline exploration
- [**Step-by-Step Tutorials**](./setup) - Learn each technique individually

### Community Support

- **GitHub Issues:** Report bugs and request features
- **Discord Community:** Real-time discussion and support  
- **Stack Overflow:** Tag questions with `expanso` and `time-windows`

### Enterprise Support

For production deployments requiring enterprise support:
- **Professional Services:** Architecture review and deployment assistance
- **24/7 Support:** Critical issue resolution and monitoring
- **Custom Development:** Specialized features and integrations

## Next Steps

Your complete aggregation pipeline is now ready for production! 

**Immediate actions:**
1. Download the complete pipeline configuration
2. Follow the deployment guide for your environment
3. Configure monitoring and alerting
4. Run performance validation tests

**Ongoing operations:**
- Monitor data quality scores and processing latency
- Review scaling patterns and adjust HPA settings
- Implement additional custom business logic
- Plan for capacity growth and geographic expansion

<div style={{display: 'flex', gap: '1rem', marginTop: '2rem', marginBottom: '2rem', flexWrap: 'wrap'}}>
  <a href="./troubleshooting" className="button button--primary button--lg" style={{flex: '1', minWidth: '200px'}}>
    Troubleshooting Guide
  </a>
  <a href="../deduplicate-events/" className="button button--secondary button--lg" style={{flex: '1', minWidth: '200px'}}>
    Explore Related Examples
  </a>
</div>

---
title: "Step 2: Parse CSV Data"
sidebar_label: "Step 2: Parse CSV Data"  
sidebar_position: 5
description: Process CSV sensor data with column mapping, type conversion, validation rules, and sensor metadata enrichment
keywords: [csv parsing, sensor data, column mapping, type conversion, data validation, iot logs]
---

# Step 2: Parse CSV Data

**Transform CSV sensor data into typed, validated objects with enrichment**. This step teaches you to parse CSV files from IoT sensors, export systems, and tabular data sources using the `csv` processor with column mapping and validation.

## What You'll Build

A CSV data parser that processes sensor logs and tabular data with these capabilities:

✅ **Column mapping** - Map CSV columns to named fields  
✅ **Type conversion** - Convert strings to numbers, booleans, and dates  
✅ **Data validation** - Validate ranges, formats, and required fields  
✅ **Sensor enrichment** - Add metadata based on sensor IDs and types  
✅ **Alert generation** - Create alerts for threshold violations  
✅ **Data quality** - Track completeness and accuracy metrics

## CSV Data Examples

Common CSV formats from different systems:

### IoT Sensor Data
```csv
timestamp,metric_name,sensor_id,value,unit,location
2025-10-20T14:23:45Z,temperature,temp-sensor-01,35.5,celsius,warehouse-a
2025-10-20T14:24:45Z,humidity,humid-sensor-01,82.1,percent,storage-b
2025-10-20T14:25:45Z,pressure,press-sensor-01,1013.25,hpa,factory-floor
```

### Application Metrics Export
```csv
date,application,cpu_usage,memory_mb,disk_io,network_kb
2025-10-20,api-server,45.2,512,1024,2048
2025-10-20,web-frontend,23.1,256,512,4096
2025-10-20,database,78.9,2048,8192,1024
```

### User Activity Data
```csv
user_id,session_id,action,timestamp,duration_sec,success
user123,sess456,login,2025-10-20T14:23:45Z,2.1,true
user123,sess456,api_call,2025-10-20T14:23:47Z,0.8,true
user789,sess789,login,2025-10-20T14:24:12Z,15.2,false
```

## Implementation

### Basic CSV Parser

Start with a simple CSV parser that maps columns to fields:

```yaml title="csv-parser-basic.yaml"
name: csv-parser-basic
description: Parse CSV sensor data with column mapping
type: pipeline
namespace: default

config:
  input:
    file:
      paths:
        - "~/expanso-logs/sensors/*.csv"
        - "/var/log/sensors/*.csv"
      codec: lines

  pipeline:
    processors:
      # Parse CSV with named columns
      - csv:
          columns:
            - timestamp
            - metric_name
            - sensor_id
            - value
            - unit
            - location
          skip_header_rows: 1  # Skip CSV header
          delimiter: ","

      # Basic field validation
      - mapping: |
          root = this
          # Ensure required fields exist
          root.timestamp = this.timestamp.or("1970-01-01T00:00:00Z")
          root.metric_name = this.metric_name.or("unknown")
          root.sensor_id = this.sensor_id.or("unknown-sensor")
          root.value = this.value.or("0")
          root.unit = this.unit.or("unknown")
          root.location = this.location.or("unknown")

  output:
    file:
      path: ~/expanso-logs/output/parsed-csv-basic.jsonl
      codec: lines
```

Deploy and test:

```bash
# Deploy the parser
expanso job deploy csv-parser-basic.yaml

# Check processing
tail -f ~/expanso-logs/output/parsed-csv-basic.jsonl
```

### Enhanced CSV Parser with Type Conversion

Add type conversion and timestamp normalization:

```yaml title="csv-parser-enhanced.yaml"
name: csv-parser-enhanced
description: Parse CSV data with type conversion and validation
type: pipeline
namespace: default

config:
  input:
    file:
      paths:
        - "~/expanso-logs/sensors/*.csv"
      codec: lines

  pipeline:
    processors:
      # Parse CSV with named columns
      - csv:
          columns:
            - timestamp
            - metric_name
            - sensor_id
            - value
            - unit
            - location
          skip_header_rows: 1
          delimiter: ","

      # Type conversion and timestamp parsing
      - mapping: |
          root = this
          
          # Parse timestamp (try multiple formats)
          root.timestamp_unix = this.timestamp.parse_timestamp("2006-01-02T15:04:05Z").catch(
            this.timestamp.parse_timestamp("2006-01-02 15:04:05").catch(
              this.timestamp.parse_timestamp("2006-01-02").catch(
                now().ts_unix()
              )
            )
          ).ts_unix()
          
          # Keep original timestamp
          root.timestamp_original = this.timestamp
          
          # Convert value to number with validation
          root.value_numeric = this.value.number().catch(-999.0)
          root.value_original = this.value
          
          # Validate value range (basic check)
          root.value_valid = this.value_numeric > -999.0 && this.value_numeric < 999999.0

      # Field normalization
      - mapping: |
          root = this
          
          # Normalize metric name
          root.metric_name = this.metric_name.lowercase().replace(" ", "_")
          
          # Normalize sensor ID format
          root.sensor_id = this.sensor_id.lowercase()
          
          # Normalize unit
          root.unit = this.unit.lowercase()
          
          # Normalize location
          root.location = this.location.lowercase().replace(" ", "_")

      # Add metadata
      - mapping: |
          root = this
          root.metadata = {
            "parsed_by": "csv-parser",
            "parsed_at": now().ts_unix(),
            "source_format": "csv",
            "row_length": this.keys().length()
          }

  output:
    file:
      path: ~/expanso-logs/output/parsed-csv-enhanced.jsonl
      codec: lines
```

### CSV Parser with Sensor Enrichment

Add sensor metadata lookup and validation rules:

```yaml title="csv-sensor-enricher.yaml"
name: csv-sensor-enricher
description: Parse CSV sensor data with enrichment and alerting
type: pipeline
namespace: default

config:
  input:
    file:
      paths:
        - "~/expanso-logs/sensors/*.csv"
      codec: lines

  pipeline:
    processors:
      # Parse CSV
      - csv:
          columns: [timestamp, metric_name, sensor_id, value, unit, location]
          skip_header_rows: 1
          delimiter: ","

      # Type conversion
      - mapping: |
          root = this
          root.timestamp_unix = this.timestamp.parse_timestamp("2006-01-02T15:04:05Z").ts_unix()
          root.value_numeric = this.value.number().catch(0.0)

      # Sensor metadata enrichment (lookup table)
      - mapping: |
          root = this
          
          # Sensor metadata based on sensor ID patterns
          let sensor_metadata = if this.sensor_id.has_prefix("temp-") {
            {
              "type": "temperature",
              "category": "environmental",
              "criticality": "high",
              "min_value": -40.0,
              "max_value": 85.0,
              "normal_range": {"min": 18.0, "max": 25.0},
              "alert_threshold": 30.0,
              "maintenance_interval_days": 30
            }
          } else if this.sensor_id.has_prefix("humid-") {
            {
              "type": "humidity", 
              "category": "environmental",
              "criticality": "medium",
              "min_value": 0.0,
              "max_value": 100.0,
              "normal_range": {"min": 30.0, "max": 70.0},
              "alert_threshold": 80.0,
              "maintenance_interval_days": 45
            }
          } else if this.sensor_id.has_prefix("press-") {
            {
              "type": "pressure",
              "category": "environmental", 
              "criticality": "low",
              "min_value": 800.0,
              "max_value": 1200.0,
              "normal_range": {"min": 1000.0, "max": 1030.0},
              "alert_threshold": 1050.0,
              "maintenance_interval_days": 60
            }
          } else if this.sensor_id.has_prefix("flow-") {
            {
              "type": "flow_rate",
              "category": "industrial",
              "criticality": "critical",
              "min_value": 0.0,
              "max_value": 1000.0,
              "normal_range": {"min": 10.0, "max": 100.0},
              "alert_threshold": 150.0,
              "maintenance_interval_days": 14
            }
          } else {
            {
              "type": "unknown",
              "category": "unclassified",
              "criticality": "low",
              "min_value": 0.0,
              "max_value": 1000.0,
              "alert_threshold": 999.0,
              "maintenance_interval_days": 365
            }
          }
          
          root.sensor_metadata = sensor_metadata

      # Data validation and quality assessment
      - mapping: |
          root = this
          
          let meta = this.sensor_metadata
          let value = this.value_numeric
          
          # Validate value range
          root.validation = {
            "in_physical_range": value >= meta.min_value && value <= meta.max_value,
            "in_normal_range": value >= meta.normal_range.min && value <= meta.normal_range.max,
            "above_alert_threshold": value > meta.alert_threshold,
            "is_numeric": this.value.number().catch(-999) > -999,
            "timestamp_valid": this.timestamp_unix > 1600000000  # After 2020
          }
          
          # Calculate data quality score
          let checks_passed = [
            root.validation.in_physical_range,
            root.validation.is_numeric,
            root.validation.timestamp_valid,
            this.sensor_id != "unknown-sensor",
            this.metric_name != "unknown"
          ].filter(check -> check == true).length()
          
          root.data_quality_score = checks_passed / 5.0

      # Generate alerts for threshold violations
      - mapping: |
          root = this
          
          # Create alert if value exceeds threshold
          root.alert = if this.validation.above_alert_threshold {
            {
              "level": if this.sensor_metadata.criticality == "critical" {
                "critical"
              } else if this.sensor_metadata.criticality == "high" {
                "warning"
              } else {
                "info"
              },
              "message": "Sensor value %s exceeds threshold %s".format(
                this.value_numeric, 
                this.sensor_metadata.alert_threshold
              ),
              "sensor_id": this.sensor_id,
              "current_value": this.value_numeric,
              "threshold": this.sensor_metadata.alert_threshold,
              "location": this.location,
              "requires_action": this.sensor_metadata.criticality in ["critical", "high"]
            }
          }

      # Location and environmental context
      - mapping: |
          root = this
          
          # Enrich with location metadata (in production, use lookup service)
          root.location_metadata = if this.location.contains("warehouse") {
            {
              "zone": "storage",
              "building": "main",
              "floor": 1,
              "climate_controlled": true,
              "business_hours": "24/7"
            }
          } else if this.location.contains("factory") {
            {
              "zone": "production", 
              "building": "manufacturing",
              "floor": 1,
              "climate_controlled": false,
              "business_hours": "6am-6pm"
            }
          } else {
            {
              "zone": "unknown",
              "building": "unknown",
              "floor": 0,
              "climate_controlled": false,
              "business_hours": "unknown"
            }
          }

  output:
    broker:
      pattern: fan_out
      outputs:
        # All sensor data to time-series database
        - file:
            path: ~/expanso-logs/output/sensor-data-enriched.jsonl
            codec: lines

        # Alerts to notification system
        - switch:
            - check: this.exists("alert")
              output:
                file:
                  path: ~/expanso-logs/output/sensor-alerts.jsonl
                  codec: lines
            - output:
                drop: {}

        # Data quality issues to monitoring
        - switch:
            - check: this.data_quality_score < 0.8
              output:
                file:
                  path: ~/expanso-logs/output/data-quality-issues.jsonl
                  codec: lines
            - output:
                drop: {}
```

## Advanced Features

### CSV Anomaly Detection

Add basic anomaly detection for sensor values:

```yaml title="csv-anomaly-detector.yaml"
name: csv-anomaly-detector
description: Detect anomalies in CSV sensor data using statistical methods
type: pipeline
namespace: default

config:
  input:
    file:
      paths:
        - "~/expanso-logs/sensors/*.csv"
      codec: lines

  pipeline:
    processors:
      # Parse CSV
      - csv:
          columns: [timestamp, metric_name, sensor_id, value, unit, location]
          skip_header_rows: 1

      # Convert types
      - mapping: |
          root = this
          root.timestamp_unix = this.timestamp.parse_timestamp("2006-01-02T15:04:05Z").ts_unix()
          root.value_numeric = this.value.number()

      # Basic anomaly detection (z-score method)
      # In production, use sliding window statistics
      - mapping: |
          root = this
          
          let value = this.value_numeric
          
          # Define expected ranges by sensor type (simplified)
          let expected_stats = if this.metric_name == "temperature" {
            {"mean": 22.0, "std_dev": 5.0}  # Room temperature baseline
          } else if this.metric_name == "humidity" {
            {"mean": 50.0, "std_dev": 15.0}  # Moderate humidity baseline
          } else if this.metric_name == "pressure" {
            {"mean": 1013.0, "std_dev": 20.0}  # Sea level pressure baseline
          } else {
            {"mean": value, "std_dev": 1.0}  # No baseline for unknown metrics
          }
          
          # Calculate z-score
          let z_score = (value - expected_stats.mean) / expected_stats.std_dev
          
          root.anomaly_analysis = {
            "z_score": z_score,
            "is_anomaly": z_score.abs() > 2.0,  # 2 standard deviations
            "anomaly_severity": if z_score.abs() > 3.0 {
              "severe"
            } else if z_score.abs() > 2.0 {
              "moderate"
            } else {
              "normal"
            },
            "expected_range": {
              "min": expected_stats.mean - (expected_stats.std_dev * 2),
              "max": expected_stats.mean + (expected_stats.std_dev * 2)
            }
          }

      # Generate anomaly alerts
      - mapping: |
          root = this
          
          root.anomaly_alert = if this.anomaly_analysis.is_anomaly {
            {
              "type": "statistical_anomaly",
              "severity": this.anomaly_analysis.anomaly_severity,
              "message": "Sensor value %.2f is %.1f standard deviations from expected %.2f".format(
                this.value_numeric,
                this.anomaly_analysis.z_score.abs(),
                this.anomaly_analysis.expected_range
              ),
              "recommended_action": if this.anomaly_analysis.anomaly_severity == "severe" {
                "immediate_investigation"
              } else {
                "monitor_trend"
              }
            }
          }

  output:
    broker:
      pattern: fan_out
      outputs:
        # All data with anomaly analysis
        - file:
            path: ~/expanso-logs/output/sensor-anomaly-analysis.jsonl
            codec: lines

        # Anomaly alerts only
        - switch:
            - check: this.anomaly_analysis.is_anomaly == true
              output:
                file:
                  path: ~/expanso-logs/output/anomaly-alerts.jsonl
                  codec: lines
            - output:
                drop: {}
```

### CSV Aggregation and Windowing

Aggregate sensor data into time windows for trend analysis:

```yaml title="csv-aggregator.yaml"
name: csv-aggregator
description: Aggregate CSV sensor data into time windows
type: pipeline
namespace: default

config:
  input:
    file:
      paths:
        - "~/expanso-logs/sensors/*.csv"
      codec: lines

  pipeline:
    processors:
      # Parse CSV
      - csv:
          columns: [timestamp, metric_name, sensor_id, value, unit, location]
          skip_header_rows: 1

      # Convert types and add windowing metadata
      - mapping: |
          root = this
          root.timestamp_unix = this.timestamp.parse_timestamp("2006-01-02T15:04:05Z").ts_unix()
          root.value_numeric = this.value.number()
          
          # Create time windows (5-minute intervals)
          let window_size = 300  # 5 minutes in seconds
          root.time_window = (this.timestamp_unix / window_size).floor() * window_size
          root.time_window_iso = root.time_window.ts_format("2006-01-02T15:04:05Z")
          
          # Create grouping key for aggregation
          root.group_key = "%s_%s_%s".format(this.sensor_id, this.metric_name, root.time_window)

      # Basic aggregation logic (simplified)
      # In production, use proper windowing processor
      - mapping: |
          root = this
          
          # Add record to aggregation context (simplified)
          root.aggregation_data = {
            "sensor_id": this.sensor_id,
            "metric_name": this.metric_name,
            "location": this.location,
            "time_window": this.time_window,
            "time_window_iso": this.time_window_iso,
            "value": this.value_numeric,
            "timestamp": this.timestamp_unix
          }

      # Create summary statistics per record
      # (In production, use actual aggregation processor)
      - mapping: |
          root = this
          
          # Calculate basic stats (this is a single record, 
          # actual aggregation would collect multiple records)
          root.window_statistics = {
            "count": 1,
            "sum": this.value_numeric,
            "avg": this.value_numeric,
            "min": this.value_numeric,
            "max": this.value_numeric,
            "first_timestamp": this.timestamp_unix,
            "last_timestamp": this.timestamp_unix,
            "window_start": this.time_window,
            "window_end": this.time_window + 300
          }

  output:
    file:
      path: ~/expanso-logs/output/sensor-aggregated.jsonl
      codec: lines
```

## Testing and Validation

### Create Test CSV Data

Generate diverse CSV samples for testing:

```bash
# Create test sensor data with various edge cases
cat > ~/expanso-logs/sensors/test-sensors.csv << 'EOF'
timestamp,metric_name,sensor_id,value,unit,location
2025-10-20T14:00:00Z,temperature,temp-sensor-01,22.5,celsius,warehouse_a
2025-10-20T14:01:00Z,temperature,temp-sensor-01,35.8,celsius,warehouse_a
2025-10-20T14:02:00Z,temperature,temp-sensor-01,invalid,celsius,warehouse_a
2025-10-20T14:03:00Z,humidity,humid-sensor-01,85.2,percent,storage_b
2025-10-20T14:04:00Z,humidity,humid-sensor-01,45.0,percent,storage_b
2025-10-20T14:05:00Z,pressure,press-sensor-01,1013.25,hpa,factory_floor
2025-10-20T14:06:00Z,flow_rate,flow-sensor-01,175.5,lpm,production_line
2025-10-20T14:07:00Z,unknown_metric,unknown-sensor,100.0,unknown,unknown_location
invalid_timestamp,temperature,temp-sensor-02,25.0,celsius,warehouse_b
2025-10-20T14:09:00Z,temperature,temp-sensor-03,-999.0,celsius,freezer_unit
EOF

# Create malformed CSV for error testing
cat > ~/expanso-logs/sensors/malformed.csv << 'EOF'
timestamp,metric_name,sensor_id,value,unit
2025-10-20T14:00:00Z,temperature,temp-sensor-01,22.5
incomplete,row,here
too,many,columns,in,this,row,extra,data
"quoted,field,with,commas",temperature,temp-sensor-01,23.0,celsius
EOF
```

### Test Parsing Results

Validate different scenarios:

```bash
# Deploy enhanced parser
expanso job deploy csv-sensor-enricher.yaml

# Wait for processing
sleep 10

# Check enriched sensor data
echo "=== Enriched Sensor Data ==="
head -2 ~/expanso-logs/output/sensor-data-enriched.jsonl | jq .

# Check generated alerts
echo "=== Sensor Alerts ==="
cat ~/expanso-logs/output/sensor-alerts.jsonl | jq .

# Check data quality issues
echo "=== Data Quality Issues ==="
cat ~/expanso-logs/output/data-quality-issues.jsonl | jq .

# Verify type conversion
echo "=== Type Conversion Results ==="
jq -r '.value_numeric' ~/expanso-logs/output/sensor-data-enriched.jsonl | head -5

# Check sensor metadata enrichment
echo "=== Sensor Metadata ==="
jq '.sensor_metadata.type' ~/expanso-logs/output/sensor-data-enriched.jsonl | sort | uniq -c
```

### Performance Testing

Test parser performance with large CSV files:

```bash
# Generate large CSV file (50k rows)
echo "timestamp,metric_name,sensor_id,value,unit,location" > ~/expanso-logs/sensors/performance-test.csv
for i in {1..50000}; do
  ts=$(date -u -d "@$((1729433025 + i * 60))" +%Y-%m-%dT%H:%M:%SZ)
  sensor_num=$((i % 10 + 1))
  value=$((RANDOM % 100)).$(($RANDOM % 100))
  echo "$ts,temperature,temp-sensor-$(printf "%02d" $sensor_num),$value,celsius,warehouse_$(($sensor_num % 3 + 1))"
done >> ~/expanso-logs/sensors/performance-test.csv

# Deploy and monitor performance
expanso job deploy csv-sensor-enricher.yaml

# Monitor processing
watch "expanso job stats csv-sensor-enricher | grep -E 'Processed|Rate|Memory'"

# Check output size
wc -l ~/expanso-logs/output/sensor-data-enriched.jsonl
du -h ~/expanso-logs/output/sensor-data-enriched.jsonl
```

## Production Considerations

### Error Handling for Malformed CSV

Handle various CSV parsing errors:

```yaml
# Add error handling processors
- catch:
    - mapping: |
        root = {
          "error_type": "csv_parse_error",
          "error": error(),
          "failed_at": now().ts_unix(),
          "original_line": this.string().slice(0, 500),
          "line_length": this.string().length()
        }
    - output:
        file:
          path: ~/expanso-logs/errors/csv-parse-errors.jsonl
          codec: lines

# Handle missing columns
- mapping: |
    root = this
    # Ensure all expected columns exist
    let expected_columns = ["timestamp", "metric_name", "sensor_id", "value", "unit", "location"]
    root.missing_columns = expected_columns.filter(col -> !this.exists(col))
    root.has_missing_columns = root.missing_columns.length() > 0
```

### Data Quality Monitoring

Track data quality metrics:

```yaml
# Add data quality assessment
- mapping: |
    root = this
    
    # Calculate completeness score
    let total_fields = 6  # Expected number of fields
    let present_fields = [
      this.exists("timestamp") && this.timestamp != "",
      this.exists("metric_name") && this.metric_name != "", 
      this.exists("sensor_id") && this.sensor_id != "",
      this.exists("value") && this.value != "",
      this.exists("unit") && this.unit != "",
      this.exists("location") && this.location != ""
    ].filter(present -> present == true).length()
    
    root.quality_metrics = {
      "completeness_score": present_fields / total_fields,
      "timestamp_valid": this.timestamp.parse_timestamp("2006-01-02T15:04:05Z").catch(null) != null,
      "value_numeric": this.value.number().catch(null) != null,
      "sensor_id_format_valid": this.sensor_id.re_match("^[a-z]+-[a-z]+-[0-9]+$"),
      "overall_quality": if present_fields >= 5 && this.value.number().catch(null) != null {
        "good"
      } else if present_fields >= 4 {
        "acceptable" 
      } else {
        "poor"
      }
    }
```

### Performance Optimization

Optimize for high-volume CSV processing:

```yaml
# Add performance optimizations
processors:
  # Batch processing for efficiency
  - archive:
      format: lines
      
  # Filter out header rows efficiently
  - mapping: |
      root = if this.string().has_prefix("timestamp,") {
        deleted()
      } else {
        this
      }
      
  # Parallel processing hint
  - parallel:
      cap: 4  # Process up to 4 records in parallel
      processors:
        - csv:
            columns: [timestamp, metric_name, sensor_id, value, unit, location]
```

---

## Summary

You've built a comprehensive CSV parser that handles:

✅ **Column mapping** with flexible delimiter support  
✅ **Type conversion** with validation and error recovery  
✅ **Sensor enrichment** with metadata lookup tables  
✅ **Data quality** assessment and monitoring  
✅ **Alert generation** for threshold violations and anomalies  
✅ **Performance optimization** for high-volume processing

## Common Issues

**Issue:** Memory usage increases with large CSV files
**Solution:** Use streaming processing and batch outputs

**Issue:** Type conversion failures on invalid data  
**Solution:** Use `.catch()` with default values and track errors

**Issue:** Inconsistent column counts
**Solution:** Validate column count and handle missing fields gracefully

---

## Next Steps

<div style={{display: 'flex', gap: '1.5rem', marginTop: '2rem', marginBottom: '3rem', flexWrap: 'wrap', justifyContent: 'flex-start'}}>
  <a href="./step-3-parse-access-logs" className="button button--primary button--lg" style={{display: 'inline-flex', alignItems: 'center', justifyContent: 'center', textDecoration: 'none', borderRadius: '8px', padding: '1rem 2rem', fontWeight: '600', minWidth: '240px', boxShadow: '0 2px 8px rgba(0,0,0,0.15)', cursor: 'pointer', transition: 'all 0.2s ease'}}>
    Step 3: Parse Access Logs
  </a>
  <a href="./troubleshooting" className="button button--secondary button--lg" style={{display: 'inline-flex', alignItems: 'center', justifyContent: 'center', textDecoration: 'none', borderRadius: '8px', padding: '1rem 2rem', fontWeight: '600', minWidth: '240px', boxShadow: '0 2px 8px rgba(0,0,0,0.15)', cursor: 'pointer', transition: 'all 0.2s ease'}}>
    Troubleshooting Guide
  </a>
</div>

**Next:** Learn to parse web server access logs using grok patterns for traffic analysis.

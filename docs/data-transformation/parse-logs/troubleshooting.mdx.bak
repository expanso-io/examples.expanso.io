---
title: Log Parser Troubleshooting Guide
sidebar_label: Troubleshooting
sidebar_position: 10
description: Comprehensive troubleshooting guide for log parser issues including format detection, performance, errors, and production problems
keywords: [troubleshooting, debugging, log parser, format detection, performance, error handling]
---

# Log Parser Troubleshooting Guide

**Diagnose and resolve common log parsing issues**. This comprehensive guide covers format detection problems, performance issues, parsing errors, and production troubleshooting scenarios with step-by-step solutions.

## Quick Diagnosis

### Check Parser Status

```bash
# Check if parser is running
expanso job status complete-log-parser

# View recent errors
expanso job logs complete-log-parser --level error --tail 50

# Check processing statistics
expanso job stats complete-log-parser

# Monitor real-time processing
expanso job metrics complete-log-parser --metric throughput --follow
```

### Common Symptoms

| Symptom | Likely Cause | Quick Fix |
|---------|--------------|-----------|
| No logs processed | Input configuration issue | Check file paths and permissions |
| Low success rate | Format detection problems | Review detection patterns |
| High memory usage | Large log files or memory leaks | Adjust batch sizes |
| Slow processing | CPU bottleneck or blocking I/O | Scale horizontally |
| Missing output | Output endpoint issues | Test connectivity |

---

## Format Detection Issues

### Issue: JSON Logs Not Detected

**Symptoms:**
- JSON logs appear in unknown format DLQ
- Low confidence scores for JSON detection
- Parsing failures for valid JSON

**Diagnosis:**
```bash
# Check detection results
jq '.detection_result' ~/expanso-logs/output/poor-quality-dlq.jsonl | head -5

# Analyze failed JSON
grep -o '^[^{]*{[^}]*}' ~/expanso-logs/app/*.jsonl | head -3

# Test JSON validity
echo '{"test": "json"}' | jq .
```

**Solutions:**

**1. Fix malformed JSON**
```yaml
# Add JSON repair processor
- mapping: |
    root = this.original_content.re_replace_all("([{,]\\s*)(\\w+)\\s*:", "$1\"$2\":").parse_json().catch({
      "raw": this.original_content,
      "error": "json_repair_failed"
    })
```

**2. Adjust detection sensitivity**
```yaml
# Lower JSON detection threshold
let json_score = if content.has_prefix("{") || content.has_prefix("[") {
  0.3  # Reduced from 0.4
} else if content.contains('"') && content.contains(":") {
  0.2  # Reduced from 0.3
} else {
  0.0
}
```

**3. Handle JSON arrays**
```yaml
# Support JSONL (one object per line)
- json_documents:
    parts: []
- split:
    size: 1  # Process one JSON object at a time
```

### Issue: CSV Detection Conflicts

**Symptoms:**
- CSV data detected as other formats
- Comma-separated text in JSON/logs incorrectly classified
- Inconsistent column parsing

**Diagnosis:**
```bash
# Check CSV structure
head -5 ~/expanso-logs/sensors/*.csv
awk -F',' '{print NF}' ~/expanso-logs/sensors/*.csv | sort | uniq -c

# Analyze detection patterns
jq -r 'select(.detection_result.format == "csv") | .detection_result.all_scores' ~/expanso-logs/output/processed-csv.jsonl
```

**Solutions:**

**1. Improve CSV detection patterns**
```yaml
# Enhanced CSV scoring
let csv_score = if comma_count >= 2 && brace_count == 0 && angle_count == 0 {
  let consistency_bonus = if content.split("\n").all(line -> line.count(",") == comma_count / content.split("\n").length()) {
    0.2  # Bonus for consistent column count
  } else {
    0.0
  }
  0.4 + comma_count / len * 20 + consistency_bonus
} else {
  0.0
}
```

**2. Add header detection**
```yaml
# Detect CSV headers
- mapping: |
    root = this
    let first_line = this.original_content.split("\n").index(0)
    root.has_csv_header = first_line.re_match("^[a-zA-Z][a-zA-Z0-9_,\\s]*$") && 
                         first_line.count(",") >= 2
```

**3. Handle quoted CSV fields**
```yaml
# Configure CSV parser for quoted fields
- csv:
    columns: ["col1", "col2", "col3", "col4", "col5"]
    delimiter: ","
    quote: '"'
    escape: '"'
    comment: "#"
```

### Issue: Access Log Misclassification

**Symptoms:**
- Access logs detected as unknown format
- Grok patterns failing to match
- Missing fields in parsed output

**Diagnosis:**
```bash
# Test grok patterns
echo '203.0.113.45 - - [20/Oct/2025:14:23:45 +0000] "GET / HTTP/1.1" 200 1234' | \
  expanso test grok '%{IPORHOST:client_ip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:method} %{NOTSPACE:request}(?: HTTP/%{NUMBER:http_version})?)" %{NUMBER:status_code} (?:%{NUMBER:bytes}|-)'

# Check access log formats
head -3 ~/expanso-logs/nginx/*.log
tail -3 ~/expanso-logs/nginx/*.log
```

**Solutions:**

**1. Add more grok patterns**
```yaml
# Support multiple access log formats
- grok:
    expressions:
      # Standard combined format
      - '%{IPORHOST:client_ip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:method} %{NOTSPACE:request}(?: HTTP/%{NUMBER:http_version})?|%{DATA:invalid_request})" %{NUMBER:status_code} (?:%{NUMBER:bytes}|-) "(?:%{DATA:referer}|-)" "(?:%{DATA:user_agent}|-)"'
      # Common format
      - '%{IPORHOST:client_ip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:method} %{NOTSPACE:request}(?: HTTP/%{NUMBER:http_version})?|%{DATA:invalid_request})" %{NUMBER:status_code} (?:%{NUMBER:bytes}|-)'
      # With response time
      - '%{IPORHOST:client_ip} - %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:method} %{NOTSPACE:request})" %{NUMBER:status_code} %{NUMBER:bytes} %{NUMBER:response_time}'
      # Custom format
      - '%{IPORHOST:client_ip} - - \[%{HTTPDATE:timestamp}\] ".*" %{NUMBER:status_code} %{NUMBER:bytes}'
    named_captures_only: true
```

**2. Improve access log detection**
```yaml
# Better access log scoring
let access_log_score = if content.re_match("\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}") {
  let base_score = 0.3
  let timestamp_bonus = if content.re_match("\\[\\d{2}/\\w{3}/\\d{4}:") { 0.2 } else { 0.0 }
  let status_bonus = if content.re_match("\\s[1-5]\\d{2}\\s") { 0.2 } else { 0.0 }
  let http_bonus = if content.contains("HTTP/") { 0.1 } else { 0.0 }
  let quote_bonus = if content.count('"') >= 2 { 0.1 } else { 0.0 }
  base_score + timestamp_bonus + status_bonus + http_bonus + quote_bonus
} else {
  0.0
}
```

### Issue: Syslog Priority Parsing Errors

**Symptoms:**
- Priority field missing or incorrect
- Facility/severity calculation errors
- Timestamp parsing failures

**Diagnosis:**
```bash
# Check syslog format
grep '^<[0-9]' ~/expanso-logs/system/*.log | head -5

# Test priority calculation
echo '<134>' | sed 's/<\([0-9]*\)>.*/\1/' | awk '{pri=$1; print "Priority:", pri, "Facility:", int(pri/8), "Severity:", pri%8}'

# Check timestamp formats
grep -o '\w{3}\s\+\d{1,2}\s\+\d{2}:\d{2}:\d{2}' ~/expanso-logs/system/*.log | head -3
```

**Solutions:**

**1. Handle priority edge cases**
```yaml
# Robust priority parsing
- mapping: |
    root = this
    let pri_str = this.priority.string()
    let pri = if pri_str.re_match("^\\d+$") {
      pri_str.number()
    } else {
      0  # Default priority if parsing fails
    }
    
    # Validate priority range (0-191 for RFC3164)
    root.priority_validated = if pri >= 0 && pri <= 191 {
      pri
    } else {
      16  # Default to local0.info
    }
    
    root.facility = (root.priority_validated / 8).floor()
    root.severity = root.priority_validated % 8
```

**2. Handle timestamp without year**
```yaml
# Syslog timestamp with year handling
- mapping: |
    root = this
    let current_year = now().ts_format("2006")
    let syslog_ts = this.timestamp
    
    # Try parsing with current year
    root.timestamp_unix = (current_year + " " + syslog_ts).parse_timestamp("2006 Jan 02 15:04:05").catch(
      # Try alternative format
      (current_year + " " + syslog_ts).parse_timestamp("2006 Jan _2 15:04:05").catch(
        # Fallback to current time
        now().ts_unix()
      )
    ).ts_unix()
```

---

## Performance Issues

### Issue: High Memory Usage

**Symptoms:**
- Parser consuming excessive RAM
- Out of memory errors
- Slow processing of large files

**Diagnosis:**
```bash
# Check memory usage
expanso job metrics complete-log-parser --metric memory
ps aux | grep expanso
free -h

# Check file sizes
du -h ~/expanso-logs/**/* | sort -h | tail -10

# Monitor memory over time
watch "expanso job metrics complete-log-parser --metric memory"
```

**Solutions:**

**1. Optimize batch processing**
```yaml
# Reduce batch sizes
output:
  http_client:
    batching:
      count: 500      # Reduced from 1000
      period: 60s     # Increased from 30s
      byte_size: 5MB  # Reduced from 10MB
```

**2. Add memory limits**
```yaml
# Configure memory management
pipeline:
  processors:
    - memory:
        limit: "2GB"
        check_interval: "10s"
        
    # Process logs in smaller chunks
    - archive:
        format: lines
        
    - parallel:
        cap: 4  # Reduced from 8
        processors:
          # Detection and parsing logic
```

**3. Implement log rotation**
```bash
# Set up logrotate
cat > /etc/logrotate.d/expanso-logs << 'EOF'
/var/log/expanso/**/*.log {
    daily
    rotate 7
    compress
    delaycompress
    missingok
    notifempty
    create 644 expanso expanso
    postrotate
        systemctl reload expanso
    endscript
}
EOF
```

### Issue: Low Processing Throughput

**Symptoms:**
- Processing rate below expected
- Backlog growing
- High CPU usage

**Diagnosis:**
```bash
# Check processing rate
expanso job stats complete-log-parser | grep -E "Rate|Processed"

# Monitor CPU usage
top -p $(pgrep expanso)
iostat 5 3

# Check pipeline bottlenecks
expanso job profile complete-log-parser
```

**Solutions:**

**1. Optimize grok patterns**
```yaml
# Use more specific patterns first (faster matching)
- grok:
    expressions:
      # Most specific first
      - '%{IPV4:client_ip} - %{USER:auth} \[%{HTTPDATE:timestamp}\] "%{WORD:method} %{URIPATH:request} HTTP/%{NUMBER:http_version}" %{NUMBER:status_code} %{NUMBER:bytes}'
      # General fallback
      - '%{IPORHOST:client_ip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:method} %{NOTSPACE:request}(?: HTTP/%{NUMBER:http_version})?|%{DATA})" %{NUMBER:status_code} (?:%{NUMBER:bytes}|-)'
```

**2. Implement early filtering**
```yaml
# Filter out uninteresting logs early
processors:
  - mapping: |
      root = if this.string().contains("health-check") || 
                this.string().contains("/favicon.ico") ||
                this.string().length() < 20 {
        deleted()
      } else {
        this
      }
```

**3. Scale horizontally**
```bash
# Deploy multiple parser instances
for i in {1..3}; do
  sed "s/complete-log-parser/complete-log-parser-$i/" complete-log-parser.yaml > parser-$i.yaml
  expanso job deploy parser-$i.yaml
done

# Use load balancer for syslog input
# Configure rsyslog to distribute across instances
```

### Issue: Output Endpoint Bottlenecks

**Symptoms:**
- Backpressure from output destinations
- Timeout errors
- Data loss

**Diagnosis:**
```bash
# Test endpoint connectivity
curl -I "${ANALYTICS_ENDPOINT}/logs"
curl -I "${SIEM_ENDPOINT}/events"

# Check output queue sizes
expanso job metrics complete-log-parser --metric output_queue

# Monitor endpoint response times
time curl -H "Content-Type: application/json" -d '{"test":true}' "${ANALYTICS_ENDPOINT}/logs"
```

**Solutions:**

**1. Add circuit breakers**
```yaml
# Implement circuit breaker pattern
- switch:
    - check: meta("circuit_breaker_analytics") != "open"
      output:
        http_client:
          url: "${ANALYTICS_ENDPOINT}/logs"
          timeout: "10s"
          max_in_flight: 5
          processors:
            # Set circuit breaker on failures
            - catch:
                - mapping: |
                    meta circuit_breaker_analytics = "open"
    - output:
        # Fallback to local storage
        file:
          path: "/var/log/expanso/fallback/analytics.jsonl"
```

**2. Implement retry logic with backoff**
```yaml
# Advanced retry configuration
output:
  http_client:
    retry_until_success: false
    max_retries: 3
    backoff:
      initial_interval: "1s"
      max_interval: "30s"
      multiplier: 2.0
```

**3. Add output buffering**
```yaml
# Buffer outputs to handle spikes
output:
  buffer:
    memory:
      limit: "100MB"
    outputs:
      - http_client:
          url: "${ANALYTICS_ENDPOINT}/logs"
```

---

## Parsing Errors

### Issue: Timestamp Parsing Failures

**Symptoms:**
- Logs with invalid or missing timestamps
- Timestamp fields showing current time instead of log time
- Timezone conversion errors

**Diagnosis:**
```bash
# Check timestamp formats in logs
grep -o '\[[^]]*\]' ~/expanso-logs/nginx/*.log | head -5
grep -o '"timestamp":"[^"]*"' ~/expanso-logs/app/*.jsonl | head -5

# Test timestamp parsing
echo "2025-10-20T14:23:45.123Z" | expanso test timestamp "2006-01-02T15:04:05.999Z07:00"
```

**Solutions:**

**1. Support multiple timestamp formats**
```yaml
# Comprehensive timestamp parsing
- mapping: |
    root = this
    let ts_raw = this.timestamp.or(this."@timestamp").or(this.time)
    
    root.timestamp_unix = ts_raw.parse_timestamp("2006-01-02T15:04:05.999Z07:00").catch(
      ts_raw.parse_timestamp("2006-01-02T15:04:05Z").catch(
        ts_raw.parse_timestamp("2006-01-02 15:04:05").catch(
          ts_raw.parse_timestamp("02/Jan/2006:15:04:05 -0700").catch(
            ts_raw.parse_timestamp("Jan 02 15:04:05").catch(
              ts_raw.parse_timestamp("2006-01-02").catch(
                # Extract Unix timestamp if already numeric
                ts_raw.number().catch(now().ts_unix())
              )
            )
          )
        )
      )
    ).ts_unix()
```

**2. Handle timezone conversion**
```yaml
# Normalize all timestamps to UTC
- mapping: |
    root = this
    # Convert timestamp to UTC if timezone info is present
    root.timestamp_utc = if this.timestamp.contains("+") || this.timestamp.contains("Z") {
      this.timestamp_unix  # Already has timezone info
    } else {
      # Assume local timezone and convert to UTC
      this.timestamp_unix + 0  # Add timezone offset if known
    }
```

**3. Add timestamp validation**
```yaml
# Validate timestamp reasonableness
- mapping: |
    root = this
    let min_valid_ts = 946684800   # 2000-01-01
    let max_valid_ts = 4102444800  # 2100-01-01
    
    root.timestamp_valid = this.timestamp_unix >= min_valid_ts && 
                          this.timestamp_unix <= max_valid_ts
                          
    root.timestamp_unix = if this.timestamp_valid {
      this.timestamp_unix
    } else {
      now().ts_unix()  # Use current time for invalid timestamps
    }
```

### Issue: Encoding and Character Issues

**Symptoms:**
- Special characters not parsing correctly
- Unicode/UTF-8 encoding errors
- Binary data in text logs

**Diagnosis:**
```bash
# Check file encoding
file ~/expanso-logs/nginx/*.log
hexdump -C ~/expanso-logs/app/application.jsonl | head -5

# Check for non-ASCII characters
grep -P '[^\x00-\x7F]' ~/expanso-logs/**/*.log | head -3
```

**Solutions:**

**1. Handle encoding issues**
```yaml
# Clean non-ASCII characters
- mapping: |
    root = this
    let content = this.string()
    
    # Remove or replace problematic characters
    root.cleaned_content = content.re_replace_all("[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]", "").
                                  re_replace_all("[\uFFFD\uFEFF]", "")  # Remove replacement chars
                                  
    # Use cleaned content for processing
    root.original_content = root.cleaned_content
```

**2. Handle Unicode properly**
```yaml
# Ensure UTF-8 handling
- mapping: |
    root = this
    
    # Validate UTF-8 and convert if necessary
    root.content_valid = this.string().is_utf8()
    root.original_content = if this.content_valid {
      this.string()
    } else {
      # Convert from Latin-1 or other encoding
      this.string().decode("latin-1").encode("utf-8")
    }
```

### Issue: Large Log Lines

**Symptoms:**
- Memory errors with very long log lines
- Parsing timeouts
- Truncated log messages

**Diagnosis:**
```bash
# Find long lines
awk 'length > 10000 {print NR, length, $0}' ~/expanso-logs/**/*.log | head -5

# Check line length distribution
awk '{print length}' ~/expanso-logs/**/*.log | sort -n | tail -10
```

**Solutions:**

**1. Implement line length limits**
```yaml
# Add input validation
input:
  file:
    scanner:
      lines:
        max_buffer_size: 1048576  # 1MB max line
        
pipeline:
  processors:
    - mapping: |
        root = if this.string().length() > 100000 {
          {
            "original_truncated": this.string().slice(0, 10000) + "... [truncated]",
            "truncation_info": {
              "original_length": this.string().length(),
              "truncated_at": 10000,
              "reason": "line_too_long"
            }
          }
        } else {
          this
        }
```

**2. Stream processing for large content**
```yaml
# Process large logs in chunks
- split:
    size: 1000  # Process in 1KB chunks
    processors:
      - mapping: |
          # Process each chunk individually
          root = this
```

---

## Production Issues

### Issue: Data Loss

**Symptoms:**
- Missing logs in output destinations
- Gaps in timestamp sequences
- Lower output volume than input volume

**Diagnosis:**
```bash
# Compare input vs output volumes
echo "Input files:"
wc -l ~/expanso-logs/**/*.{jsonl,csv,log}

echo "Output files:"
wc -l ~/expanso-logs/output/*.jsonl

# Check for errors
expanso job logs complete-log-parser --level error | grep -E "failed|error|timeout"

# Verify processing pipeline
expanso job describe complete-log-parser
```

**Solutions:**

**1. Add data integrity checks**
```yaml
# Add sequence numbers for tracking
- mapping: |
    root = this
    root.sequence_id = uuid_v4()
    root.processing_metadata = {
      "received_at": now().ts_unix(),
      "source_file": meta("file_path").or("unknown")
    }
```

**2. Implement exactly-once delivery**
```yaml
# Add idempotency keys
output:
  http_client:
    headers:
      X-Idempotency-Key: "${! metadata.sequence_id }"
    retry_until_success: true
    max_retries: 5
```

**3. Add audit logging**
```yaml
# Log all processing decisions
- mapping: |
    root = this
    # Send audit record
    meta audit_log = {
      "action": "log_processed",
      "sequence_id": this.metadata.sequence_id,
      "format": this.detection_result.format,
      "success": this.parsing_metadata.success,
      "timestamp": now().ts_unix()
    }

# Separate audit output
output:
  broker:
    pattern: fan_out
    outputs:
      # Regular processing
      - # ... existing outputs ...
      
      # Audit trail
      - file:
          path: "/var/log/expanso/audit/processing.log"
          codec: lines
          processors:
            - mapping: 'root = meta("audit_log")'
```

### Issue: Security Alerts Not Triggering

**Symptoms:**
- Security events not reaching SIEM
- Missing critical alerts
- False positive security events

**Diagnosis:**
```bash
# Check security event detection
jq 'select(.security_analysis.risk_level == "high")' ~/expanso-logs/output/high-quality-processed.jsonl

# Test SIEM connectivity
curl -H "Authorization: Bearer $SIEM_TOKEN" "$SIEM_ENDPOINT/events" -d '{"test": "event"}'

# Check alert routing
expanso job logs complete-log-parser | grep -E "security|alert|siem"
```

**Solutions:**

**1. Improve security detection patterns**
```yaml
# Enhanced security indicators
- mapping: |
    root = this
    let content = this.message.or(this.original_content).lowercase()
    
    root.security_analysis = {
      "indicators": {
        # Authentication events
        "auth_failure": content.re_match("failed login|authentication failed|invalid password|access denied"),
        "privilege_escalation": content.re_match("sudo|su |elevation|admin|root"),
        "account_manipulation": content.re_match("user added|user deleted|password changed|account locked"),
        
        # Network security
        "network_intrusion": content.re_match("port scan|brute force|ddos|syn flood"),
        "malicious_ip": content.re_match("blocked|banned|suspicious|malicious"),
        "firewall_events": content.re_match("firewall|iptables|blocked|dropped"),
        
        # System security
        "malware_detection": content.re_match("virus|malware|trojan|backdoor|rootkit"),
        "system_compromise": content.re_match("compromise|breach|unauthorized|exploit"),
        "data_exfiltration": content.re_match("download|export|transfer|copy|exfiltrate")
      }
    }
    
    # Calculate risk score
    let risk_indicators = root.security_analysis.indicators.values().filter(v -> v == true).length()
    root.security_analysis.risk_score = risk_indicators / 9.0
    root.security_analysis.risk_level = if risk_indicators >= 3 {
      "high"
    } else if risk_indicators >= 1 {
      "medium"
    } else {
      "low"
    }
```

**2. Add alert validation**
```yaml
# Validate alerts before sending
- switch:
    - check: this.security_analysis.risk_level in ["high", "medium"]
      processors:
        # Add alert context
        - mapping: |
            root.alert_payload = {
              "alert_id": uuid_v4(),
              "timestamp": this.timestamp_unix,
              "severity": this.security_analysis.risk_level,
              "source": {
                "hostname": this.hostname.or("unknown"),
                "service": this.service.or(this.tag).or("unknown"),
                "format": this.metadata.format
              },
              "event": {
                "message": this.message.or(this.original_content).slice(0, 500),
                "indicators": this.security_analysis.indicators,
                "risk_score": this.security_analysis.risk_score
              },
              "metadata": this.metadata
            }
```

### Issue: Compliance Audit Failures

**Symptoms:**
- Data retention policy violations
- Missing audit trails
- Privacy regulation non-compliance

**Diagnosis:**
```bash
# Check data retention
find /var/log/expanso -name "*.jsonl" -mtime +30 -ls

# Verify PII anonymization
jq 'select(has("client_ip"))' ~/expanso-logs/output/*.jsonl | wc -l
jq '.client_ip_hash' ~/expanso-logs/output/processed-access.jsonl | head -5

# Check audit completeness
grep -c "audit_log" /var/log/expanso/audit/processing.log
```

**Solutions:**

**1. Implement automated data retention**
```bash
# Create retention policy script
cat > /opt/expanso/retention-policy.sh << 'EOF'
#!/bin/bash
# Expanso log retention policy

# Remove logs older than 30 days
find /var/log/expanso -name "*.jsonl" -mtime +30 -delete

# Compress logs older than 7 days
find /var/log/expanso -name "*.jsonl" -mtime +7 ! -name "*.gz" -exec gzip {} \;

# Archive to cold storage logs older than 90 days
find /var/log/expanso -name "*.gz" -mtime +90 -exec mv {} /archive/expanso/ \;

# Log retention actions
echo "$(date): Retention policy executed" >> /var/log/expanso/retention.log
EOF

chmod +x /opt/expanso/retention-policy.sh

# Add to crontab
echo "0 2 * * * /opt/expanso/retention-policy.sh" | crontab -
```

**2. Enhance PII protection**
```yaml
# Comprehensive PII removal
- mapping: |
    root = this
    
    # Hash all IP addresses
    root = this.walk(value -> 
      if value.type() == "string" && value.re_match("\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}") {
        (value + env("IP_SALT")).hash("sha256").slice(0, 16)
      } else {
        value
      }
    )
    
    # Remove email addresses
    root = this.walk(value ->
      if value.type() == "string" && value.re_match("[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}") {
        "email_redacted_" + value.hash("sha256").slice(0, 8)
      } else {
        value
      }
    )
    
    # Add privacy metadata
    root.privacy_compliance = {
      "pii_removed": true,
      "anonymization_method": "sha256_hash",
      "gdpr_compliant": true,
      "retention_class": "standard_30_days"
    }
```

---

## Getting Help

### Enable Debug Logging

```yaml
# Add debug information to pipeline
- mapping: |
    root = this
    root.debug_info = {
      "input_size": this.string().length(),
      "detection_time": now().ts_unix() - this.processing_start,
      "memory_usage": "check_with_metrics",
      "processing_path": this.parsing_metadata.processor
    }
```

### Export Diagnostics

```bash
# Create comprehensive diagnostics
cat > /tmp/expanso-diagnostics.sh << 'EOF'
#!/bin/bash
echo "=== Expanso Log Parser Diagnostics ==="
echo "Timestamp: $(date)"
echo

echo "=== Job Status ==="
expanso job status complete-log-parser
echo

echo "=== Recent Errors ==="
expanso job logs complete-log-parser --level error --tail 20
echo

echo "=== Performance Metrics ==="
expanso job stats complete-log-parser
echo

echo "=== Configuration ==="
expanso job describe complete-log-parser | head -50
echo

echo "=== System Resources ==="
free -h
df -h /var/log/expanso
echo

echo "=== Network Connectivity ==="
curl -I "${ANALYTICS_ENDPOINT}/health" 2>/dev/null || echo "Analytics endpoint unreachable"
curl -I "${SIEM_ENDPOINT}/health" 2>/dev/null || echo "SIEM endpoint unreachable"
echo

echo "=== Log File Status ==="
ls -la /var/log/expanso/**/*.{log,jsonl} 2>/dev/null | tail -10
EOF

chmod +x /tmp/expanso-diagnostics.sh
/tmp/expanso-diagnostics.sh > /tmp/expanso-diagnostics.txt
```

### Support Checklist

When requesting support, provide:

✅ **Configuration files** - Complete pipeline YAML  
✅ **Error logs** - Recent error messages and stack traces  
✅ **Sample data** - Anonymized examples of problematic logs  
✅ **Environment details** - OS, Expanso version, resource limits  
✅ **Performance metrics** - Throughput, latency, resource usage  
✅ **Expected vs actual** - What you expected vs what happened  

### Community Resources

- **Documentation:** [Expanso Docs](https://docs.expanso.io)
- **Community Forum:** [Expanso Community](https://community.expanso.io)
- **GitHub Issues:** [Expanso GitHub](https://github.com/expanso-io/expanso)
- **Slack Channel:** [#expanso-support](https://slack.expanso.io)

---

## Summary

This troubleshooting guide covers the most common issues you'll encounter when deploying log parsers:

✅ **Format detection** problems and solutions  
✅ **Performance optimization** for high-volume processing  
✅ **Error handling** and recovery strategies  
✅ **Production monitoring** and maintenance  
✅ **Security and compliance** troubleshooting  
✅ **Support resources** and diagnostic tools

Most issues can be resolved by following the systematic diagnosis and solution steps provided. For complex problems, use the diagnostic tools to gather information before reaching out for support.

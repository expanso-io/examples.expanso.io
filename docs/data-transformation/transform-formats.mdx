---
title: Transform Formats
sidebar_label: Transform Formats
sidebar_position: 5
description: Transform data between JSON, Avro, Parquet, and Protobuf formats
keywords: [format-conversion, avro, parquet, protobuf, json, serialization]
---

import CodeBlock from '@theme/CodeBlock';
import pipelineYaml from '!!raw-loader!../../examples/data-transformation/transform-formats.yaml';


# Transform Formats

Transform data between different serialization formats (JSON, Avro, Parquet, Protobuf) to optimize for bandwidth, storage, processing speed, or compatibility.

## Complete Examples

For full working configurations with all variations, see:
- [Transform Formats Examples](https://github.com/expanso-io/docs.expanso.io/tree/main/examples/data-transformation/transform-formats)

Includes: JSON to Avro, Avro to Parquet, JSON to Protobuf, and multi-format auto-detection.

## When to Use

**Use format transformation when:**
- Converting human-readable JSON to efficient binary formats
- Streaming data needs Avro for Kafka
- Analytics/storage requires columnar Parquet
- Microservices use Protobuf for gRPC
- Reducing bandwidth by 60-90%

**Example:** JSON sensor data (1KB/event) â†’ Avro (400B/event) â†’ 60% bandwidth reduction

## Format Overview

| Format | Binary | Schema | Compression | Best For |
|--------|--------|--------|-------------|----------|
| JSON | No | No | Text | APIs, logs, debugging |
| Avro | Yes | Required | Binary | Kafka, streaming |
| Parquet | Yes | Required | Columnar | Analytics, cloud storage |
| Protobuf | Yes | Required | Binary | gRPC, microservices |

### When to Use Each Format

**JSON:**
- Human-readable debugging
- REST APIs
- Log aggregation
- No schema requirements

**Avro:**
- Kafka streaming
- Schema evolution needed
- Row-oriented processing
- Data lake ingestion

**Parquet:**
- Cloud storage (S3, GCS)
- Analytics queries (BigQuery, Athena)
- Columnar aggregations
- Long-term archival

**Protobuf:**
- gRPC services
- Low-latency microservices
- Type-safe contracts
- Cross-language compatibility

## Basic Transformations

### JSON to Avro

Convert JSON to compact binary Avro for Kafka streaming.

<CodeBlock language="yaml" title="transform-formats.yaml" showLineNumbers>
  {pipelineYaml}
</CodeBlock>

<a
  href="/files/data-transformation/transform-formats.yaml"
  download
  className="button button--primary button--lg margin-top--md"
>
  ðŸ“¥ Download Pipeline
</a>

---


**Input (JSON):**
```json
{
  "sensor_id": "temp_42",
  "location": "warehouse_north",
  "temperature": 72.5,
  "humidity": 45.2,
  "timestamp": "2025-10-20T14:23:45.123Z",
  "metadata": {
    "device_type": "DHT22",
    "firmware_version": "1.2.3"
  }
}
```

**Output:** Binary Avro (60% smaller)

### Avro to Parquet

Convert streaming Avro to columnar Parquet for cloud storage and analytics.

```yaml
config:
  input:
    kafka:
      addresses: ["${KAFKA_BROKER}"]
      topics: ["sensor-readings-avro"]
      consumer_group: parquet-converter

  pipeline:
    processors:
      # Decode Avro
      - avro:
          operator: from_json
          encoding: binary

      # Convert to Parquet
      - parquet:
          default_compression: snappy
          default_encoding: PLAIN

  output:
    # Batch to S3 every 5 minutes
    aws_s3:
      bucket: "${S3_BUCKET}"
      path: "sensors/date=${!timestamp().ts_format(\"2006-01-02\")}/hour=${!timestamp().ts_format(\"15\")}/data.parquet"
      batching:
        count: 10000
        period: 5m
      compression: snappy
```

**Data reduction:**
```
JSON:    1000 events Ã— 1KB = 1000KB
Avro:    1000 events Ã— 400B = 400KB  (60% reduction)
Parquet: 1000 events Ã— 100B = 100KB  (90% reduction)
```

### JSON to Protobuf

Convert JSON to compact Protobuf for gRPC microservices.

```yaml
config:
  input:
    http_server:
      address: "0.0.0.0:8080"
      path: /events

  pipeline:
    processors:
      # Parse JSON
      - json_documents:
          parts: []

      # Convert to Protobuf
      - protobuf:
          operator: to_json
          message: UserAction
          import_paths: ["/schemas"]

  output:
    grpc:
      url: "${GRPC_ENDPOINT}"
      verb: POST
```

**Protobuf Schema (.proto):**
```protobuf
syntax = "proto3";

message UserAction {
  string user_id = 1;
  string action = 2;
  double amount = 3;
  string timestamp = 4;
}
```

**Output:** Binary Protobuf (70% smaller than JSON)

## Schema Management

### Avro Schemas

**Option 1: Inline schema**
```yaml
- avro:
    operator: to_json
    schema: |
      {
        "type": "record",
        "name": "SensorReading",
        "fields": [...]
      }
```

**Option 2: Schema Registry**
```yaml
- avro:
    operator: to_json
    schema_registry:
      url: "${SCHEMA_REGISTRY}"
      subject: "sensor-readings-value"
```

**Option 3: File-based schema**
```yaml
- avro:
    operator: to_json
    schema_file: "/schemas/sensor_reading.avsc"
```

### Protobuf Schemas

**File-based (.proto files):**
```yaml
- protobuf:
    operator: to_json
    message: "SensorReading"
    import_paths: ["/schemas"]
```

### Parquet Schemas

**Derived from Avro schema:**
```yaml
- parquet:
    schema_file: "/schemas/sensor_reading.parquet.schema"
    default_compression: snappy
```

## Multi-Format Auto-Detection

Auto-detect input format and convert appropriately:

```yaml
config:
  input:
    http_server:
      address: "0.0.0.0:8080"
      path: /convert

  pipeline:
    processors:
      # Detect format from Content-Type header
      - switch:
          # JSON input
          - check: meta("Content-Type").contains("json")
            processors:
              - json_documents: {}
              - mapping: |
                  root.format = "json"

          # Avro input
          - check: meta("Content-Type").contains("avro")
            processors:
              - avro:
                  operator: from_json
              - mapping: |
                  root.format = "avro"

          # Protobuf input
          - check: meta("Content-Type").contains("protobuf")
            processors:
              - protobuf:
                  operator: from_json
              - mapping: |
                  root.format = "protobuf"

      # Route to appropriate converter
      - switch:
          # Convert to Parquet for analytics
          - check: meta("destination") == "analytics"
            processors:
              - parquet:
                  default_compression: snappy

          # Convert to Avro for streaming
          - check: meta("destination") == "streaming"
            processors:
              - avro:
                  operator: to_json
```

## Performance Comparison

### Serialization Size (1000 events)

| Format | Size | Compression | Final Size | Reduction |
|--------|------|-------------|------------|-----------|
| JSON | 1000 KB | None | 1000 KB | 0% |
| JSON (gzip) | 1000 KB | gzip | 200 KB | 80% |
| Avro | 400 KB | Snappy | 350 KB | 65% |
| Parquet | 100 KB | Snappy | 80 KB | 92% |
| Protobuf | 300 KB | None | 300 KB | 70% |

### Serialization Speed (events/sec)

| Format | Serialize | Deserialize |
|--------|-----------|-------------|
| JSON | 50,000 | 40,000 |
| Avro | 100,000 | 120,000 |
| Parquet | 80,000 | 150,000 |
| Protobuf | 150,000 | 180,000 |

## Production Considerations

### Schema Evolution

**Avro supports forward/backward compatibility:**

```json
// Version 1
{
  "type": "record",
  "name": "SensorReading",
  "fields": [
    {"name": "sensor_id", "type": "string"},
    {"name": "temperature", "type": "double"}
  ]
}

// Version 2 (backward compatible)
{
  "type": "record",
  "name": "SensorReading",
  "fields": [
    {"name": "sensor_id", "type": "string"},
    {"name": "temperature", "type": "double"},
    {"name": "humidity", "type": ["null", "double"], "default": null}
  ]
}
```

New field has default value â†’ old readers can still read new data.

### Memory Management

**Problem:** Parquet loads entire row group into memory.

**Solution: Reduce batch size**
```yaml
output:
  aws_s3:
    batching:
      count: 1000  # Smaller batches
      period: 1m
```

### Schema Registry

**Use schema registry for version management:**

```yaml
- avro:
    schema_registry:
      url: "http://schema-registry:8081"
      subject: "sensor-readings-value"
      # Auto-register new schemas
      auto_register_schemas: true
```

**Benefits:**
- Centralized schema storage
- Version tracking
- Compatibility checks
- Schema reuse across services

## Troubleshooting

### Schema Mismatch Errors

**Error:** `avro: field "temperature" not found in schema`

**Cause:** JSON field doesn't match Avro schema

**Fix: Update mapping to match schema**
```bloblang
# Ensure all schema fields are present
root.sensor_id = this.sensor_id
root.temperature = this.temperature.or(0.0)  # Default if missing
root.humidity = this.humidity.or(0.0)
```

### Nested Fields Not Flattening

**Problem:** Avro schema is flat, but JSON has nested fields.

**Solution: Flatten in mapping**
```bloblang
# Input: {"metadata": {"device_type": "DHT22"}}
# Schema expects: {"device_type": "string"}

root.sensor_id = this.sensor_id
root.device_type = this.metadata.device_type
root.firmware_version = this.metadata.firmware_version
```

### Schema Registry Connection Failures

**Error:** `schema registry: connection refused`

**Check schema registry:**
```bash
curl http://schema-registry:8081/subjects
```

**Fix: Update URL and add timeout**
```yaml
- avro:
    schema_registry:
      url: "http://schema-registry:8081"
      timeout: 10s
      max_retries: 3
```

### Large Parquet Files

**Problem:** Parquet files too large for queries.

**Solution: Partition by time**
```yaml
output:
  aws_s3:
    # Partition by date and hour
    path: "sensors/date=${!timestamp().ts_format(\"2006-01-02\")}/hour=${!timestamp().ts_format(\"15\")}/data.parquet"
    batching:
      count: 10000
      period: 5m
```

**Result:** Smaller files, faster queries (query only relevant partitions).

## Bandwidth Savings

### Before (JSON only):
```
1000 events/sec Ã— 1KB = 1MB/sec
Daily: 86GB, Monthly: 2.6TB
Cost at $0.09/GB: $234/month
```

### After (Avro for streaming):
```
1000 events/sec Ã— 400B = 400KB/sec
Daily: 34GB, Monthly: 1TB
Cost at $0.09/GB: $90/month
Savings: $144/month (62% reduction)
```

### After (Parquet for storage):
```
1000 events/sec Ã— 100B = 100KB/sec
Daily: 8.6GB, Monthly: 260GB
Cost at $0.09/GB: $23/month
Savings: $211/month (90% reduction)
```

## Format Selection Guide

| Requirement | Recommended Format |
|-------------|-------------------|
| Human-readable | JSON |
| Kafka streaming | Avro |
| Cloud analytics | Parquet |
| gRPC services | Protobuf |
| Schema evolution | Avro |
| Columnar queries | Parquet |
| Fastest serialization | Protobuf |
| Smallest size | Parquet |
| No schema needed | JSON |

## See Also

- [Avro Processor](https://docs.expanso.io/components/processors/avro) - Avro configuration
- [Protobuf Processor](https://docs.expanso.io/components/processors/protobuf) - Protobuf configuration
- [Bloblang Functions](https://docs.expanso.io/guides/bloblang) - Transformation functions

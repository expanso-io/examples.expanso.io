---
title: Parse Structured Logs
description: Extract structured data from JSON, CSV, syslog, and application logs
sidebar_position: 1
---

import CodeBlock from '@theme/CodeBlock';
import pipelineYaml from '!!raw-loader!../../examples/data-transformation/parse-logs.yaml';


# Parse Structured Logs

Parse common log formats into structured data for routing, transformation, and analysis. This guide covers JSON, CSV, syslog, Apache/Nginx access logs, and custom formats using declarative processors.

## Log Formats Covered

1. **JSON Logs** - Use `json_documents` processor
2. **CSV Logs** - Use `csv` processor
3. **Apache/Nginx Logs** - Use `grok` processor with patterns
4. **Syslog Messages** - Use `syslog` processor
5. **Custom Formats** - Use `regex` patterns

## Format 1: JSON Logs

Parse JSON application logs, normalize timestamps, filter by level, and remove unnecessary fields.

<CodeBlock language="yaml" title="json-log-parser.yaml" showLineNumbers>
  {pipelineYaml}
</CodeBlock>

<a
  href="/files/data-transformation/parse-logs.yaml"
  download
  className="button button--primary button--lg margin-top--md"
>
  ðŸ“¥ Download Pipeline
</a>

---


**Input:**
```json
{"timestamp":"2025-10-20T14:23:45.123Z","level":"error","service":"api","message":"Database connection failed","error":"connection timeout","duration_ms":5000}
```

**Output:**
```json
{
  "timestamp_unix": 1729433025,
  "timestamp_iso": "2025-10-20T14:23:45.123Z",
  "level": "ERROR",
  "service": "api",
  "message": "Database connection failed",
  "duration_ms": 5000,
  "error_details": {
    "message": "connection timeout",
    "timestamp": 1729433025
  },
  "metadata": {
    "parsed_by": "json-parser",
    "parsed_at": 1729433025,
    "source_node": "edge-node-01"
  }
}
```

## Format 2: CSV Logs

Parse CSV sensor data with column mapping, type conversion, and validation.

```yaml title="csv-log-parser.yaml"
name: csv-log-parser
description: Parse CSV sensor logs
type: pipeline
namespace: default

config:
  input:
    file:
      paths:
        - /var/log/sensors/*.csv
      codec: lines

  pipeline:
    processors:
      # Parse CSV with named columns
      - csv:
          columns:
            - timestamp
            - metric_name
            - sensor_id
            - value
            - unit
          skip_header_rows: 1
          delimiter: ","

      # Parse timestamp
      - mapping: |
          root = this
          root.timestamp_unix = this.timestamp.parse_timestamp("2006-01-02T15:04:05.999Z07:00").ts_unix()

      # Convert value to number
      - mapping: |
          root = this
          root.value_numeric = this.value.number()

      # Add sensor metadata
      - mapping: |
          root = this
          root.sensor_metadata = if this.sensor_id.has_prefix("temp-") {
            {"type": "temperature", "location": "warehouse"}
          } else if this.sensor_id.has_prefix("humid-") {
            {"type": "humidity", "location": "storage"}
          } else {
            {"type": "unknown", "location": "unknown"}
          }

      # Validate ranges and create alerts
      - mapping: |
          root = this
          root.alert = if this.metric_name == "temperature" && this.value_numeric > 30 {
            {"level": "critical", "message": "Temperature exceeds threshold"}
          } else if this.metric_name == "humidity" && this.value_numeric > 80 {
            {"level": "warning", "message": "Humidity approaching threshold"}
          }

  output:
    broker:
      pattern: fan_out
      outputs:
        # All data to time-series database
        - http_client:
            url: "${TIMESERIES_ENDPOINT}/write"
            verb: POST
            batching:
              count: 500
              period: 30s

        # Critical alerts only
        - switch:
            - check: this.alert.level == "critical"
              output:
                http_client:
                  url: "${ALERT_ENDPOINT}/alerts"
                  verb: POST
            - output:
                drop: {}
```

**Input (CSV):**
```csv
timestamp,metric_name,sensor_id,value,unit
2025-10-20T14:23:45.123Z,temperature,temp-sensor-01,35.5,celsius
```

**Output:**
```json
{
  "timestamp": "2025-10-20T14:23:45.123Z",
  "timestamp_unix": 1729433025,
  "metric_name": "temperature",
  "sensor_id": "temp-sensor-01",
  "value": "35.5",
  "value_numeric": 35.5,
  "unit": "celsius",
  "sensor_metadata": {
    "type": "temperature",
    "location": "warehouse"
  },
  "alert": {
    "level": "critical",
    "message": "Temperature exceeds threshold"
  }
}
```

## Format 3: Apache/Nginx Access Logs

Parse web server access logs using grok patterns.

```yaml title="access-log-parser.yaml"
name: access-log-parser
description: Parse Apache/Nginx access logs
type: pipeline
namespace: default

config:
  input:
    file:
      paths:
        - /var/log/nginx/access.log
      codec: lines

  pipeline:
    processors:
      # Parse using grok pattern (Apache Combined Format)
      - grok:
          expressions:
            - '%{IPORHOST:client_ip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:method} %{NOTSPACE:request}(?: HTTP/%{NUMBER:http_version})?|%{DATA})" %{NUMBER:status_code} (?:%{NUMBER:bytes}|-) "(?:%{DATA:referer}|-)" "(?:%{DATA:user_agent}|-)"'
          named_captures_only: true

      # Parse timestamp
      - mapping: |
          root = this
          root.timestamp_unix = this.timestamp.parse_timestamp("02/Jan/2006:15:04:05 -0700").ts_unix()

      # Convert numeric fields
      - mapping: |
          root = this
          root.status_code = this.status_code.number()
          root.bytes = this.bytes.or("0").number()

      # Classify request
      - mapping: |
          root = this
          root.request_category = if this.request.contains("/api/") {
            "api"
          } else if this.request.contains("/static/") {
            "static"
          } else if this.request.contains("/admin/") {
            "admin"
          } else {
            "page"
          }
          root.is_error = this.status_code >= 400
          root.is_server_error = this.status_code >= 500

      # Parse user agent
      - mapping: |
          root = this
          root.browser = if this.user_agent.contains("Chrome") {
            "Chrome"
          } else if this.user_agent.contains("Firefox") {
            "Firefox"
          } else if this.user_agent.contains("Safari") {
            "Safari"
          } else {
            "Other"
          }
          root.is_mobile = this.user_agent.lowercase().contains("mobile")

      # Hash IP for privacy
      - mapping: |
          root = this
          root.client_ip_hash = (this.client_ip + env("IP_SALT").or("default_salt")).hash("sha256").slice(0, 16)
          root = this.without("client_ip")

      # Filter static resources
      - mapping: |
          root = if this.request.re_match(".*\\.(css|js|png|jpg|gif|ico)$") {
            deleted()
          } else {
            this
          }

  output:
    broker:
      pattern: fan_out
      outputs:
        # All access logs
        - http_client:
            url: "${ANALYTICS_ENDPOINT}/logs/access"
            verb: POST
            batching:
              count: 1000
              period: 30s

        # Server errors to alerts
        - switch:
            - check: this.is_server_error == true
              output:
                http_client:
                  url: "${SLACK_WEBHOOK}"
                  verb: POST
                  processors:
                    - mapping: |
                        root.text = "Server Error: %s %s returned %d".format(
                          this.method,
                          this.request,
                          this.status_code
                        )
            - output:
                drop: {}
```

**Input (Nginx Combined Format):**
```
203.0.113.45 - user123 [20/Oct/2025:14:23:45 +0000] "GET /api/users HTTP/1.1" 200 1234 "https://example.com" "Mozilla/5.0 Chrome/91.0"
```

**Output:**
```json
{
  "client_ip_hash": "7b9d4e2f1c8a6e3b",
  "auth": "user123",
  "timestamp_unix": 1729433025,
  "method": "GET",
  "request": "/api/users",
  "http_version": "1.1",
  "status_code": 200,
  "bytes": 1234,
  "referer": "https://example.com",
  "request_category": "api",
  "is_error": false,
  "is_server_error": false,
  "browser": "Chrome",
  "is_mobile": false
}
```

## Format 4: Syslog Messages

Parse RFC3164 syslog messages with priority, facility, and severity extraction.

```yaml title="syslog-parser.yaml"
name: syslog-parser
description: Parse RFC3164 syslog messages
type: pipeline
namespace: default

config:
  input:
    socket_server:
      network: udp
      address: "0.0.0.0:514"

  pipeline:
    processors:
      # Parse syslog format (RFC3164)
      - grok:
          expressions:
            - '<%{POSINT:priority}>%{SYSLOGTIMESTAMP:timestamp} %{SYSLOGHOST:hostname} %{DATA:tag}(?:\[%{POSINT:pid}\])?: %{GREEDYDATA:message}'
          named_captures_only: true

      # Parse priority to facility and severity
      - mapping: |
          root = this
          let pri = this.priority.number()
          root.facility = (pri / 8).floor()
          root.severity = pri % 8
          root.severity_name = if this.severity == 0 {
            "Emergency"
          } else if this.severity == 1 {
            "Alert"
          } else if this.severity == 2 {
            "Critical"
          } else if this.severity == 3 {
            "Error"
          } else if this.severity == 4 {
            "Warning"
          } else {
            "Informational"
          }

      # Parse timestamp (add current year)
      - mapping: |
          root = this
          let year = now().ts_format("2006")
          let timestamp_with_year = year + " " + this.timestamp
          root.timestamp_unix = timestamp_with_year.parse_timestamp("2006 Jan 02 15:04:05").ts_unix()

      # Extract application and PID
      - mapping: |
          root = this
          root.application = this.tag
          root.process_id = this.pid.or("").number()

      # Filter by severity (Warning and above)
      - mapping: |
          root = if this.severity <= 4 {
            this
          } else {
            deleted()
          }

  output:
    switch:
      # Critical messages to PagerDuty
      - check: this.severity <= 2
        output:
          http_client:
            url: "${PAGERDUTY_ENDPOINT}"
            verb: POST
            processors:
              - mapping: |
                  root.routing_key = env("PAGERDUTY_KEY")
                  root.event_action = "trigger"
                  root.payload = {
                    "summary": this.message,
                    "severity": this.severity_name.lowercase(),
                    "source": this.hostname
                  }

      # Error/Warning to analytics
      - output:
          http_client:
            url: "${ANALYTICS_ENDPOINT}/logs/syslog"
            verb: POST
            batching:
              count: 100
              period: 10s
```

**Input (RFC3164):**
```
<134>Oct 20 14:23:45 edge-node-01 app[12345]: Database connection established
```

**Output:**
```json
{
  "priority": 134,
  "facility": 16,
  "severity": 6,
  "severity_name": "Informational",
  "timestamp_unix": 1729433025,
  "hostname": "edge-node-01",
  "tag": "app",
  "pid": "12345",
  "process_id": 12345,
  "application": "app",
  "message": "Database connection established"
}
```

## Format 5: Custom Application Logs

Parse custom log formats using regex patterns.

```yaml title="custom-log-parser.yaml"
name: custom-log-parser
description: Parse custom application logs
type: pipeline
namespace: default

config:
  input:
    file:
      paths:
        - /var/log/myapp/*.log
      codec: lines

  pipeline:
    processors:
      # Parse custom format: [TIMESTAMP] [LEVEL] [RequestID:ID] MESSAGE
      - grok:
          expressions:
            - '\[%{TIMESTAMP_ISO8601:timestamp}\] \[%{LOGLEVEL:level}\] \[RequestID:%{DATA:request_id}\] %{GREEDYDATA:message}'
          named_captures_only: true

      # Parse timestamp
      - mapping: |
          root = this
          root.timestamp_unix = this.timestamp.parse_timestamp("2006-01-02 15:04:05").ts_unix()

      # Normalize level
      - mapping: |
          root = this
          root.level = this.level.uppercase()

      # Classify error type
      - mapping: |
          root = this
          root.error_type = if this.message.contains("timeout") {
            "timeout"
          } else if this.message.contains("connection") {
            "connection_error"
          } else if this.message.contains("invalid") {
            "validation_error"
          } else {
            "unknown"
          }

  output:
    http_client:
      url: "${LOG_ENDPOINT}/logs"
      verb: POST
      batching:
        count: 100
        period: 10s
```

**Input:**
```
[2025-10-20 14:23:45] [ERROR] [RequestID:abc123] Failed to process request: invalid token
```

**Output:**
```json
{
  "timestamp": "2025-10-20 14:23:45",
  "timestamp_unix": 1729433025,
  "level": "ERROR",
  "request_id": "abc123",
  "message": "Failed to process request: invalid token",
  "error_type": "validation_error"
}
```

## Multi-Format Parser

Detect and parse multiple log formats in a single pipeline.

```yaml title="multi-format-parser.yaml"
name: multi-format-parser
description: Automatically detect and parse multiple log formats
type: pipeline
namespace: default

config:
  input:
    broker:
      pattern: fan_in
      inputs:
        - file:
            paths: ["/var/log/app/*.jsonl"]
            codec: lines
        - file:
            paths: ["/var/log/sensors/*.csv"]
            codec: lines
        - file:
            paths: ["/var/log/nginx/access.log"]
            codec: lines
        - socket_server:
            network: udp
            address: "0.0.0.0:514"

  pipeline:
    processors:
      # Detect format
      - mapping: |
          root = this
          root.detected_format = if this.string().has_prefix("{") {
            "json"
          } else if this.string().has_prefix("<") {
            "syslog"
          } else if this.string().contains(" - - [") {
            "access_log"
          } else if this.string().contains(",") {
            "csv"
          } else {
            "custom"
          }
          meta log_format = root.detected_format

      # Route to format-specific parsers
      - switch:
          # JSON format
          - check: meta("log_format") == "json"
            processors:
              - json_documents:
                  parts: []
              - mapping: |
                  root = this
                  root.timestamp_unix = this.timestamp.parse_timestamp("2006-01-02T15:04:05.999Z07:00").ts_unix()

          # CSV format
          - check: meta("log_format") == "csv"
            processors:
              - csv:
                  columns: [timestamp, metric_name, sensor_id, value, unit]
              - mapping: |
                  root = this
                  root.value_numeric = this.value.number()

          # Access log format
          - check: meta("log_format") == "access_log"
            processors:
              - grok:
                  expressions:
                    - '%{IPORHOST:client_ip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:method} %{NOTSPACE:request}(?: HTTP/%{NUMBER:http_version})?|%{DATA})" %{NUMBER:status_code} (?:%{NUMBER:bytes}|-)'
              - mapping: |
                  root = this
                  root.status_code = this.status_code.number()

          # Syslog format
          - check: meta("log_format") == "syslog"
            processors:
              - grok:
                  expressions:
                    - '<%{POSINT:priority}>%{SYSLOGTIMESTAMP:timestamp} %{SYSLOGHOST:hostname} %{DATA:tag}: %{GREEDYDATA:message}'

          # Unknown format
          - processors:
              - mapping: |
                  root = {"raw_message": this.string(), "format": "unparsed"}

      # Add common metadata
      - mapping: |
          root = this
          root.metadata = {
            "parsed_at": now().ts_unix(),
            "source_node": env("NODE_ID").or("unknown"),
            "detected_format": meta("log_format")
          }

  output:
    switch:
      # Route by format
      - check: meta("log_format") == "csv"
        output:
          http_client:
            url: "${TIMESERIES_ENDPOINT}/write"
            verb: POST

      - check: meta("log_format") == "access_log"
        output:
          http_client:
            url: "${ANALYTICS_ENDPOINT}/logs/access"
            verb: POST

      - check: meta("log_format") == "json"
        output:
          http_client:
            url: "${LOG_ENDPOINT}/logs"
            verb: POST

      # Unparsed logs to DLQ
      - output:
          file:
            path: /var/log/expanso/unparsed.jsonl
            codec: lines
```

## Production Considerations

### Performance

Processor throughput:

| Processor | Throughput | CPU Usage |
|-----------|------------|-----------|
| json_documents | 100k logs/sec | Low |
| csv | 80k logs/sec | Low |
| grok (simple) | 20k logs/sec | Medium |
| grok (complex) | 5k logs/sec | High |

Use `json_documents` and `csv` processors when possible. Reserve grok for complex formats.

### Error Handling

Wrap risky operations in try/catch:

```yaml
processors:
  - mapping: |
      root = this
      # Try to parse, fallback on failure
      root.timestamp_unix = this.timestamp.parse_timestamp("2006-01-02T15:04:05.999Z07:00").catch(now().ts_unix())
      root.structured = this.message.parse_json().catch({"raw": this.message})

  # Send failures to DLQ
  - catch:
      - mapping: |
          root = this
          root.error = error()
          root.failed_at = now().ts_unix()
      - output:
          file:
            path: /var/log/expanso/parse-failures.jsonl
            codec: lines
```

### Timestamp Parsing

Try multiple formats:

```yaml
processors:
  - mapping: |
      root = this
      # Try formats in order
      root.timestamp_unix = this.timestamp.parse_timestamp("2006-01-02T15:04:05.999Z07:00").catch(
        this.timestamp.parse_timestamp("02/Jan/2006:15:04:05 -0700").catch(
          this.timestamp.parse_timestamp("Jan 02 15:04:05").catch(
            now().ts_unix()
          )
        )
      )
```

Common timestamp formats:

| Format | Example | Go Layout |
|--------|---------|-----------|
| ISO8601 | 2025-10-20T14:23:45.123Z | 2006-01-02T15:04:05.999Z07:00 |
| Apache | 20/Oct/2025:14:23:45 +0000 | 02/Jan/2006:15:04:05 -0700 |
| Syslog | Oct 20 14:23:45 | Jan 02 15:04:05 |

### Bandwidth Optimization

Filter, sample, and compress:

```yaml
processors:
  # Filter by level
  - mapping: |
      root = if this.level.or("INFO") in ["DEBUG", "TRACE"] {
        deleted()
      } else {
        this
      }

  # Sample INFO logs (keep 10%)
  - mapping: |
      root = if this.level == "INFO" && random_int() % 10 != 0 {
        deleted()
      } else {
        this
      }

  # Remove unnecessary fields
  - mapping: |
      root = this.without("debug_info", "stack_trace", "raw_request")

output:
  http_client:
    url: "${LOG_ENDPOINT}"
    verb: POST
    compression: gzip
    batching:
      count: 1000
      period: 30s
```

### Monitoring

Key metrics to track:

```promql
# Parse success rate
rate(pipeline_processor_success_total[5m]) /
  rate(pipeline_processor_total[5m])

# Errors by format
sum by (format) (rate(pipeline_processor_error_total[5m]))

# Unparsed logs (DLQ)
rate(pipeline_output_sent_total{output="dlq"}[5m])
```

Alert on:
- Parse success rate < 95%
- Unparsed logs > 100/minute
- CPU usage > 80%

## Troubleshooting

### Grok Patterns Not Matching

Test patterns using online debugger or test mode:

```bash
# Test pattern with sample data
expanso job test log-parser.yaml --input sample.log
```

### High CPU Usage

Optimize grok patterns or scale horizontally:

```bash
# Check CPU usage
expanso job stats log-parser

# Scale replicas
expanso job scale log-parser --replicas 3
```

### Out of Memory

Increase buffer size or truncate large logs:

```yaml
input:
  file:
    scanner:
      lines:
        max_buffer_size: 10485760  # 10MB

processors:
  - mapping: |
      root = if this.string().length() > 1000000 {
        {"truncated": true, "preview": this.string().slice(0, 1000)}
      } else {
        this
      }
```

### Unparsed Logs in DLQ

Investigate common patterns:

```bash
# View unparsed logs
cat /var/log/expanso/unparsed.jsonl | head -10

# Find common patterns
cat /var/log/expanso/unparsed.jsonl | \
  jq -r '.raw_message' | \
  head -100 | sort | uniq -c | sort -nr
```

## Complete Examples

For full working configurations with all variations, see:
- [Parse Logs Examples](https://github.com/expanso-io/docs.expanso.io/tree/main/examples/data-transformation/parse-logs)

Includes: JSON parser, CSV parser, access log parser, syslog parser, and multi-format parser.

## See Also

- [Remove PII](/examples/data-security/remove-pii) - Hash and anonymize log data
- [Fan-Out Pattern](/examples/data-routing/fan-out-pattern) - Route logs to multiple destinations
- [Bloblang Guide](https://docs.expanso.io/guides/bloblang) - Transformation language reference

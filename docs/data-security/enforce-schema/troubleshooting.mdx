---
title: Schema Validation Troubleshooting
sidebar_label: Troubleshooting
sidebar_position: 8
description: Comprehensive troubleshooting guide for schema validation issues, performance problems, and operational challenges
keywords: [troubleshooting, debugging, schema validation, performance, operations]
---

# Schema Validation Troubleshooting

Comprehensive troubleshooting guide for resolving common and complex issues with schema validation pipelines. This guide covers performance problems, validation errors, operational challenges, and system failures.

## Quick Diagnostic Commands

```bash
# Pipeline health check
expanso job status complete-schema-validation

# Recent pipeline logs  
expanso job logs complete-schema-validation --tail 100

# Validation metrics
curl -s http://localhost:8080/metrics | grep -E "validation|quality|error"

# DLQ status
find /var/log/expanso/dlq -name "*.jsonl" -mtime -1 | wc -l

# System resources
free -h && df -h /var/log/expanso
```

---

## Validation Issues

### Issue: All Messages Failing Validation

**Symptoms:**
- 100% of messages routed to DLQ
- Zero successful validations
- Analytics receiving no data

**Possible Causes:**
1. Schema file missing or corrupted
2. Incorrect schema file path
3. Schema syntax errors
4. Permission issues

**Diagnosis:**
```bash
# Check schema file exists
ls -l /etc/expanso/schemas/sensor-schema-v1.0.0.json

# Validate schema syntax
jq empty /etc/expanso/schemas/sensor-schema-v1.0.0.json

# Test schema against known-good message
echo '{
  "sensor_id": "sensor-42",
  "timestamp": "2025-10-20T14:30:00Z",
  "location": {"building": "warehouse-3", "floor": 2, "zone": "A"},
  "readings": {"temperature_celsius": 23.5, "humidity_percent": 45.2},
  "metadata": {"firmware_version": "2.1.0", "battery_percent": 87}
}' | jq --schema-file /etc/expanso/schemas/sensor-schema-v1.0.0.json

# Check file permissions
stat /etc/expanso/schemas/sensor-schema-v1.0.0.json

# Check pipeline configuration
expanso job describe complete-schema-validation | grep schema_path
```

**Solutions:**

**1. Fix missing schema file:**
```bash
# Recreate schema file
curl -o /etc/expanso/schemas/sensor-schema-v1.0.0.json \
  https://example.com/schemas/sensor-schema-v1.0.0.json

# Set correct permissions
chmod 644 /etc/expanso/schemas/sensor-schema-v1.0.0.json
chown expanso:expanso /etc/expanso/schemas/sensor-schema-v1.0.0.json
```

**2. Fix schema syntax:**
```bash
# Validate and fix JSON syntax
jq . /etc/expanso/schemas/sensor-schema-v1.0.0.json > /tmp/schema-fixed.json
mv /tmp/schema-fixed.json /etc/expanso/schemas/sensor-schema-v1.0.0.json
```

**3. Update pipeline configuration:**
```bash
# Update schema path in pipeline
expanso job update complete-schema-validation \
  --set config.pipeline.processors[3].try[0].json_schema.schema_path="file:///etc/expanso/schemas/sensor-schema-v1.0.0.json"

# Restart pipeline
expanso job restart complete-schema-validation
```

### Issue: High False Positive Rate

**Symptoms:**
- Valid-looking messages failing validation
- Specific sensor types always failing
- Success rate below expected levels

**Possible Causes:**
1. Schema too restrictive
2. Data format evolution
3. Timezone or encoding issues
4. Locale-specific number formatting

**Diagnosis:**
```bash
# Analyze failed messages by error category
find /var/log/expanso/dlq -name "*.jsonl" -mtime -1 -exec cat {} \; | \
  jq -r '.dlq_metadata.error_category' | sort | uniq -c | sort -rn

# Check specific validation errors
find /var/log/expanso/dlq -name "*.jsonl" -mtime -1 -exec cat {} \; | \
  jq -r '.validation.error_details.primary_error' | head -10

# Identify patterns in failed sensors
find /var/log/expanso/dlq -name "*.jsonl" -mtime -1 -exec cat {} \; | \
  jq -r '.sensor_id' | sort | uniq -c | sort -rn | head -10

# Sample failed messages for manual inspection
find /var/log/expanso/dlq -name "*.jsonl" -mtime -1 -exec cat {} \; | \
  head -5 | jq .
```

**Solutions:**

**1. Relax schema constraints:**
```json
{
  "properties": {
    "sensor_id": {
      "type": "string",
      "pattern": "^(sensor|device)-[0-9]+$"  // More flexible pattern
    },
    "readings": {
      "properties": {
        "temperature_celsius": {
          "type": "number",
          "minimum": -60,  // Expanded range
          "maximum": 120
        }
      }
    }
  }
}
```

**2. Handle data format variations:**
```yaml
pipeline:
  processors:
    # Normalize data before validation
    - mapping: |
        root = this
        
        # Normalize sensor ID format
        if this.sensor_id.exists() {
          root.sensor_id = this.sensor_id.re_replace("device-", "sensor-")
        }
        
        # Handle timezone variations
        if this.timestamp.exists() && !this.timestamp.contains("Z") {
          root.timestamp = this.timestamp + "Z"
        }

    # Then apply schema validation
    - json_schema:
        schema_path: "file:///etc/expanso/schemas/sensor-schema-v1.0.0.json"
```

**3. Implement schema versioning:**
```yaml
pipeline:
  processors:
    # Try multiple schema versions
    - switch:
        - check: 'this.schema_version == "2.0"'
          processors:
            - json_schema:
                schema_path: "file:///etc/expanso/schemas/sensor-schema-v2.0.0.json"
        - check: 'this.schema_version == "1.1"'  
          processors:
            - json_schema:
                schema_path: "file:///etc/expanso/schemas/sensor-schema-v1.1.0.json"
        - processors:
            - json_schema:
                schema_path: "file:///etc/expanso/schemas/sensor-schema-v1.0.0.json"
```

### Issue: Schema Evolution Breaking Existing Sensors

**Symptoms:**
- Sudden increase in validation failures after schema update
- Previously working sensors now failing
- Specific firmware versions affected

**Diagnosis:**
```bash
# Compare validation rates before/after schema change
curl -s "http://localhost:9090/api/v1/query_range" \
  -G \
  --data-urlencode 'query=rate(schema_validation_success_total[5m])' \
  --data-urlencode 'start=2025-10-20T10:00:00Z' \
  --data-urlencode 'end=2025-10-20T16:00:00Z' \
  --data-urlencode 'step=300'

# Test schema against production samples
tail -1000 /var/log/expanso/audit/*/schema-validation-audit-*.jsonl | \
  grep '"validation_status":"passed"' | \
  head -100 | \
  jq -r 'select(.audit_type == "data_processed")' | \
  while read -r message; do
    echo "$message" | jq --schema-file /etc/expanso/schemas/sensor-schema-v1.0.0.json >/dev/null 2>&1 || \
    echo "WOULD FAIL: $message"
  done
```

**Solutions:**

**1. Implement backward-compatible schema:**
```json
{
  "anyOf": [
    {
      "$ref": "#/definitions/v1_format"
    },
    {
      "$ref": "#/definitions/v2_format" 
    }
  ],
  "definitions": {
    "v1_format": {
      "properties": {
        "temperature": {"type": "number"}  // Old field name
      }
    },
    "v2_format": {
      "properties": {
        "temperature_celsius": {"type": "number"}  // New field name
      }
    }
  }
}
```

**2. Create schema migration pipeline:**
```yaml
pipeline:
  processors:
    # Migrate old format to new format
    - mapping: |
        root = this
        
        # Migrate temperature field name
        if this.temperature.exists() && !this.temperature_celsius.exists() {
          root.readings.temperature_celsius = this.temperature
          root.readings = root.readings.without("temperature")
        }
        
        # Add default values for new required fields
        if !this.metadata.exists() {
          root.metadata = {
            "firmware_version": "1.0.0",
            "battery_percent": 100
          }
        }

    # Apply new schema after migration
    - json_schema:
        schema_path: "file:///etc/expanso/schemas/sensor-schema-v2.0.0.json"
```

**3. Gradual rollout strategy:**
```bash
# Deploy to subset of nodes first
expanso job update complete-schema-validation \
  --selector "role=sensor-collector,validation=required,environment=staging"

# Monitor for 24 hours
sleep 86400

# Rollout to production if validation rate > 99%
SUCCESS_RATE=$(curl -s "http://localhost:9090/api/v1/query" \
  --data-urlencode 'query=avg_over_time(rate(schema_validation_success_total[24h])/rate(pipeline_input_received[24h]))[24h:1h]' | \
  jq -r '.data.result[0].value[1]')

if (( $(echo "$SUCCESS_RATE > 0.99" | bc -l) )); then
  expanso job update complete-schema-validation \
    --selector "role=sensor-collector,validation=required"
else
  echo "Rollout halted: Success rate $SUCCESS_RATE below threshold"
fi
```

---

## Performance Issues

### Issue: High Processing Latency

**Symptoms:**
- P95 latency > 500ms
- Processing time increasing over time
- Timeout errors in logs

**Possible Causes:**
1. Schema complexity too high
2. Memory leaks
3. Network latency to endpoints
4. Resource contention

**Diagnosis:**
```bash
# Check latency distribution
curl -s "http://localhost:9090/api/v1/query" \
  --data-urlencode 'query=histogram_quantile(0.95, rate(total_processing_time_ms_bucket[5m]))'

# Monitor memory usage trend
curl -s "http://localhost:9090/api/v1/query_range" \
  -G \
  --data-urlencode 'query=process_resident_memory_bytes' \
  --data-urlencode 'start=2025-10-20T10:00:00Z' \
  --data-urlencode 'end=2025-10-20T16:00:00Z' \
  --data-urlencode 'step=300'

# Check CPU usage
top -p $(pgrep -f expanso) -n 5

# Test schema validation performance
time for i in {1..1000}; do
  echo '{
    "sensor_id": "sensor-42",
    "timestamp": "2025-10-20T14:30:00Z",
    "location": {"building": "warehouse-3", "floor": 2, "zone": "A"},
    "readings": {"temperature_celsius": 23.5, "humidity_percent": 45.2},
    "metadata": {"firmware_version": "2.1.0", "battery_percent": 87}
  }' | jq --schema-file /etc/expanso/schemas/sensor-schema-v1.0.0.json >/dev/null
done

# Check network latency to endpoints
curl -w "@curl-format.txt" -o /dev/null -s "${ANALYTICS_ENDPOINT}/health"

# Create curl timing format file
cat > curl-format.txt << 'EOF'
     time_namelookup:  %{time_namelookup}s\n
        time_connect:  %{time_connect}s\n
     time_appconnect:  %{time_appconnect}s\n
    time_pretransfer:  %{time_pretransfer}s\n
       time_redirect:  %{time_redirect}s\n
  time_starttransfer:  %{time_starttransfer}s\n
                     ----------\n
          time_total:  %{time_total}s\n
EOF
```

**Solutions:**

**1. Optimize schema for performance:**
```json
{
  "description": "Performance-optimized schema",
  "type": "object",
  "required": ["sensor_id", "readings"],
  "properties": {
    "sensor_id": {
      "type": "string",
      "minLength": 5,
      "maxLength": 50
    },
    "readings": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "temperature_celsius": {"type": "number"},
        "humidity_percent": {"type": "number"}
      }
    }
  },
  "additionalProperties": false
}
```

**2. Enable parallel processing:**
```yaml
pipeline:
  processors:
    - parallel:
        cap: 10  # Process up to 10 messages in parallel
        processors:
          - json_schema:
              schema_path: "file:///etc/expanso/schemas/sensor-schema-v1.0.0.json"
```

**3. Implement caching:**
```yaml
pipeline:
  processors:
    # Cache validation results for identical messages
    - cache:
        resource: validation_cache
        key: "validation_${!content().hash()}"
        ttl: 300s  # 5 minute cache
    
    # Only validate if not in cache
    - switch:
        - check: '!meta("cache_hit")'
          processors:
            - json_schema:
                schema_path: "file:///etc/expanso/schemas/sensor-schema-v1.0.0.json"
```

**4. Optimize batching:**
```yaml
output:
  http_client:
    batching:
      count: 500      # Larger batches
      period: 2s      # Faster flush
      byte_size: 50MB # Size-based batching
    compression: gzip # Reduce network overhead
```

### Issue: Memory Leaks

**Symptoms:**
- Memory usage continuously increasing
- Pipeline eventually crashes with OOM
- Slower processing over time

**Diagnosis:**
```bash
# Monitor memory growth over time
watch -n 30 'ps -p $(pgrep expanso) -o pid,vsz,rss,pcpu,pmem,time'

# Check for memory leaks in pipeline metrics
curl -s http://localhost:8080/metrics | grep -E "memory|heap|gc"

# Analyze garbage collection patterns
expanso job logs complete-schema-validation | grep -E "gc|memory|heap" | tail -20

# Memory dump analysis (if available)
jcmd $(pgrep expanso) GC.dump /tmp/heap-dump.hprof
```

**Solutions:**

**1. Limit message size:**
```yaml
input:
  http_server:
    max_request_size: "1MB"  # Limit request size

pipeline:
  processors:
    # Drop oversized messages
    - switch:
        - check: 'content().bytes() > 100000'  # 100KB limit
          processors:
            - mapping: |
                root = {
                  "error": "message_too_large",
                  "size_bytes": content().bytes()
                }
                meta validation_status = "rejected_size"
        - processors:
            - json_schema:
                schema_path: "file:///etc/expanso/schemas/sensor-schema-v1.0.0.json"
```

**2. Implement memory limits:**
```yaml
deployment:
  resources:
    limits:
      memory: "2Gi"
      cpu: "2"
    requests:
      memory: "1Gi" 
      cpu: "1"
```

**3. Add garbage collection tuning:**
```bash
# Set JVM options for better GC (if using Java-based processor)
export JAVA_OPTS="-Xmx2g -XX:+UseG1GC -XX:MaxGCPauseMillis=200"

# Monitor GC effectiveness
jstat -gc $(pgrep expanso) 10s
```

### Issue: Low Throughput

**Symptoms:**
- Processing fewer messages than expected
- Backlog building up
- Pipeline can't keep up with input rate

**Diagnosis:**
```bash
# Check current throughput
curl -s "http://localhost:9090/api/v1/query" \
  --data-urlencode 'query=rate(pipeline_input_received[1m])'

curl -s "http://localhost:9090/api/v1/query" \
  --data-urlencode 'query=rate(pipeline_output_sent_total[1m])'

# Identify bottlenecks
curl -s http://localhost:8080/metrics | grep -E "duration|time" | sort -k2 -nr

# Check for resource constraints
iostat -x 1 5
vmstat 1 5
```

**Solutions:**

**1. Scale horizontally:**
```bash
# Deploy to more edge nodes
expanso node label edge-node-003 role=sensor-collector validation=required
expanso node label edge-node-004 role=sensor-collector validation=required

# Verify pipeline deploys to new nodes
expanso job status complete-schema-validation
```

**2. Optimize processing pipeline:**
```yaml
pipeline:
  processors:
    # Remove unnecessary processing
    - json_documents:
        parts: []
    
    # Streamlined validation
    - json_schema:
        schema_path: "file:///etc/expanso/schemas/sensor-schema-v1.0.0.json"
    
    # Minimal metadata
    - mapping: |
        root = this
        root.validation_status = "passed"
        meta validation_status = "passed"
```

**3. Increase rate limits:**
```yaml
input:
  http_server:
    rate_limit: "2000/1s"  # Increase from 1000/s
    timeout: 10s           # Increase timeout
```

---

## Operational Issues

### Issue: DLQ Growing Too Fast

**Symptoms:**
- DLQ files consuming excessive disk space
- Disk usage alerts firing
- Old DLQ files not being cleaned up

**Diagnosis:**
```bash
# Check DLQ disk usage
du -sh /var/log/expanso/dlq/*

# Count DLQ entries by priority
find /var/log/expanso/dlq/urgent -name "*.jsonl" | wc -l
find /var/log/expanso/dlq/high -name "*.jsonl" | wc -l
find /var/log/expanso/dlq/normal -name "*.jsonl" | wc -l

# Check DLQ growth rate
ls -lt /var/log/expanso/dlq/*/*.jsonl | head -20

# Analyze DLQ entry patterns
find /var/log/expanso/dlq -name "*.jsonl" -mtime -1 -exec cat {} \; | \
  jq -r '.dlq_metadata.error_category' | sort | uniq -c | sort -rn
```

**Solutions:**

**1. Implement DLQ rotation:**
```bash
# Create DLQ rotation script
cat > /opt/expanso/scripts/dlq-rotate.sh << 'EOF'
#!/bin/bash

MAX_FILE_SIZE_MB=100
DLQ_BASE_DIR="/var/log/expanso/dlq"

find "$DLQ_BASE_DIR" -name "*.jsonl" -size +${MAX_FILE_SIZE_MB}M | while read file; do
  timestamp=$(date +%Y%m%d-%H%M%S)
  rotated="${file%.jsonl}-${timestamp}.jsonl.gz"
  
  gzip -c "$file" > "$rotated"
  rm "$file"
  
  echo "Rotated: $file -> $rotated"
done
EOF

chmod +x /opt/expanso/scripts/dlq-rotate.sh

# Schedule rotation
echo "0 */6 * * * /opt/expanso/scripts/dlq-rotate.sh" | crontab -
```

**2. Aggressive cleanup for low-priority entries:**
```bash
# Clean up normal priority DLQ more frequently
find /var/log/expanso/dlq/normal -name "*.jsonl" -mtime +2 -delete

# Archive urgent/high priority before deletion
find /var/log/expanso/dlq/urgent -name "*.jsonl" -mtime +30 -exec gzip {} \;
find /var/log/expanso/dlq/high -name "*.jsonl" -mtime +7 -exec gzip {} \;
```

**3. Reduce DLQ verbosity:**
```yaml
pipeline:
  processors:
    # Only store essential error information
    - switch:
        - check: 'meta("validation_status") == "failed"'
          processors:
            - mapping: |
                root = {
                  "message_id": this.message_id,
                  "sensor_id": this.sensor_id.or("unknown"),
                  "error_category": this.validation.error_details.classification.error_category,
                  "timestamp": now()
                }
```

### Issue: Pipeline Not Starting

**Symptoms:**
- Pipeline status shows "failed" or "pending"
- No HTTP endpoint responding
- No metrics being generated

**Diagnosis:**
```bash
# Check pipeline status and logs
expanso job status complete-schema-validation
expanso job logs complete-schema-validation --tail 50

# Check node resources
expanso node describe $(expanso node list -q)

# Verify schema files
find /etc/expanso/schemas -name "*.json" -exec jq empty {} \;

# Check port availability
netstat -ln | grep :8080

# Verify node selector matching
expanso node list --show-labels | grep validation
```

**Solutions:**

**1. Fix resource constraints:**
```bash
# Check node capacity
kubectl describe node $(hostname)

# Reduce resource requirements
expanso job update complete-schema-validation \
  --set deployment.resources.requests.memory=512Mi \
  --set deployment.resources.requests.cpu=500m
```

**2. Fix configuration issues:**
```bash
# Validate pipeline configuration
expanso job validate enforce-schema-complete.yaml

# Check schema file permissions
chmod 644 /etc/expanso/schemas/*.json
chown expanso:expanso /etc/expanso/schemas/*.json
```

**3. Port conflict resolution:**
```bash
# Find process using port
lsof -i :8080

# Kill conflicting process or change port
expanso job update complete-schema-validation \
  --set config.input.http_server.address="0.0.0.0:8081"
```

### Issue: Analytics Endpoint Unreachable

**Symptoms:**
- Valid messages not reaching analytics
- Analytics endpoint errors in logs
- High retry rates in metrics

**Diagnosis:**
```bash
# Test analytics endpoint connectivity
curl -v "${ANALYTICS_ENDPOINT}/health"

# Check endpoint configuration
echo "Analytics endpoint: $ANALYTICS_ENDPOINT"
echo "Fallback endpoint: $ANALYTICS_FALLBACK_ENDPOINT"

# Review retry metrics
curl -s http://localhost:8080/metrics | grep -E "retry|failure"

# Check DNS resolution
nslookup $(echo "$ANALYTICS_ENDPOINT" | cut -d/ -f3)
```

**Solutions:**

**1. Update endpoint configuration:**
```bash
# Update environment variables
export ANALYTICS_ENDPOINT="http://new-analytics-api:8080"
export ANALYTICS_FALLBACK_ENDPOINT="http://analytics-backup:8080"

# Restart pipeline to pick up new config
expanso job restart complete-schema-validation
```

**2. Configure endpoint discovery:**
```yaml
output:
  http_client:
    # Use service discovery
    url: "http://analytics-service.default.svc.cluster.local:8080/ingest"
    
    # Increase retry configuration
    max_retries: 5
    backoff:
      initial_interval: 2s
      max_interval: 60s
    
    # Add circuit breaker
    circuit_breaker:
      enabled: true
      failure_threshold: 10
      recovery_timeout: 30s
```

**3. Implement fallback strategy:**
```yaml
output:
  broker:
    pattern: fan_out
    outputs:
      # Primary analytics
      - try:
          - http_client:
              url: "${ANALYTICS_ENDPOINT}/ingest"
              timeout: 5s
        
        # Fallback to file storage
        catch:
          - file:
              path: "/var/log/expanso/analytics-fallback/data-${!timestamp_unix()}.jsonl"
              codec: lines
```

---

## Security Issues

### Issue: Suspected Security Violations

**Symptoms:**
- Urgent DLQ entries with security violations
- Unexpected additional properties in messages
- Alerts firing for security incidents

**Investigation:**
```bash
# Analyze security violations
find /var/log/expanso/dlq/urgent -name "*.jsonl" -mtime -1 -exec cat {} \; | \
  jq -r 'select(.dlq_metadata.error_category == "security_violation")' | \
  jq -r '.sensor_id' | sort | uniq -c | sort -rn

# Check source IPs (if available in logs)
expanso job logs complete-schema-validation | grep security_violation | \
  grep -oE '\b[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\b' | \
  sort | uniq -c | sort -rn

# Review audit logs
find /var/log/expanso/audit -name "*.jsonl" -mtime -1 -exec grep security_violation {} \;
```

**Immediate Response:**

**1. Block suspicious sources:**
```bash
# Identify top offending sensors
SUSPICIOUS_SENSORS=$(find /var/log/expanso/dlq/urgent -name "*.jsonl" -mtime -1 -exec cat {} \; | \
  jq -r 'select(.dlq_metadata.error_category == "security_violation") | .sensor_id' | \
  sort | uniq -c | sort -rn | head -10 | awk '{print $2}')

# Create blocking rule
cat > /tmp/security-block.yaml << 'EOF'
pipeline:
  processors:
    # Block suspicious sensors
    - switch:
        - check: |
            ["sensor-malicious-1", "sensor-malicious-2"].contains(this.sensor_id)
          processors:
            - mapping: |
                root = {
                  "error": "sensor_blocked_security",
                  "sensor_id": this.sensor_id,
                  "blocked_at": now()
                }
                meta validation_status = "blocked"
        
        # Continue normal processing for others
        - processors:
            - json_schema:
                schema_path: "file:///etc/expanso/schemas/sensor-schema-v1.0.0.json"
EOF
```

**2. Enhanced monitoring:**
```bash
# Set up security monitoring
cat > /tmp/security-monitor.sh << 'EOF'
#!/bin/bash

THRESHOLD=5
WINDOW_MINUTES=10

while true; do
  VIOLATIONS=$(find /var/log/expanso/dlq/urgent -name "*.jsonl" -mmin -$WINDOW_MINUTES | \
    xargs grep -l security_violation | wc -l)
  
  if [ "$VIOLATIONS" -gt "$THRESHOLD" ]; then
    echo "ALERT: $VIOLATIONS security violations in last $WINDOW_MINUTES minutes"
    # Send alert to security team
    curl -X POST "$ALERT_ENDPOINT/security-incident" \
      -d "{\"violations\": $VIOLATIONS, \"window_minutes\": $WINDOW_MINUTES}"
  fi
  
  sleep 300  # Check every 5 minutes
done
EOF

chmod +x /tmp/security-monitor.sh
nohup /tmp/security-monitor.sh &
```

### Issue: Schema Bypass Attempts

**Symptoms:**
- Messages with unexpected structure passing validation
- Schema validation not being applied consistently
- Inconsistent data quality scores

**Investigation:**
```bash
# Check if schema validation is being skipped
expanso job logs complete-schema-validation | grep -E "(skip|bypass|ignore)"

# Verify schema is being applied to all messages
curl -s http://localhost:8080/metrics | grep schema_validation_total

# Check for alternative processing paths
expanso job describe complete-schema-validation | grep -A 10 -B 10 switch
```

**Solutions:**

**1. Enforce validation on all paths:**
```yaml
pipeline:
  processors:
    # Ensure all messages go through validation
    - json_schema:
        schema_path: "file:///etc/expanso/schemas/sensor-schema-v1.0.0.json"
    
    # No conditional logic before validation
    - mapping: |
        root = this
        meta validation_passed = "true"
```

**2. Add validation checkpoints:**
```yaml
pipeline:
  processors:
    # Multiple validation points
    - json_schema:
        schema_path: "file:///etc/expanso/schemas/sensor-schema-v1.0.0.json"
    
    # Additional structural checks
    - mapping: |
        if !this.sensor_id.exists() || !this.readings.exists() {
          throw("critical_field_missing")
        }
        root = this
    
    # Final validation before output
    - switch:
        - check: 'meta("validation_passed") != "true"'
          processors:
            - mapping: 'throw("validation_not_confirmed")'
```

---

## Monitoring and Alerting Issues

### Issue: Missing Metrics

**Symptoms:**
- Prometheus metrics not updating
- Dashboards showing no data
- Alerts not firing

**Diagnosis:**
```bash
# Check metrics endpoint
curl -s http://localhost:8080/metrics

# Verify Prometheus scraping
curl -s http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | select(.labels.job == "schema-validation")'

# Check metrics configuration
expanso job describe complete-schema-validation | grep -A 20 metrics

# Test pipeline health endpoint
curl -s http://localhost:8080/ping
```

**Solutions:**

**1. Fix metrics configuration:**
```yaml
metrics:
  prometheus:
    path: /metrics
    push_interval: 15s
    # Ensure pushgateway URL is correct
    push_url: "http://prometheus-pushgateway:9091"
  
  # Add custom metrics
  mapping: |
    root = this
    meta pipeline = "complete-schema-validation"
    meta validation_enabled = "true"
```

**2. Verify network connectivity:**
```bash
# Test pushgateway connectivity
curl -v http://prometheus-pushgateway:9091/metrics

# Check firewall rules
iptables -L | grep 9091

# Test from pipeline container
expanso job exec complete-schema-validation -- \
  curl -v http://prometheus-pushgateway:9091/metrics
```

### Issue: False Positive Alerts

**Symptoms:**
- Alerts firing for normal conditions
- Alert fatigue from too many notifications
- Alerts not correlating with actual problems

**Solutions:**

**1. Tune alert thresholds:**
```yaml
# Adjust thresholds based on baseline
- alert: HighValidationFailureRate
  expr: |
    sum(rate(schema_validation_failure_total[5m])) / 
    sum(rate(pipeline_input_received[5m])) > 0.10  # Increased from 0.05
  for: 10m  # Increased from 2m
```

**2. Add context to alerts:**
```yaml
- alert: DataQualityDegraded
  expr: avg(data_quality_score) < 90
  for: 15m
  annotations:
    description: |
      Data quality score is {{ $value }}%
      Recent error categories: {{ range query "topk(3, sum by (error_category)(rate(validation_errors_by_category_total[5m])))" }}
      - {{ .Metric.error_category }}: {{ .Value }}
      {{ end }}
```

**3. Implement alert suppression:**
```yaml
- alert: ValidationFailureRateHigh
  expr: (rate of failures) > threshold
  for: 5m
  # Only alert during business hours
  labels:
    severity: warning
  annotations:
    summary: "High validation failure rate"
  # Add silence during maintenance windows
  active_time_intervals:
    - business_hours
```

---

## Recovery Procedures

### Complete System Recovery

**When to use:** Pipeline completely down, multiple components failing

**Recovery steps:**

**1. Assess scope of failure:**
```bash
# Check overall system health
expanso cluster status
expanso node list
expanso job list | grep schema

# Check infrastructure dependencies
curl -s http://prometheus:9090/api/v1/targets
curl -s http://analytics-api:8080/health
curl -s http://alertmanager:9093/api/v1/status
```

**2. Restore core services:**
```bash
# Restart pipeline
expanso job restart complete-schema-validation

# If that fails, redeploy
expanso job delete complete-schema-validation
expanso job deploy enforce-schema-complete.yaml

# Verify schema files
find /etc/expanso/schemas -name "*.json" -exec jq empty {} \;
```

**3. Validate recovery:**
```bash
# Test end-to-end flow
curl -X POST http://localhost:8080/sensor/readings \
  -H "Content-Type: application/json" \
  -d '{
    "sensor_id": "sensor-recovery-test",
    "timestamp": "2025-10-20T14:30:00Z",
    "location": {"building": "warehouse-1", "floor": 1, "zone": "A"},
    "readings": {"temperature_celsius": 20.0, "humidity_percent": 50.0},
    "metadata": {"firmware_version": "1.0.0", "battery_percent": 100}
  }'

# Check metrics
curl -s http://localhost:8080/metrics | grep validation

# Verify DLQ is empty
find /var/log/expanso/dlq -name "*.jsonl" -mmin -5 | wc -l
```

### Data Loss Prevention

**Scenario:** Pipeline failure with potential message loss

**Mitigation:**

**1. Enable persistent queuing:**
```yaml
input:
  http_server:
    # Enable message persistence
    sync_response:
      enabled: false  # Don't wait for processing
    
    # Store messages temporarily
    batching:
      count: 100
      period: 5s
      
output:
  # Always write to disk first
  broker:
    pattern: fan_out
    outputs:
      # Backup to disk
      - file:
          path: "/var/log/expanso/backup/messages-${!timestamp_unix()}.jsonl"
          codec: lines
      
      # Primary processing
      - http_client:
          url: "${ANALYTICS_ENDPOINT}"
```

**2. Implement replay capability:**
```bash
# Create replay script
cat > /opt/expanso/scripts/replay-messages.sh << 'EOF'
#!/bin/bash

BACKUP_DIR="/var/log/expanso/backup"
PIPELINE_ENDPOINT="http://localhost:8080/sensor/readings"

echo "Starting message replay from $BACKUP_DIR"

find "$BACKUP_DIR" -name "messages-*.jsonl" -mmin +5 | sort | while read backup_file; do
  echo "Replaying: $backup_file"
  
  while IFS= read -r message; do
    curl -s -X POST "$PIPELINE_ENDPOINT" \
      -H "Content-Type: application/json" \
      -d "$message" >/dev/null
  done < "$backup_file"
  
  # Archive processed file
  mv "$backup_file" "${backup_file}.processed"
done

echo "Replay completed"
EOF

chmod +x /opt/expanso/scripts/replay-messages.sh
```

---

## Emergency Contacts and Escalation

### Escalation Matrix

**Level 1 - Self Service (0-30 minutes):**
- Check this troubleshooting guide
- Review pipeline logs and metrics
- Attempt basic restarts and reconfigurations

**Level 2 - Platform Team (30-60 minutes):**
- Contact: #platform-engineering Slack
- For: Pipeline configuration issues, resource constraints
- Provide: Job status, recent logs, metric screenshots

**Level 3 - Data Engineering (60+ minutes):**
- Contact: #data-engineering Slack  
- For: Schema evolution, validation logic issues
- Provide: DLQ analysis, error patterns, schema files

**Level 4 - Security Team (Immediate for security):**
- Contact: security@company.com
- For: Security violations, suspicious patterns
- Provide: Urgent DLQ entries, audit logs, source analysis

### Emergency Procedures

**Critical Data Quality Incident:**
1. **Immediate:** Stop pipeline if actively corrupting data
2. **Document:** Capture current state (logs, metrics, DLQ)  
3. **Isolate:** Block problematic sources if identified
4. **Communicate:** Alert stakeholders about potential data impact
5. **Recover:** Implement fix and validate thoroughly
6. **Post-mortem:** Conduct analysis to prevent recurrence

**Security Incident:**
1. **Immediate:** Block suspicious sources
2. **Preserve:** Capture evidence (logs, DLQ entries, audit trail)
3. **Alert:** Notify security team immediately
4. **Investigate:** Analyze attack patterns and scope
5. **Remediate:** Implement additional security controls
6. **Report:** Follow security incident reporting procedures

This troubleshooting guide covers the most common issues encountered in production schema validation deployments. For issues not covered here, contact the platform engineering team with detailed diagnostic information.

---
title: "Step 3: Route Validation Failures to Dead Letter Queue"
sidebar_label: "Step 3: Route Failures to DLQ"
sidebar_position: 5
description: Implement dead letter queue routing for validation failures while sending valid data to analytics
keywords: [dead letter queue, dlq, routing, validation failures, error handling, data pipeline]
---

# Step 3: Route Validation Failures to Dead Letter Queue

Learn how to implement sophisticated routing logic that sends valid data to analytics systems while capturing validation failures in a dead letter queue for investigation. This step ensures that data quality issues don't block processing while preserving failed messages for debugging and improvement.

## Understanding Dead Letter Queue Patterns

A Dead Letter Queue (DLQ) is a service implementation pattern for handling message processing failures. In schema validation, the DLQ captures messages that fail validation while allowing valid messages to proceed normally.

### DLQ Routing Flow

```
Validated Message → Route Decision → Valid: Analytics | Invalid: DLQ
                       ↓               ↓                    ↓
                  Check Status     Process Data        Investigate
```

**Key components:**
1. **Routing Logic** - Use validation metadata to make routing decisions
2. **Analytics Path** - Send valid messages to downstream processing systems
3. **DLQ Path** - Store invalid messages with enriched error context
4. **Parallel Processing** - Both paths operate independently for resilience
5. **Error Enrichment** - Add debugging context to failed messages

## Step 3.1: Create Basic DLQ Routing

Start with a simple routing pipeline that separates valid and invalid messages:

```bash
# Create basic DLQ routing pipeline
mkdir -p /tmp/dlq-routing

cat > /tmp/dlq-routing/basic-dlq-routing.yaml << 'EOF'
name: basic-dlq-routing
description: Basic dead letter queue routing for schema validation failures
type: pipeline
namespace: testing
labels:
  environment: testing
  routing: dlq
priority: 60

selector:
  match_labels:
    role: sensor-collector
    validation: required

deployment:
  strategy: rolling
  max_parallel: 1
  health_check:
    type: http
    endpoint: /ping
    interval: 30s

config:
  input:
    http_server:
      address: "0.0.0.0:8080"
      path: /sensor/readings
      allowed_verbs:
        - POST
      timeout: 5s

  # Pipeline with schema validation and routing
  pipeline:
    processors:
      # Step 1: Parse JSON
      - json_documents:
          parts: []

      # Step 2: Add metadata
      - mapping: |
          root = this
          root.pipeline_metadata = {
            "received_at": now(),
            "node_id": env("NODE_ID").or("unknown"),
            "pipeline_version": "basic-dlq-routing"
          }

      # Step 3: Schema validation with error capture
      - try:
          # Attempt validation
          - json_schema:
              schema_path: "file:///etc/expanso/schemas/sensor-schema-v1.0.0.json"

          # Success path: mark as valid
          - mapping: |
              root = this
              root.validation = {
                "status": "passed",
                "schema_version": "1.0.0",
                "validated_at": now()
              }
              meta validation_status = "passed"

        # Failure path: mark as invalid
        catch:
          - mapping: |
              root = this
              root.validation = {
                "status": "failed",
                "schema_version": "1.0.0",
                "validated_at": now(),
                "error": error()
              }
              meta validation_status = "failed"

  # Simple DLQ routing based on validation status
  output:
    switch:
      # Route 1: Valid messages to analytics
      - check: 'meta("validation_status") == "passed"'
        output:
          http_client:
            url: "${ANALYTICS_ENDPOINT:http://localhost:9000}/ingest"
            verb: POST
            headers:
              Content-Type: application/json
            max_retries: 3

      # Route 2: Invalid messages to DLQ file
      - check: 'meta("validation_status") == "failed"'
        output:
          file:
            path: "/tmp/dlq/validation-failures-${!timestamp_unix()}.jsonl"
            codec: lines

logger:
  level: INFO
  format: json

metrics:
  prometheus:
    path: /metrics
EOF

# Create DLQ directory
mkdir -p /tmp/dlq

# Deploy basic DLQ routing pipeline
expanso job deploy /tmp/dlq-routing/basic-dlq-routing.yaml

# Wait for deployment
sleep 10

# Verify deployment
expanso job status basic-dlq-routing
```

### Test Basic DLQ Routing

```bash
echo "=== Testing Basic DLQ Routing ==="

# Test 1: Send valid message (should go to analytics)
echo "Test 1: Valid message to analytics"
curl -X POST http://localhost:8080/sensor/readings \
  -H "Content-Type: application/json" \
  -d '{
    "sensor_id": "sensor-dlq-test-1",
    "timestamp": "2025-10-20T14:30:00Z",
    "location": {"building": "warehouse-3", "floor": 2, "zone": "A"},
    "readings": {"temperature_celsius": 23.5, "humidity_percent": 45.2},
    "metadata": {"firmware_version": "2.1.0", "battery_percent": 87}
  }'

echo "Valid message sent (should route to analytics)"
echo ""

# Test 2: Send invalid message (should go to DLQ)
echo "Test 2: Invalid message to DLQ"
curl -X POST http://localhost:8080/sensor/readings \
  -H "Content-Type: application/json" \
  -d '{
    "sensor_id": "sensor-dlq-test-2",
    "timestamp": "invalid-timestamp",
    "readings": {"temperature_celsius": -999}
  }'

echo "Invalid message sent (should route to DLQ)"
echo ""

# Wait for processing
sleep 5

# Check DLQ contents
echo "=== DLQ Contents ==="
if ls /tmp/dlq/validation-failures-*.jsonl >/dev/null 2>&1; then
  echo "DLQ files found:"
  ls -la /tmp/dlq/validation-failures-*.jsonl
  echo ""
  echo "DLQ entries:"
  cat /tmp/dlq/validation-failures-*.jsonl | jq .
else
  echo "No DLQ files found (check if invalid messages were processed)"
fi
```

## Step 3.2: Enhance DLQ with Error Context

Create an enhanced DLQ system that adds rich debugging context to failed messages:

```bash
# Create enhanced DLQ pipeline with error context
cat > /tmp/dlq-routing/enhanced-dlq-routing.yaml << 'EOF'
name: enhanced-dlq-routing  
description: Enhanced DLQ routing with rich error context and debugging information
type: pipeline
namespace: testing
labels:
  environment: testing
  routing: enhanced-dlq
priority: 70

selector:
  match_labels:
    role: sensor-collector
    validation: required

deployment:
  strategy: rolling
  max_parallel: 1
  health_check:
    type: http
    endpoint: /ping
    interval: 30s

config:
  input:
    http_server:
      address: "0.0.0.0:8080"
      path: /sensor/readings
      allowed_verbs:
        - POST
      timeout: 5s

  # Enhanced pipeline with rich error context
  pipeline:
    processors:
      # Step 1: Initialize processing metadata
      - mapping: |
          root = this
          meta original_content = content()
          meta start_time = now()
          meta input_size = content().bytes()

      # Step 2: Parse JSON with error context
      - try:
          - json_documents:
              parts: []
          
          # JSON parsing succeeded
          - mapping: |
              meta parse_status = "success"
        
        catch:
          # JSON parsing failed - create detailed error context
          - mapping: |
              root = {
                "error_type": "json_parse_error",
                "error_details": {
                  "message": error(),
                  "parse_failed_at": now(),
                  "input_preview": meta("original_content").bytes(0, 500),  # First 500 characters
                  "input_size_bytes": meta("input_size"),
                  "likely_cause": "malformed_json_syntax"
                },
                "raw_input": meta("original_content"),
                "processing_context": {
                  "node_id": env("NODE_ID").or("unknown"),
                  "pipeline_version": "enhanced-dlq-routing",
                  "received_at": meta("start_time"),
                  "processed_at": now()
                }
              }
              meta parse_status = "failed"
              meta validation_status = "parse_failed"

      # Step 3: Add pipeline metadata for successfully parsed messages
      - switch:
          - check: 'meta("parse_status") == "success"'
            processors:
              - mapping: |
                  root = this
                  root.pipeline_metadata = {
                    "received_at": meta("start_time"),
                    "node_id": env("NODE_ID").or("unknown"),
                    "pipeline_version": "enhanced-dlq-routing",
                    "input_size_bytes": meta("input_size"),
                    "original_content": meta("original_content")
                  }

      # Step 4: Schema validation with detailed error analysis
      - switch:
          - check: 'meta("parse_status") == "success"'
            processors:
              - try:
                  # Attempt schema validation
                  - json_schema:
                      schema_path: "file:///etc/expanso/schemas/sensor-schema-v1.0.0.json"

                  # Validation success - enrich with success context
                  - mapping: |
                      root = this
                      root.validation = {
                        "status": "passed",
                        "schema_version": "1.0.0",
                        "validated_at": now(),
                        "processing_time_ms": (now() - meta("start_time")).parse_duration().milliseconds()
                      }
                      meta validation_status = "passed"

                # Validation failed - create comprehensive error analysis
                catch:
                  - mapping: |
                      root = this
                      
                      # Analyze the validation error for common issues
                      error_msg = error()
                      error_analysis = {
                        "primary_error": error_msg,
                        "error_category": if error_msg.contains("required") { "missing_required_field" }
                                         else if error_msg.contains("type") { "invalid_data_type" }
                                         else if error_msg.contains("minimum") || error_msg.contains("maximum") { "value_out_of_range" }
                                         else if error_msg.contains("format") { "invalid_format" }
                                         else if error_msg.contains("pattern") { "pattern_mismatch" }
                                         else if error_msg.contains("additionalProperties") { "unexpected_property" }
                                         else { "unknown_validation_error" },
                        "field_analysis": {
                          "sensor_id_present": this.sensor_id.exists(),
                          "sensor_id_type": this.sensor_id.type(),
                          "timestamp_present": this.timestamp.exists(),
                          "timestamp_type": this.timestamp.type(),
                          "location_present": this.location.exists(),
                          "readings_present": this.readings.exists(),
                          "metadata_present": this.metadata.exists()
                        }
                      }
                      
                      # Create detailed validation failure context
                      root.validation = {
                        "status": "failed",
                        "schema_version": "1.0.0",
                        "validated_at": now(),
                        "processing_time_ms": (now() - meta("start_time")).parse_duration().milliseconds(),
                        "error_analysis": error_analysis,
                        "debugging_hints": if error_analysis.error_category == "missing_required_field" {
                          ["Check that all required fields are present: sensor_id, timestamp, location, readings, metadata"]
                        } else if error_analysis.error_category == "invalid_data_type" {
                          ["Verify data types: sensor_id (string), timestamp (string), floor (integer), temperature (number)"]
                        } else if error_analysis.error_category == "value_out_of_range" {
                          ["Check value ranges: temperature (-50 to 100°C), humidity (0-100%), battery (0-100%)"]
                        } else if error_analysis.error_category == "pattern_mismatch" {
                          ["Verify patterns: sensor_id (sensor-123), building (warehouse-N), zone (A-Z), firmware (X.Y.Z)"]
                        } else if error_analysis.error_category == "unexpected_property" {
                          ["Remove unexpected fields - only allowed properties are: sensor_id, timestamp, location, readings, metadata"]
                        } else {
                          ["Review the JSON Schema specification for detailed validation rules"]
                        }
                      }
                      
                      meta validation_status = "failed"

      # Step 5: Enrich with data quality assessment
      - mapping: |
          root = this
          
          # Calculate data quality score and metrics
          quality_assessment = {
            "validation_passed": meta("validation_status") == "passed",
            "data_completeness": if this.sensor_id.exists() && this.timestamp.exists() && this.location.exists() && this.readings.exists() && this.metadata.exists() { 100 }
                                else { ((if this.sensor_id.exists() { 1 } else { 0 }) + 
                                       (if this.timestamp.exists() { 1 } else { 0 }) +
                                       (if this.location.exists() { 1 } else { 0 }) +
                                       (if this.readings.exists() { 1 } else { 0 }) +
                                       (if this.metadata.exists() { 1 } else { 0 })) * 20 },
            "data_freshness": if this.timestamp.exists() { 
                                (now() - this.timestamp.parse_timestamp("%Y-%m-%dT%H:%M:%S%z")).seconds()
                             } else { -1 },
            "processing_node": env("NODE_ID").or("unknown"),
            "assessment_timestamp": now()
          }
          
          root.data_quality = quality_assessment

      # Step 6: Add routing metadata and correlation IDs
      - mapping: |
          root = this
          
          # Generate correlation ID for tracking
          correlation_id = uuid_v4()
          root.correlation_id = correlation_id
          
          # Add routing metadata
          root.routing = {
            "destination": if meta("validation_status") == "passed" { "analytics" } else { "dlq" },
            "routing_decision_time": now(),
            "correlation_id": correlation_id,
            "message_flow": if meta("validation_status") == "passed" { "success_path" } else { "failure_path" }
          }

  # Enhanced output routing with parallel processing
  output:
    broker:
      pattern: fan_out
      outputs:
        # Output 1: Valid messages to analytics
        - label: analytics
          switch:
            - check: 'meta("validation_status") == "passed"'
              output:
                http_client:
                  url: "${ANALYTICS_ENDPOINT:http://localhost:9000}/ingest"
                  verb: POST
                  headers:
                    Content-Type: application/json
                    X-Correlation-ID: "${!json(\"correlation_id\")}"
                  batching:
                    count: 50
                    period: 5s
                  max_retries: 3
                  backoff:
                    initial_interval: 1s
                    max_interval: 30s

            # Drop messages that failed validation (they go to DLQ)
            - output:
                drop: {}

        # Output 2: Invalid messages to structured DLQ
        - label: dead_letter_queue
          switch:
            - check: 'meta("validation_status") == "failed" || meta("validation_status") == "parse_failed"'
              output:
                # Date-partitioned DLQ files for easier management
                file:
                  path: "/tmp/dlq/${!timestamp_unix_date('2006/01/02')}/validation-failures-${!timestamp_unix()}.jsonl"
                  codec: lines
                  batching:
                    count: 10
                    period: 30s

            # Drop messages that passed validation (they go to analytics)
            - output:
                drop: {}

        # Output 3: All message metadata to monitoring
        - label: monitoring
          processors:
            # Extract monitoring data only
            - mapping: |
                root = {
                  "correlation_id": this.correlation_id,
                  "validation_status": meta("validation_status"),
                  "sensor_id": this.sensor_id.or("unknown"),
                  "node_id": env("NODE_ID").or("unknown"),
                  "processing_time_ms": this.validation.processing_time_ms.or(0),
                  "data_quality_score": this.data_quality.data_completeness.or(0),
                  "timestamp": now(),
                  "routing_destination": this.routing.destination
                }

          http_client:
            url: "${METRICS_ENDPOINT:http://localhost:9001}/validation-metrics"
            verb: POST
            headers:
              Content-Type: application/json
            max_retries: 0  # Fire and forget for metrics

# Enhanced logging with correlation IDs
logger:
  level: INFO
  format: json
  static_fields:
    component: enhanced-dlq-routing

# Prometheus metrics with DLQ routing labels  
metrics:
  prometheus:
    path: /metrics
  mapping: |
    root = this
    meta pipeline = "enhanced-dlq-routing"
    meta dlq_enabled = "true"
    meta correlation_id = this.correlation_id.or("unknown")
EOF

# Create date-partitioned DLQ directory structure
mkdir -p /tmp/dlq/$(date +%Y/%m/%d)

# Deploy enhanced DLQ pipeline
expanso job delete basic-dlq-routing
expanso job deploy /tmp/dlq-routing/enhanced-dlq-routing.yaml

# Wait for deployment
sleep 15

# Verify deployment
expanso job status enhanced-dlq-routing
```

### Test Enhanced DLQ with Multiple Error Types

```bash
echo "=== Testing Enhanced DLQ with Multiple Error Types ==="

# Test 1: Valid message
echo "Test 1: Valid message (should route to analytics)"
curl -X POST http://localhost:8080/sensor/readings \
  -H "Content-Type: application/json" \
  -d '{
    "sensor_id": "sensor-enhanced-test-1",
    "timestamp": "2025-10-20T14:30:00Z",
    "location": {"building": "warehouse-3", "floor": 2, "zone": "A"},
    "readings": {"temperature_celsius": 23.5, "humidity_percent": 45.2},
    "metadata": {"firmware_version": "2.1.0", "battery_percent": 87}
  }'
echo ""

# Test 2: JSON parse error
echo "Test 2: Invalid JSON (should route to DLQ with parse error)"
curl -X POST http://localhost:8080/sensor/readings \
  -H "Content-Type: application/json" \
  -d '{"sensor_id": "sensor-enhanced-test-2", "invalid": json syntax}'
echo ""

# Test 3: Missing required fields
echo "Test 3: Missing required fields (should route to DLQ)"
curl -X POST http://localhost:8080/sensor/readings \
  -H "Content-Type: application/json" \
  -d '{"sensor_id": "sensor-enhanced-test-3"}'
echo ""

# Test 4: Wrong data types
echo "Test 4: Wrong data types (should route to DLQ)"
curl -X POST http://localhost:8080/sensor/readings \
  -H "Content-Type: application/json" \
  -d '{
    "sensor_id": "sensor-enhanced-test-4",
    "timestamp": "2025-10-20T14:30:00Z",
    "location": {"building": "warehouse-3", "floor": "second floor", "zone": "A"},
    "readings": {"temperature_celsius": "twenty-three", "humidity_percent": 45.2},
    "metadata": {"firmware_version": "2.1.0", "battery_percent": 87}
  }'
echo ""

# Test 5: Values out of range
echo "Test 5: Values out of range (should route to DLQ)"
curl -X POST http://localhost:8080/sensor/readings \
  -H "Content-Type: application/json" \
  -d '{
    "sensor_id": "sensor-enhanced-test-5",
    "timestamp": "2025-10-20T14:30:00Z",
    "location": {"building": "warehouse-3", "floor": 2, "zone": "A"},
    "readings": {"temperature_celsius": -500, "humidity_percent": 150},
    "metadata": {"firmware_version": "2.1.0", "battery_percent": 87}
  }'
echo ""

# Test 6: Additional properties (security test)
echo "Test 6: Additional properties (should route to DLQ)"
curl -X POST http://localhost:8080/sensor/readings \
  -H "Content-Type: application/json" \
  -d '{
    "sensor_id": "sensor-enhanced-test-6",
    "timestamp": "2025-10-20T14:30:00Z",
    "location": {"building": "warehouse-3", "floor": 2, "zone": "A"},
    "readings": {"temperature_celsius": 23.5, "humidity_percent": 45.2},
    "metadata": {"firmware_version": "2.1.0", "battery_percent": 87},
    "admin_override": {"command": "shutdown"}
  }'
echo ""

# Wait for processing
sleep 10

# Check DLQ structure and contents
echo "=== Enhanced DLQ Analysis ==="
echo "DLQ directory structure:"
find /tmp/dlq -type f -name "*.jsonl" | head -10

if find /tmp/dlq -name "*.jsonl" -type f | head -1 | read first_file; then
  echo ""
  echo "Sample DLQ entries with error analysis:"
  cat /tmp/dlq/*/*.jsonl | head -3 | jq '
    {
      correlation_id: .correlation_id,
      validation_status: .validation.status,
      error_category: .validation.error_analysis.error_category,
      debugging_hints: .validation.debugging_hints,
      data_quality: .data_quality.data_completeness,
      routing_destination: .routing.destination
    }'
else
  echo "No DLQ entries found"
fi
```

## Step 3.3: Implement DLQ Management and Retention

Create a comprehensive DLQ management system with retention policies:

```bash
# Create DLQ management pipeline
cat > /tmp/dlq-routing/production-dlq-with-management.yaml << 'EOF'
name: production-dlq-routing
description: Production DLQ routing with management, retention, and alerting
type: pipeline
namespace: production
labels:
  environment: production
  routing: production-dlq
  management: automated
priority: 100

selector:
  match_labels:
    role: sensor-collector
    validation: required

deployment:
  strategy: rolling
  max_parallel: 2
  health_check:
    type: http
    endpoint: /ping
    interval: 30s
  auto_rollback: true

config:
  input:
    http_server:
      address: "0.0.0.0:8080"
      path: /sensor/readings
      allowed_verbs:
        - POST
      timeout: 5s
      rate_limit: "1000/1s"

  # Production pipeline with comprehensive DLQ management
  pipeline:
    processors:
      # Initialize processing with unique message ID
      - mapping: |
          root = this
          message_id = uuid_v4()
          root.message_id = message_id
          meta original_content = content()
          meta start_time = now()
          meta input_size = content().bytes()
          meta message_id = message_id

      # JSON parsing with detailed error tracking
      - try:
          - json_documents:
              parts: []
          - mapping: |
              meta parse_status = "success"
        catch:
          - mapping: |
              root = {
                "message_id": meta("message_id"),
                "error_type": "json_parse_error",
                "error_details": {
                  "message": error(),
                  "parse_failed_at": now(),
                  "input_size_bytes": meta("input_size"),
                  "content_preview": meta("original_content").bytes(0, 200)
                },
                "dlq_metadata": {
                  "dlq_entry_time": now(),
                  "dlq_reason": "json_parse_failure",
                  "retention_class": "short_term",  # Keep for 7 days
                  "investigation_priority": "low"
                }
              }
              meta parse_status = "failed"
              meta validation_status = "parse_failed"

      # Enhanced metadata for successfully parsed messages
      - switch:
          - check: 'meta("parse_status") == "success"'
            processors:
              - mapping: |
                  root = this
                  root.pipeline_metadata = {
                    "message_id": meta("message_id"),
                    "received_at": meta("start_time"),
                    "node_id": env("NODE_ID").or("unknown"),
                    "pipeline_version": "production-dlq-v1.0.0",
                    "input_size_bytes": meta("input_size"),
                    "environment": "production"
                  }

      # Schema validation with smart error classification
      - switch:
          - check: 'meta("parse_status") == "success"'
            processors:
              - try:
                  - json_schema:
                      schema_path: "file:///etc/expanso/schemas/sensor-schema-v1.0.0.json"

                  # Success: enrich with success metadata
                  - mapping: |
                      root = this
                      root.validation = {
                        "status": "passed",
                        "schema_version": "1.0.0",
                        "validated_at": now(),
                        "processing_time_ms": (now() - meta("start_time")).parse_duration().milliseconds()
                      }
                      meta validation_status = "passed"

                # Failure: intelligent error classification for DLQ management
                catch:
                  - mapping: |
                      root = this
                      error_msg = error()
                      
                      # Classify error severity and investigation priority
                      error_classification = {
                        "error_category": if error_msg.contains("required") { "missing_required_field" }
                                         else if error_msg.contains("type") { "invalid_data_type" }
                                         else if error_msg.contains("minimum") || error_msg.contains("maximum") { "value_out_of_range" }
                                         else if error_msg.contains("additionalProperties") { "security_violation" }
                                         else if error_msg.contains("pattern") { "format_violation" }
                                         else { "unknown_schema_error" },
                        "severity": if error_msg.contains("additionalProperties") { "high" }
                                   else if error_msg.contains("required") { "medium" }
                                   else { "low" },
                        "investigation_priority": if error_msg.contains("additionalProperties") { "urgent" }
                                                else if error_msg.contains("required") { "high" }
                                                else if this.sensor_id.or("").re_match(".*prod.*") { "high" }
                                                else { "normal" },
                        "retention_class": if error_msg.contains("additionalProperties") { "long_term" }  # Keep 90 days
                                          else if error_msg.contains("required") { "medium_term" }     # Keep 30 days
                                          else { "short_term" }                                        # Keep 7 days
                      }
                      
                      root.validation = {
                        "status": "failed",
                        "schema_version": "1.0.0",
                        "validated_at": now(),
                        "processing_time_ms": (now() - meta("start_time")).parse_duration().milliseconds(),
                        "error_details": {
                          "primary_error": error_msg,
                          "classification": error_classification
                        }
                      }
                      
                      # DLQ-specific metadata
                      root.dlq_metadata = {
                        "dlq_entry_time": now(),
                        "dlq_reason": "schema_validation_failure",
                        "error_category": error_classification.error_category,
                        "severity": error_classification.severity,
                        "investigation_priority": error_classification.investigation_priority,
                        "retention_class": error_classification.retention_class,
                        "auto_retry_eligible": error_classification.error_category == "value_out_of_range" || error_classification.error_category == "format_violation"
                      }
                      
                      meta validation_status = "failed"

      # Data quality assessment and enrichment
      - mapping: |
          root = this
          root.data_quality = {
            "validation_passed": meta("validation_status") == "passed",
            "message_id": meta("message_id"),
            "processing_node": env("NODE_ID").or("unknown"),
            "assessment_timestamp": now(),
            "quality_metrics": {
              "field_completeness": if this.sensor_id.exists() && this.timestamp.exists() && this.location.exists() && this.readings.exists() && this.metadata.exists() { 100 } else { 0 },
              "data_freshness_seconds": if this.timestamp.exists() && meta("validation_status") == "passed" { 
                                         (now() - this.timestamp.parse_timestamp("%Y-%m-%dT%H:%M:%S%z")).seconds() 
                                       } else { -1 }
            }
          }

  # Sophisticated output routing with multiple destinations
  output:
    broker:
      pattern: fan_out
      outputs:
        # Analytics path: High-throughput processing for valid data
        - label: analytics_pipeline
          switch:
            - check: 'meta("validation_status") == "passed"'
              output:
                broker:
                  pattern: greedy
                  outputs:
                    # Primary analytics endpoint
                    - http_client:
                        url: "${ANALYTICS_ENDPOINT:http://localhost:9000}/ingest"
                        verb: POST
                        headers:
                          Content-Type: application/json
                          X-Message-ID: "${!json(\"message_id\")}"
                        batching:
                          count: 100
                          period: 5s
                        max_retries: 3
                        backoff:
                          initial_interval: 1s
                          max_interval: 30s

                    # Fallback analytics endpoint
                    - http_client:
                        url: "${ANALYTICS_FALLBACK_ENDPOINT:http://localhost:9002}/ingest"
                        verb: POST
                        headers:
                          Content-Type: application/json
                        max_retries: 1

            - output:
                drop: {}

        # DLQ path: Intelligent routing based on error classification
        - label: dead_letter_queue
          switch:
            - check: 'meta("validation_status") == "failed" || meta("validation_status") == "parse_failed"'
              output:
                switch:
                  # High-priority errors: Immediate DLQ + alert
                  - check: 'this.dlq_metadata.investigation_priority == "urgent"'
                    output:
                      broker:
                        pattern: fan_out
                        outputs:
                          # Write to high-priority DLQ
                          - file:
                              path: "/var/log/expanso/dlq/urgent/${!timestamp_unix_date('2006/01/02')}/urgent-failures-${!timestamp_unix()}.jsonl"
                              codec: lines
                              batching:
                                count: 1  # Immediate write
                                period: 1s

                          # Send alert for urgent issues
                          - http_client:
                              url: "${ALERT_ENDPOINT:http://localhost:9003}/urgent-validation-failure"
                              verb: POST
                              headers:
                                Content-Type: application/json
                              max_retries: 2

                  # Medium-priority errors: Standard DLQ
                  - check: 'this.dlq_metadata.investigation_priority == "high"'
                    output:
                      file:
                        path: "/var/log/expanso/dlq/high/${!timestamp_unix_date('2006/01/02')}/high-priority-failures-${!timestamp_unix()}.jsonl"
                        codec: lines
                        batching:
                          count: 10
                          period: 10s

                  # Low-priority errors: Batched DLQ
                  - output:
                      file:
                        path: "/var/log/expanso/dlq/normal/${!timestamp_unix_date('2006/01/02')}/validation-failures-${!timestamp_unix()}.jsonl"
                        codec: lines
                        batching:
                          count: 50
                          period: 60s

            - output:
                drop: {}

        # Metrics and monitoring: Real-time data quality tracking
        - label: metrics_and_monitoring
          processors:
            - mapping: |
                root = {
                  "message_id": this.message_id,
                  "validation_status": meta("validation_status"),
                  "sensor_id": this.sensor_id.or("unknown"),
                  "node_id": env("NODE_ID").or("unknown"),
                  "processing_time_ms": if meta("validation_status") == "passed" { this.validation.processing_time_ms } else { this.validation.processing_time_ms.or(0) },
                  "error_category": if meta("validation_status") == "failed" { this.validation.error_details.classification.error_category } else { null },
                  "investigation_priority": if meta("validation_status") == "failed" { this.dlq_metadata.investigation_priority } else { null },
                  "data_quality_score": this.data_quality.quality_metrics.field_completeness,
                  "timestamp": now()
                }

          http_client:
            url: "${METRICS_ENDPOINT:http://localhost:9001}/validation-metrics"
            verb: POST
            headers:
              Content-Type: application/json
            batching:
              count: 100
              period: 10s
            max_retries: 0

# Production logging configuration
logger:
  level: INFO
  format: json
  static_fields:
    component: production-dlq-routing
    version: "1.0.0"

# Enhanced Prometheus metrics
metrics:
  prometheus:
    path: /metrics
    push_interval: 10s
  mapping: |
    root = this
    meta pipeline = "production-dlq-routing"
    meta environment = "production"
    meta dlq_management = "enabled"
EOF

# Create production DLQ directory structure
sudo mkdir -p /var/log/expanso/dlq/{urgent,high,normal}
sudo chown -R $USER:$USER /var/log/expanso/dlq

# Deploy production DLQ pipeline
expanso job delete enhanced-dlq-routing
expanso job deploy /tmp/dlq-routing/production-dlq-with-management.yaml

# Wait for deployment
sleep 15

# Verify deployment
expanso job status production-dlq-routing
```

## Step 3.4: Create DLQ Monitoring and Management Tools

Create tools for monitoring and managing the DLQ:

```bash
# Create DLQ monitoring and management scripts
mkdir -p /tmp/dlq-tools

# DLQ statistics script
cat > /tmp/dlq-tools/dlq-stats.sh << 'EOF'
#!/bin/bash

DLQ_BASE_DIR="/var/log/expanso/dlq"

echo "=== DLQ Statistics Report ==="
echo "Generated: $(date)"
echo ""

# Overall statistics
echo "### Overall DLQ Statistics ###"
total_files=$(find "$DLQ_BASE_DIR" -name "*.jsonl" -type f | wc -l)
total_entries=$(find "$DLQ_BASE_DIR" -name "*.jsonl" -type f -exec wc -l {} \; | awk '{sum+=$1} END {print sum}')
total_size=$(find "$DLQ_BASE_DIR" -name "*.jsonl" -type f -exec ls -l {} \; | awk '{sum+=$5} END {printf "%.2f MB", sum/1024/1024}')

echo "Total DLQ files: $total_files"
echo "Total DLQ entries: $total_entries"
echo "Total DLQ size: $total_size"
echo ""

# Priority-based statistics
for priority in urgent high normal; do
  if [ -d "$DLQ_BASE_DIR/$priority" ]; then
    echo "### $priority Priority DLQ ###"
    priority_files=$(find "$DLQ_BASE_DIR/$priority" -name "*.jsonl" -type f | wc -l)
    priority_entries=$(find "$DLQ_BASE_DIR/$priority" -name "*.jsonl" -type f -exec wc -l {} \; 2>/dev/null | awk '{sum+=$1} END {print sum+0}')
    echo "$priority files: $priority_files"
    echo "$priority entries: $priority_entries"
    
    # Recent entries (last 24 hours)
    recent_entries=$(find "$DLQ_BASE_DIR/$priority" -name "*.jsonl" -type f -mtime -1 -exec wc -l {} \; 2>/dev/null | awk '{sum+=$1} END {print sum+0}')
    echo "$priority recent entries (24h): $recent_entries"
    echo ""
  fi
done

# Error category analysis
echo "### Error Category Analysis ###"
if [ -n "$(find "$DLQ_BASE_DIR" -name "*.jsonl" -type f)" ]; then
  echo "Top error categories:"
  find "$DLQ_BASE_DIR" -name "*.jsonl" -type f -exec cat {} \; 2>/dev/null | \
    jq -r '.dlq_metadata.error_category // .validation.error_analysis.error_category // "unknown"' 2>/dev/null | \
    sort | uniq -c | sort -rn | head -10 | \
    while read count category; do
      echo "  $category: $count"
    done
  echo ""
fi

# Sensor analysis
echo "### Top Problematic Sensors ###"
if [ -n "$(find "$DLQ_BASE_DIR" -name "*.jsonl" -type f)" ]; then
  find "$DLQ_BASE_DIR" -name "*.jsonl" -type f -exec cat {} \; 2>/dev/null | \
    jq -r '.sensor_id // "unknown"' 2>/dev/null | \
    sort | uniq -c | sort -rn | head -10 | \
    while read count sensor; do
      echo "  $sensor: $count failures"
    done
fi

echo ""
echo "=== End DLQ Statistics Report ==="
EOF

# DLQ cleanup script with retention policies
cat > /tmp/dlq-tools/dlq-cleanup.sh << 'EOF'
#!/bin/bash

DLQ_BASE_DIR="/var/log/expanso/dlq"
ARCHIVE_DIR="/var/log/expanso/dlq-archive"

# Retention policies (in days)
URGENT_RETENTION=90
HIGH_RETENTION=30
NORMAL_RETENTION=7

echo "=== DLQ Cleanup Starting ==="
echo "Cleanup started: $(date)"

# Create archive directory
mkdir -p "$ARCHIVE_DIR"/{urgent,high,normal}

cleanup_priority() {
  local priority=$1
  local retention_days=$2
  local priority_dir="$DLQ_BASE_DIR/$priority"
  local archive_dir="$ARCHIVE_DIR/$priority"
  
  if [ ! -d "$priority_dir" ]; then
    echo "Warning: $priority_dir does not exist"
    return
  fi
  
  echo "Cleaning $priority priority DLQ (retention: ${retention_days} days)"
  
  # Find files older than retention period
  old_files=$(find "$priority_dir" -name "*.jsonl" -type f -mtime +$retention_days)
  
  if [ -n "$old_files" ]; then
    file_count=$(echo "$old_files" | wc -l)
    echo "  Found $file_count old files to process"
    
    # Archive files before deletion
    echo "$old_files" | while read file; do
      if [ -f "$file" ]; then
        # Create archive path maintaining directory structure
        relative_path=${file#$priority_dir/}
        archive_path="$archive_dir/$relative_path"
        archive_dir_only=$(dirname "$archive_path")
        
        # Create archive directory
        mkdir -p "$archive_dir_only"
        
        # Compress and move to archive
        gzip -c "$file" > "${archive_path}.gz"
        
        # Remove original file
        rm "$file"
        
        echo "    Archived: $file -> ${archive_path}.gz"
      fi
    done
    
    # Remove empty directories
    find "$priority_dir" -type d -empty -delete 2>/dev/null
    
    echo "  Completed $priority cleanup"
  else
    echo "  No old files found for $priority"
  fi
}

# Clean each priority level
cleanup_priority "urgent" $URGENT_RETENTION
cleanup_priority "high" $HIGH_RETENTION  
cleanup_priority "normal" $NORMAL_RETENTION

# Clean up old archives (keep archives for 1 year)
echo "Cleaning old archives (older than 365 days)"
old_archives=$(find "$ARCHIVE_DIR" -name "*.gz" -type f -mtime +365)
if [ -n "$old_archives" ]; then
  archive_count=$(echo "$old_archives" | wc -l)
  echo "  Removing $archive_count old archive files"
  echo "$old_archives" | xargs rm -f
else
  echo "  No old archives to remove"
fi

echo "=== DLQ Cleanup Completed ==="
echo "Cleanup finished: $(date)"
EOF

# DLQ investigation script
cat > /tmp/dlq-tools/dlq-investigate.sh << 'EOF'
#!/bin/bash

DLQ_BASE_DIR="/var/log/expanso/dlq"

show_usage() {
  echo "Usage: $0 [options]"
  echo "Options:"
  echo "  --sensor <sensor_id>     Show failures for specific sensor"
  echo "  --category <category>    Show failures for specific error category"
  echo "  --priority <priority>    Show failures for specific priority (urgent|high|normal)"
  echo "  --recent <hours>         Show failures from last N hours (default: 24)"
  echo "  --count                  Show summary counts only"
  echo "  --help                   Show this help"
  echo ""
  echo "Examples:"
  echo "  $0 --sensor sensor-42"
  echo "  $0 --category missing_required_field --recent 6"
  echo "  $0 --priority urgent"
}

# Default values
SENSOR=""
CATEGORY=""
PRIORITY=""
RECENT_HOURS=24
COUNT_ONLY=false

# Parse arguments
while [[ $# -gt 0 ]]; do
  case $1 in
    --sensor)
      SENSOR="$2"
      shift 2
      ;;
    --category)
      CATEGORY="$2"
      shift 2
      ;;
    --priority)
      PRIORITY="$2"
      shift 2
      ;;
    --recent)
      RECENT_HOURS="$2"
      shift 2
      ;;
    --count)
      COUNT_ONLY=true
      shift
      ;;
    --help)
      show_usage
      exit 0
      ;;
    *)
      echo "Unknown option: $1"
      show_usage
      exit 1
      ;;
  esac
done

echo "=== DLQ Investigation Report ==="
echo "Generated: $(date)"
echo "Filter parameters:"
[ -n "$SENSOR" ] && echo "  Sensor: $SENSOR"
[ -n "$CATEGORY" ] && echo "  Category: $CATEGORY"
[ -n "$PRIORITY" ] && echo "  Priority: $PRIORITY"
echo "  Recent hours: $RECENT_HOURS"
echo ""

# Find relevant files
if [ -n "$PRIORITY" ]; then
  search_dir="$DLQ_BASE_DIR/$PRIORITY"
else
  search_dir="$DLQ_BASE_DIR"
fi

# Find files from recent hours
recent_files=$(find "$search_dir" -name "*.jsonl" -type f -mmin -$((RECENT_HOURS * 60)))

if [ -z "$recent_files" ]; then
  echo "No DLQ files found matching criteria"
  exit 0
fi

# Process files and apply filters
process_entries() {
  local jq_filter='.'
  
  # Apply sensor filter
  if [ -n "$SENSOR" ]; then
    jq_filter="$jq_filter | select(.sensor_id == \"$SENSOR\")"
  fi
  
  # Apply category filter
  if [ -n "$CATEGORY" ]; then
    jq_filter="$jq_filter | select(.dlq_metadata.error_category == \"$CATEGORY\" or .validation.error_analysis.error_category == \"$CATEGORY\")"
  fi
  
  echo "$recent_files" | xargs cat 2>/dev/null | jq -r "$jq_filter"
}

if [ "$COUNT_ONLY" = true ]; then
  echo "### Summary Counts ###"
  total_count=$(process_entries | jq -s length)
  echo "Total matching entries: $total_count"
  
  if [ $total_count -gt 0 ]; then
    echo ""
    echo "Breakdown by error category:"
    process_entries | jq -r '.dlq_metadata.error_category // .validation.error_analysis.error_category // "unknown"' | \
      sort | uniq -c | sort -rn | while read count cat; do
        echo "  $cat: $count"
      done
  fi
else
  echo "### Detailed Entries ###"
  process_entries | jq '{
    message_id: .message_id,
    sensor_id: .sensor_id,
    dlq_entry_time: .dlq_metadata.dlq_entry_time,
    error_category: (.dlq_metadata.error_category // .validation.error_analysis.error_category // "unknown"),
    priority: .dlq_metadata.investigation_priority,
    error_message: .validation.error_details.primary_error,
    debugging_hints: .validation.debugging_hints
  }' | head -20
fi

echo ""
echo "=== End Investigation Report ==="
EOF

# Make scripts executable
chmod +x /tmp/dlq-tools/*.sh

echo "DLQ management tools created:"
echo "  /tmp/dlq-tools/dlq-stats.sh - Generate DLQ statistics"
echo "  /tmp/dlq-tools/dlq-cleanup.sh - Clean up old DLQ files with retention"
echo "  /tmp/dlq-tools/dlq-investigate.sh - Investigate specific DLQ failures"
```

### Test DLQ Management Tools

```bash
echo "=== Testing DLQ Management Tools ==="

# Generate test DLQ data first
echo "Generating test DLQ data..."
for i in {1..5}; do
  curl -X POST http://localhost:8080/sensor/readings \
    -H "Content-Type: application/json" \
    -d "{\"sensor_id\": \"test-sensor-$i\", \"invalid\": \"data\"}" \
    >/dev/null 2>&1
done

# Wait for DLQ processing
sleep 5

# Test DLQ statistics
echo "1. Testing DLQ statistics:"
/tmp/dlq-tools/dlq-stats.sh

echo ""
echo "2. Testing DLQ investigation (recent failures):"
/tmp/dlq-tools/dlq-investigate.sh --count --recent 1

echo ""
echo "3. Testing DLQ investigation (specific sensor):"
/tmp/dlq-tools/dlq-investigate.sh --sensor test-sensor-1 --count

echo ""
echo "4. Testing DLQ cleanup (dry run - files are recent):"
echo "Note: Files are too recent for cleanup, but script structure verified"
/tmp/dlq-tools/dlq-cleanup.sh | head -10
```

## Step 3.5: Set Up DLQ Alerting and Monitoring

Create alerting rules for DLQ monitoring:

```bash
# Create DLQ alerting configuration
mkdir -p /tmp/dlq-monitoring

cat > /tmp/dlq-monitoring/dlq-alerts.yaml << 'EOF'
# Prometheus alerting rules for DLQ monitoring
groups:
  - name: dlq_validation_alerts
    interval: 30s
    rules:
      # Alert on high validation failure rate
      - alert: HighValidationFailureRate
        expr: |
          sum(rate(pipeline_output_sent_total{output="dead_letter_queue"}[5m])) /
          sum(rate(pipeline_input_received_total[5m])) > 0.1
        for: 2m
        labels:
          severity: warning
          component: schema-validation
        annotations:
          summary: "High schema validation failure rate detected"
          description: "Validation failure rate is {{ $value | humanizePercentage }} over the last 5 minutes"

      # Alert on urgent DLQ entries
      - alert: UrgentDLQEntries
        expr: |
          increase(dlq_urgent_entries_total[5m]) > 0
        for: 0m  # Immediate alert
        labels:
          severity: critical
          component: schema-validation
        annotations:
          summary: "Urgent validation failures detected"
          description: "{{ $value }} urgent validation failures in the last 5 minutes"

      # Alert on DLQ disk space usage
      - alert: DLQDiskSpaceHigh
        expr: |
          (dlq_disk_usage_bytes / dlq_disk_total_bytes) > 0.8
        for: 5m
        labels:
          severity: warning
          component: dlq-management
        annotations:
          summary: "DLQ disk space usage high"
          description: "DLQ disk usage is {{ $value | humanizePercentage }} of available space"

      # Alert on DLQ file count growth
      - alert: DLQFileCountGrowth
        expr: |
          increase(dlq_file_count_total[1h]) > 100
        for: 0m
        labels:
          severity: warning
          component: dlq-management
        annotations:
          summary: "Rapid DLQ file growth"
          description: "{{ $value }} new DLQ files created in the last hour"

      # Alert on specific sensor failures
      - alert: SensorValidationFailures
        expr: |
          sum by (sensor_id) (increase(dlq_entries_by_sensor_total[10m])) > 10
        for: 5m
        labels:
          severity: warning
          component: data-quality
        annotations:
          summary: "High failure rate for sensor {{ $labels.sensor_id }}"
          description: "Sensor {{ $labels.sensor_id }} has {{ $value }} validation failures in 10 minutes"
EOF

# Create DLQ monitoring dashboard configuration
cat > /tmp/dlq-monitoring/dlq-dashboard.json << 'EOF'
{
  "dashboard": {
    "title": "Schema Validation & DLQ Monitoring",
    "tags": ["schema-validation", "dlq", "data-quality"],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "panels": [
      {
        "title": "Validation Success Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(rate(pipeline_output_sent_total{output=\"analytics\"}[5m])) / sum(rate(pipeline_input_received_total[5m]))",
            "legendFormat": "Success Rate"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit",
            "min": 0,
            "max": 1,
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 0.95},
                {"color": "green", "value": 0.99}
              ]
            }
          }
        }
      },
      {
        "title": "DLQ Entry Rate by Priority",
        "type": "graph",
        "targets": [
          {
            "expr": "sum by (priority) (rate(dlq_entries_by_priority_total[5m]))",
            "legendFormat": "{{ priority }}"
          }
        ]
      },
      {
        "title": "Error Categories",
        "type": "piechart",
        "targets": [
          {
            "expr": "sum by (error_category) (dlq_entries_by_category_total)",
            "legendFormat": "{{ error_category }}"
          }
        ]
      },
      {
        "title": "Processing Time Distribution",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(validation_processing_time_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(validation_processing_time_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          }
        ]
      }
    ]
  }
}
EOF

echo "DLQ monitoring configuration created:"
echo "  /tmp/dlq-monitoring/dlq-alerts.yaml - Prometheus alerting rules"
echo "  /tmp/dlq-monitoring/dlq-dashboard.json - Grafana dashboard config"
```

## Troubleshooting DLQ Routing

### Issue: Valid Messages Going to DLQ

```bash
# Debug routing logic
echo "Debugging DLQ routing logic..."

# Check validation metadata
curl -X POST http://localhost:8080/sensor/readings \
  -H "Content-Type: application/json" \
  -d '{
    "sensor_id": "debug-sensor-1",
    "timestamp": "2025-10-20T14:30:00Z",
    "location": {"building": "warehouse-3", "floor": 2, "zone": "A"},
    "readings": {"temperature_celsius": 23.5, "humidity_percent": 45.2},
    "metadata": {"firmware_version": "2.1.0", "battery_percent": 87}
  }' &

# Check pipeline logs for routing decisions
sleep 2
expanso job logs production-dlq-routing --tail 10 | grep -E "(validation_status|routing)"
```

### Issue: DLQ Files Growing Too Large

```bash
# Monitor DLQ file sizes
watch -n 10 'du -sh /var/log/expanso/dlq/*/* 2>/dev/null | sort -hr | head -5'

# Implement DLQ file rotation
cat > /tmp/dlq-tools/dlq-rotate.sh << 'EOF'
#!/bin/bash

MAX_FILE_SIZE_MB=100
DLQ_BASE_DIR="/var/log/expanso/dlq"

find "$DLQ_BASE_DIR" -name "*.jsonl" -type f -size +${MAX_FILE_SIZE_MB}M | while read file; do
  echo "Rotating large DLQ file: $file"
  
  # Create rotated filename
  timestamp=$(date +%Y%m%d-%H%M%S)
  rotated="${file%.jsonl}-${timestamp}.jsonl.gz"
  
  # Compress and rename
  gzip -c "$file" > "$rotated"
  rm "$file"
  
  echo "Rotated to: $rotated"
done
EOF

chmod +x /tmp/dlq-tools/dlq-rotate.sh
```

### Issue: High Memory Usage from DLQ Processing

```bash
# Monitor memory usage
free -h

# Optimize DLQ pipeline for memory efficiency
cat > /tmp/dlq-optimization.yaml << 'EOF'
# Memory-optimized DLQ configuration
config:
  pipeline:
    processors:
      # Limit message size to prevent memory issues
      - switch:
          - check: 'content().bytes() > 1000000'  # 1MB limit
            processors:
              - mapping: |
                  root = {
                    "error": "message_too_large",
                    "size_bytes": content().bytes(),
                    "truncated_content": content().bytes(0, 1000)
                  }
                  meta validation_status = "rejected_size"
          
          # Process normal-sized messages
          - processors:
              - json_schema:
                  schema_path: "file:///etc/expanso/schemas/sensor-schema-v1.0.0.json"

  output:
    # Smaller batching to reduce memory buffer size
    broker:
      pattern: fan_out
      outputs:
        - label: dlq
          file:
            path: "/var/log/expanso/dlq/validation-failures-${!timestamp_unix()}.jsonl"
            codec: lines
            batching:
              count: 10      # Smaller batches
              period: 10s    # More frequent flushes
              byte_size: 1MB # Size-based batching
EOF
```

## Summary

You've successfully implemented comprehensive DLQ routing that:

✅ **Routes based on validation status** with intelligent switch logic
✅ **Enriches error context** with debugging information and error classification  
✅ **Implements priority-based routing** (urgent, high, normal)
✅ **Provides DLQ management tools** for statistics, cleanup, and investigation
✅ **Creates monitoring and alerting** for DLQ health and trends
✅ **Handles production scenarios** with retention policies and file rotation

**DLQ routing features implemented:**
- Validation metadata-based routing decisions
- Multi-destination fan-out (analytics + DLQ + metrics)
- Error classification and priority assignment
- Date-partitioned DLQ storage for easier management
- Correlation IDs for message tracking across systems
- Batching and retry logic for reliable delivery

**Management and monitoring capabilities:**
- DLQ statistics and analysis tools
- Automated cleanup with retention policies
- Investigation tools for debugging specific failures
- Alerting rules for DLQ health monitoring
- Performance optimization for high-throughput scenarios

**Next step:** [Monitor Data Quality Metrics](./step-4-monitor-quality-metrics) to implement comprehensive monitoring and alerting for the complete validation system.

## Reference Files Created

- `/tmp/dlq-routing/basic-dlq-routing.yaml` - Simple DLQ routing
- `/tmp/dlq-routing/enhanced-dlq-routing.yaml` - Enhanced routing with error context
- `/tmp/dlq-routing/production-dlq-with-management.yaml` - Production DLQ system
- `/tmp/dlq-tools/dlq-stats.sh` - DLQ statistics and analysis
- `/tmp/dlq-tools/dlq-cleanup.sh` - Automated cleanup with retention
- `/tmp/dlq-tools/dlq-investigate.sh` - Investigation and debugging tool
- `/tmp/dlq-monitoring/dlq-alerts.yaml` - Prometheus alerting rules
- `/tmp/dlq-monitoring/dlq-dashboard.json` - Grafana dashboard configuration

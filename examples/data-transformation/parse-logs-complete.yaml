#
# Complete Multi-Format Log Parser
# Production-ready pipeline for parsing JSON, CSV, access logs, syslog, and custom formats
#
# Features:
# - Automatic format detection with confidence scoring
# - Format-specific processing pipelines  
# - Privacy-compliant data anonymization
# - Real-time monitoring and alerting
# - Error handling with dead letter queues
# - Horizontal scaling support
#

name: complete-log-parser
description: Production-ready multi-format log parser with monitoring and alerting
type: pipeline
namespace: default

config:
  #
  # INPUT CONFIGURATION
  # Supports multiple log sources: files, syslog, HTTP endpoints
  #
  input:
    broker:
      pattern: fan_in
      inputs:
        # File-based log collection
        - file:
            paths:
              - "${LOG_PATH}/app/*.jsonl"      # JSON application logs
              - "${LOG_PATH}/sensors/*.csv"    # CSV sensor data
              - "${LOG_PATH}/nginx/*.log"      # Nginx access logs
              - "${LOG_PATH}/apache/*.log"     # Apache access logs
              - "${LOG_PATH}/system/*.log"     # System logs
            codec: lines
            scanner:
              lines:
                max_buffer_size: 1048576  # 1MB max line size
            metadata:
              source: "file_input"

        # Real-time syslog ingestion (UDP)
        - socket_server:
            network: udp
            address: "0.0.0.0:514"
            metadata:
              source: "syslog_udp"
            
        # Secure syslog over TCP with TLS
        - socket_server:
            network: tcp
            address: "0.0.0.0:6514"
            tls:
              enabled: true
              cert_file: "${TLS_CERT_PATH}"
              key_file: "${TLS_KEY_PATH}"
            metadata:
              source: "syslog_tls"

        # HTTP log ingestion endpoint
        - http_server:
            address: "0.0.0.0:8080"
            path: "/logs/ingest"
            allowed_verbs: ["POST"]
            metadata:
              source: "http_api"

  #
  # PROCESSING PIPELINE
  # Multi-stage processing with format detection and routing
  #
  pipeline:
    processors:
      #
      # STAGE 1: Rate Limiting and Input Validation
      #
      - rate_limit:
          count: 10000
          per: "1s"
          
      - mapping: |
          # Filter out empty or oversized logs
          root = if this.string().length() == 0 || this.string().length() > 1000000 {
            deleted()
          } else {
            this
          }

      #
      # STAGE 2: Advanced Format Detection
      # Machine learning-like scoring system for format identification
      #
      - mapping: |
          root = this
          let content = this.string()
          let len = content.length()
          
          # Character frequency analysis for pattern recognition
          let char_stats = {
            "commas": content.count(","),
            "braces": content.count("{") + content.count("}"),
            "brackets": content.count("[") + content.count("]"),
            "quotes": content.count('"'),
            "angles": content.count("<") + content.count(">"),
            "colons": content.count(":"),
            "spaces": content.count(" "),
            "digits": content.re_find_all("\\d").length()
          }
          
          # JSON format scoring with weighted indicators
          let json_indicators = [
            content.has_prefix("{") || content.has_prefix("["),    # Strong indicator: 40 points
            content.contains('"') && content.contains(":"),        # Structure: 25 points
            char_stats.braces > 0 && char_stats.quotes >= char_stats.braces,  # Balance: 20 points
            content.contains("timestamp") || content.contains("@timestamp"),   # Timestamp: 10 points
            content.contains("level") || content.contains("severity")          # Log level: 5 points
          ]
          
          # CSV format scoring
          let csv_indicators = [
            char_stats.commas >= 2 && char_stats.braces == 0,     # Structure: 35 points
            content.re_match("^[^,]+,[^,]+,[^,]+"),               # Pattern: 25 points
            char_stats.commas > char_stats.quotes / 2,            # Delimiter ratio: 20 points
            !content.contains("{") && !content.contains("<"),     # No other formats: 15 points
            len < 500 && char_stats.spaces < char_stats.commas    # Compactness: 5 points
          ]
          
          # Access log scoring with web server patterns
          let access_log_indicators = [
            content.re_match("\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}"),    # IP address: 30 points
            content.contains(" - ") && content.contains("[") && content.contains("]"),  # Structure: 25 points
            content.contains('"') && content.re_match("\\s\\d{3}\\s"),        # Status codes: 25 points
            content.contains("HTTP/") && content.re_match("GET|POST|PUT"),    # HTTP methods: 15 points
            content.re_match("\\[\\d{2}/\\w{3}/\\d{4}:")                      # Timestamp: 5 points
          ]
          
          # Syslog scoring with RFC3164/5424 patterns
          let syslog_indicators = [
            content.re_match("^<\\d+>"),                          # Priority: 40 points
            content.re_match("\\w{3}\\s+\\d{1,2}\\s+\\d{2}:\\d{2}:\\d{2}"),  # Timestamp: 30 points
            content.contains(":") && char_stats.angles > 0,       # Structure: 20 points
            len > 50 && len < 1000,                              # Reasonable length: 5 points
            content.re_match("kernel|sshd|nginx|systemd")         # Common services: 5 points
          ]
          
          # Calculate weighted scores (0.0 - 1.0)
          let json_score = json_indicators.enumerate().map_each(item -> 
            if item.value { [40, 25, 20, 10, 5].index(item.index) } else { 0 }
          ).fold(0, acc -> acc + this) / 100.0
          
          let csv_score = csv_indicators.enumerate().map_each(item ->
            if item.value { [35, 25, 20, 15, 5].index(item.index) } else { 0 }  
          ).fold(0, acc -> acc + this) / 100.0
          
          let access_score = access_log_indicators.enumerate().map_each(item ->
            if item.value { [30, 25, 25, 15, 5].index(item.index) } else { 0 }
          ).fold(0, acc -> acc + this) / 100.0
          
          let syslog_score = syslog_indicators.enumerate().map_each(item ->
            if item.value { [40, 30, 20, 5, 5].index(item.index) } else { 0 }
          ).fold(0, acc -> acc + this) / 100.0
          
          # Determine best format match with confidence levels
          let scores = [
            {"format": "json", "score": json_score},
            {"format": "csv", "score": csv_score},
            {"format": "access_log", "score": access_score},
            {"format": "syslog", "score": syslog_score}
          ].sort_by("score").reverse()
          
          let detected = scores.index(0)
          
          root.detection_result = {
            "format": if detected.score >= 0.6 {
              detected.format       # High confidence
            } else if detected.score >= 0.3 {
              detected.format       # Medium confidence
            } else {
              "unknown"            # Low confidence
            },
            "confidence": if detected.score >= 0.8 {
              "high"
            } else if detected.score >= 0.5 {
              "medium"
            } else if detected.score >= 0.3 {
              "low"
            } else {
              "very_low"
            },
            "score": detected.score,
            "all_scores": scores,
            "char_analysis": char_stats
          }
          
          root.original_content = content
          root.processing_start = now().ts_unix()
          
          # Set routing metadata for downstream processors
          meta log_format = root.detection_result.format
          meta confidence = root.detection_result.confidence

      #
      # STAGE 3: Format-Specific Processing
      # Route to specialized processors based on detected format
      #
      - switch:
          #
          # JSON Processing Pipeline
          # Handles application logs, microservice logs, API logs
          #
          - check: meta("log_format") == "json"
            processors:
              # Error handling for malformed JSON
              - catch:
                  - mapping: |
                      root.json_parse_error = {
                        "error": error(),
                        "timestamp": now().ts_unix(),
                        "content_preview": this.original_content.slice(0, 200)
                      }
                      
              # Parse JSON documents
              - json_documents:
                  parts: []
                  
              # Post-processing and normalization
              - mapping: |
                  root = if this.exists("json_parse_error") {
                    this  # Keep error information
                  } else {
                    this.without("original_content")  # Remove raw content after successful parsing
                  }
                  
                  # Multi-format timestamp normalization
                  let raw_ts = this.timestamp.or(this."@timestamp").or(this.time).or(this.created_at)
                  root.timestamp_unix = raw_ts.parse_timestamp("2006-01-02T15:04:05.999Z07:00").catch(
                    raw_ts.parse_timestamp("2006-01-02T15:04:05Z").catch(
                      raw_ts.parse_timestamp("2006-01-02 15:04:05").catch(
                        raw_ts.number().catch(now().ts_unix())  # Unix timestamp or current time
                      )
                    )
                  ).ts_unix()
                  
                  # Standardize log level field
                  root.level = this.level.or(this.severity).or("INFO").uppercase().
                              replace("WARN", "WARNING").replace("ERR", "ERROR")
                  
                  # Extract service/application identifier
                  root.service = this.service.or(this.logger).or(this.app).or(this.component).or("unknown")
                  
                  # Standardize message field
                  root.message = this.message.or(this.msg).or(this.log).or(this.content).or("No message")
                  
                  # Add JSON-specific metadata
                  root.parsing_metadata = {
                    "processor": "json_parser",
                    "success": !this.exists("json_parse_error"),
                    "fields_extracted": this.keys().filter(key -> !key.has_prefix("parsing_")).length(),
                    "has_structured_data": this.keys().length() > 5,
                    "nested_levels": this.walk(value -> if value.type() == "object" { 1 } else { 0 }).fold(0, acc -> acc + this)
                  }

          #
          # CSV Processing Pipeline
          # Handles sensor data, exports, tabular logs
          #
          - check: meta("log_format") == "csv"
            processors:
              # Flexible column parsing (up to 10 columns)
              - csv:
                  columns: ["col1", "col2", "col3", "col4", "col5", "col6", "col7", "col8", "col9", "col10"]
                  skip_header_rows: 0
                  delimiter: ","
                  quote: '"'
                  escape: '"'
                  
              # Intelligent column mapping based on content
              - mapping: |
                  root = this
                  
                  # Analyze content to determine mapping strategy
                  let mapping_confidence = "high"
                  let mapped_data = if this.col1.re_match("\\d{4}-\\d{2}-\\d{2}") && this.col2.re_match("temperature|humidity|pressure|flow") {
                    # IoT sensor data format
                    {
                      "timestamp": this.col1,
                      "metric_name": this.col2, 
                      "sensor_id": this.col3,
                      "value": this.col4,
                      "unit": this.col5,
                      "location": this.col6,
                      "data_type": "sensor",
                      "schema_version": "iot_v1"
                    }
                  } else if this.col1.re_match("\\d{4}-\\d{2}-\\d{2}") && this.col2.re_match("[a-zA-Z]") {
                    # Generic time-series data
                    {
                      "timestamp": this.col1,
                      "series_name": this.col2,
                      "value": this.col3,
                      "category": this.col4,
                      "metadata": this.col5,
                      "data_type": "timeseries",
                      "schema_version": "ts_v1"
                    }
                  } else if this.col1.re_match("^[a-zA-Z0-9_-]+$") && this.col2.re_match("\\d") {
                    # Key-value export format
                    {
                      "identifier": this.col1,
                      "value": this.col2,
                      "category": this.col3,
                      "data_type": "export",
                      "schema_version": "export_v1"
                    }
                  } else {
                    # Generic CSV with fallback mapping
                    let mapping_confidence = "low"
                    let non_empty_cols = [this.col1, this.col2, this.col3, this.col4, this.col5, 
                                         this.col6, this.col7, this.col8, this.col9, this.col10].
                                         filter(col -> col != null && col != "")
                    {
                      "field_1": this.col1,
                      "field_2": this.col2,
                      "field_3": this.col3,
                      "field_4": this.col4,
                      "field_5": this.col5,
                      "column_count": non_empty_cols.length(),
                      "data_type": "generic",
                      "schema_version": "generic_v1"
                    }
                  }
                  
                  root = mapped_data
                  
                  # Timestamp processing with multiple format support
                  root.timestamp_unix = this.timestamp.parse_timestamp("2006-01-02T15:04:05Z").catch(
                    this.timestamp.parse_timestamp("2006-01-02").catch(
                      this.timestamp.parse_timestamp("2006-01-02 15:04:05").catch(
                        now().ts_unix()
                      )
                    )
                  ).ts_unix()
                  
                  # Type conversion for numeric values with validation
                  root.value_numeric = this.value.number().catch(
                    this.value.replace(",", "").number().catch(0.0)  # Handle comma decimal separators
                  )
                  
                  # Data quality assessment
                  root.data_quality = {
                    "completeness": (this.keys().filter(key -> this.get(key) != "").length() / this.keys().length()),
                    "numeric_valid": this.value_numeric != 0.0 || this.value == "0",
                    "timestamp_valid": this.timestamp_unix > 1577836800,  # After 2020-01-01
                    "schema_confidence": mapping_confidence
                  }
                  
                  # CSV-specific metadata
                  root.parsing_metadata = {
                    "processor": "csv_parser",
                    "success": true,
                    "detected_columns": this.keys().filter(key -> key.has_prefix("col") || key.has_prefix("field")).length(),
                    "mapping_type": this.data_type,
                    "mapping_confidence": mapping_confidence,
                    "schema_version": this.schema_version
                  }

          #
          # Access Log Processing Pipeline
          # Handles Apache/Nginx access logs with multiple format variants
          #
          - check: meta("log_format") == "access_log"
            processors:
              # Multiple grok patterns for different access log formats
              - grok:
                  expressions:
                    # Combined Log Format (most common) with optional response time
                    - '%{IPORHOST:client_ip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] "(?:%{WORD:method} %{NOTSPACE:request}(?: HTTP/%{NUMBER:http_version})?|%{DATA:invalid_request})" %{NUMBER:status_code} (?:%{NUMBER:bytes}|-) "(?:%{DATA:referer}|-)" "(?:%{DATA:user_agent}|-)"(?:\\s+%{NUMBER:response_time})?'
                    # Common Log Format (basic)
                    - '%{IPORHOST:client_ip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] "(?:%{WORD:method} %{NOTSPACE:request}(?: HTTP/%{NUMBER:http_version})?|%{DATA:invalid_request})" %{NUMBER:status_code} (?:%{NUMBER:bytes}|-)'
                    # Custom format with response time first
                    - '%{IPORHOST:client_ip} - %{USER:auth} \\[%{HTTPDATE:timestamp}\\] "(?:%{WORD:method} %{NOTSPACE:request}(?: HTTP/%{NUMBER:http_version})?)" %{NUMBER:status_code} %{NUMBER:bytes} %{NUMBER:response_time} "(?:%{DATA:referer}|-)" "(?:%{DATA:user_agent}|-)"'
                    # Nginx custom format
                    - '%{IPORHOST:client_ip} - %{USER:remote_user} \\[%{HTTPDATE:timestamp}\\] "(?:%{WORD:method} %{NOTSPACE:request} HTTP/%{NUMBER:http_version})" %{NUMBER:status_code} %{NUMBER:bytes} "(?:%{DATA:referer}|-)" "(?:%{DATA:user_agent}|-)" "%{DATA:x_forwarded_for}"'
                  named_captures_only: true
                  
              # Post-processing and enrichment
              - mapping: |
                  root = this
                  
                  # Timestamp processing (Apache/Nginx timestamp format)
                  root.timestamp_unix = this.timestamp.parse_timestamp("02/Jan/2006:15:04:05 -0700").ts_unix()
                  
                  # Numeric field conversion with validation
                  root.status_code = this.status_code.number()
                  root.bytes = this.bytes.or("0").replace("-", "0").number()
                  root.response_time = this.response_time.or("0").number()
                  root.http_version = this.http_version.or("1.1")
                  
                  # Request analysis and classification
                  root.request_analysis = {
                    "method": this.method,
                    "path": this.request,
                    "category": if this.request.contains("/api/") {
                      "api"
                    } else if this.request.re_match(".*\\.(css|js|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf)$") {
                      "static_asset"
                    } else if this.request.contains("/admin/") {
                      "admin_interface"
                    } else if this.request.contains("/health") || this.request.contains("/status") {
                      "monitoring"
                    } else if this.request == "/" {
                      "home_page"
                    } else {
                      "content_page"
                    },
                    "file_extension": if this.request.contains(".") {
                      this.request.split(".").index(-1).lowercase()
                    } else {
                      "none"
                    },
                    "query_params": this.request.contains("?"),
                    "path_depth": this.request.split("/").length() - 1
                  }
                  
                  # User agent analysis for client insights
                  let ua = this.user_agent.or("")
                  root.client_analysis = {
                    # Browser detection
                    "browser": if ua.contains("Chrome") && !ua.contains("Edg") && !ua.contains("Opera") {
                      "Chrome"
                    } else if ua.contains("Firefox") {
                      "Firefox" 
                    } else if ua.contains("Safari") && !ua.contains("Chrome") {
                      "Safari"
                    } else if ua.contains("Edg") {
                      "Edge"
                    } else if ua.contains("Opera") {
                      "Opera"
                    } else {
                      "Other"
                    },
                    # Operating system detection
                    "os": if ua.contains("Windows") {
                      "Windows"
                    } else if ua.contains("Mac OS") || ua.contains("macOS") {
                      "macOS"
                    } else if ua.contains("Linux") && !ua.contains("Android") {
                      "Linux"
                    } else if ua.contains("iPhone") || ua.contains("iPad") {
                      "iOS"
                    } else if ua.contains("Android") {
                      "Android"
                    } else {
                      "Other"
                    },
                    # Device classification
                    "device_type": if ua.contains("Mobile") || ua.contains("iPhone") {
                      "mobile"
                    } else if ua.contains("iPad") || ua.contains("Tablet") {
                      "tablet"
                    } else {
                      "desktop"
                    },
                    # Bot detection
                    "is_bot": ua.lowercase().re_match("bot|crawler|spider|scraper") || 
                             ua.contains("curl") || ua.contains("wget") ||
                             this.request.contains("robots.txt"),
                    "bot_type": if ua.contains("Googlebot") {
                      "search_engine"
                    } else if ua.contains("monitoring") {
                      "monitoring"
                    } else if ua.contains("curl|wget") {
                      "script"
                    } else {
                      "unknown"
                    }
                  }
                  
                  # Performance metrics
                  root.performance_metrics = {
                    "response_time_ms": this.response_time * 1000,
                    "performance_category": if this.response_time < 0.1 {
                      "excellent"
                    } else if this.response_time < 0.5 {
                      "good"
                    } else if this.response_time < 2.0 {
                      "acceptable"
                    } else if this.response_time < 5.0 {
                      "slow"
                    } else {
                      "critical"
                    },
                    "bandwidth_kbps": if this.response_time > 0 { (this.bytes / 1024.0) / this.response_time } else { 0 },
                    "size_category": if this.bytes < 1024 {
                      "small"
                    } else if this.bytes < 51200 {
                      "medium"
                    } else if this.bytes < 1048576 {
                      "large"
                    } else {
                      "huge"
                    }
                  }
                  
                  # Status code analysis
                  root.status_analysis = {
                    "category": if this.status_code < 300 {
                      "success"
                    } else if this.status_code < 400 {
                      "redirect"
                    } else if this.status_code < 500 {
                      "client_error"
                    } else {
                      "server_error"
                    },
                    "is_error": this.status_code >= 400,
                    "requires_attention": this.status_code >= 500 || 
                                        (this.status_code == 404 && !this.request.contains("favicon.ico"))
                  }
                  
                  # Privacy protection (GDPR compliance)
                  let ip_salt = env("IP_SALT").or("default_salt_change_in_production")
                  root.client_ip_hash = (this.client_ip + ip_salt).hash("sha256").slice(0, 16)
                  root.ip_geolocation = {
                    "is_internal": this.client_ip.has_prefix("10.") || 
                                  this.client_ip.has_prefix("192.168.") || 
                                  this.client_ip.re_match("^172\\.(1[6-9]|2[0-9]|3[0-1])\\."),
                    "ip_class": if this.client_ip.has_prefix("10.") {
                      "private_class_a"
                    } else if this.client_ip.has_prefix("192.168.") {
                      "private_class_c"
                    } else {
                      "public"
                    }
                  }
                  
                  # Remove original IP address for privacy
                  root = this.without("client_ip")
                  
                  # Access log specific metadata
                  root.parsing_metadata = {
                    "processor": "access_log_parser",
                    "success": this.exists("status_code") && this.status_code > 0,
                    "log_format": if this.exists("user_agent") && this.exists("referer") {
                      "combined"
                    } else if this.exists("response_time") {
                      "custom_with_timing"
                    } else {
                      "common"
                    },
                    "has_response_time": this.response_time > 0,
                    "has_user_agent": this.user_agent != "-" && this.user_agent != ""
                  }

          #
          # Syslog Processing Pipeline  
          # Handles RFC3164 and RFC5424 syslog messages
          #
          - check: meta("log_format") == "syslog"
            processors:
              # Multiple grok patterns for different syslog formats
              - grok:
                  expressions:
                    # RFC3164 with PID
                    - '<%{POSINT:priority}>%{SYSLOGTIMESTAMP:timestamp} %{SYSLOGHOST:hostname} %{DATA:tag}\\[%{POSINT:pid}\\]: %{GREEDYDATA:message}'
                    # RFC3164 without PID
                    - '<%{POSINT:priority}>%{SYSLOGTIMESTAMP:timestamp} %{SYSLOGHOST:hostname} %{DATA:tag}: %{GREEDYDATA:message}'
                    # RFC5424 format
                    - '<%{POSINT:priority}>%{POSINT:version} %{TIMESTAMP_ISO8601:timestamp} %{SYSLOGHOST:hostname} %{DATA:tag} %{DATA:proc_id} %{DATA:msg_id} (?:%{DATA:structured_data}|\\-) %{GREEDYDATA:message}'
                    # Cisco/network device format
                    - '<%{POSINT:priority}>%{SYSLOGTIMESTAMP:timestamp} %{IPORHOST:hostname} %%{DATA:tag}-%{POSINT:severity_code}-%{DATA:mnemonic}: %{GREEDYDATA:message}'
                  named_captures_only: true
                  
              # Priority decomposition and enrichment
              - mapping: |
                  root = this
                  
                  # RFC3164/5424 priority decomposition
                  let pri = this.priority.number()
                  root.facility = (pri / 8).floor()
                  root.severity = pri % 8
                  
                  # Enhanced facility information with categorization
                  root.facility_info = if this.facility == 0 {
                    {"name": "kernel", "category": "system", "criticality": "critical", "description": "Kernel messages"}
                  } else if this.facility == 1 {
                    {"name": "user", "category": "user", "criticality": "low", "description": "User-level messages"}
                  } else if this.facility == 2 {
                    {"name": "mail", "category": "service", "criticality": "medium", "description": "Mail system"}
                  } else if this.facility == 3 {
                    {"name": "daemon", "category": "service", "criticality": "high", "description": "System daemons"}
                  } else if this.facility == 4 {
                    {"name": "security", "category": "security", "criticality": "critical", "description": "Security/authorization messages"}
                  } else if this.facility == 5 {
                    {"name": "syslogd", "category": "logging", "criticality": "medium", "description": "Messages generated by syslogd"}
                  } else if this.facility == 6 {
                    {"name": "line_printer", "category": "service", "criticality": "low", "description": "Line printer subsystem"}
                  } else if this.facility >= 16 && this.facility <= 23 {
                    {"name": "local" + (this.facility - 16).string(), "category": "application", "criticality": "medium", "description": "Local use facility"}
                  } else {
                    {"name": "unknown", "category": "unknown", "criticality": "low", "description": "Unknown facility"}
                  }
                  
                  # Enhanced severity information with operational impact
                  root.severity_info = {
                    "code": this.severity,
                    "name": if this.severity == 0 { "emergency" }
                           else if this.severity == 1 { "alert" }
                           else if this.severity == 2 { "critical" }
                           else if this.severity == 3 { "error" }
                           else if this.severity == 4 { "warning" }
                           else if this.severity == 5 { "notice" }
                           else if this.severity == 6 { "informational" }
                           else { "debug" },
                    "level": if this.severity <= 2 { "critical" }
                            else if this.severity <= 4 { "warning" }
                            else if this.severity <= 5 { "notice" }
                            else { "info" },
                    "requires_alert": this.severity <= 3,
                    "requires_immediate_action": this.severity <= 1,
                    "color_code": if this.severity <= 2 { "red" }
                                 else if this.severity <= 4 { "yellow" }
                                 else { "green" }
                  }
                  
                  # Timestamp processing for different RFC formats
                  root.timestamp_unix = if this.exists("version") && this.version != "" {
                    # RFC5424 with ISO8601 timestamp
                    this.timestamp.parse_timestamp("2006-01-02T15:04:05.999Z07:00").catch(
                      this.timestamp.parse_timestamp("2006-01-02T15:04:05Z").ts_unix()
                    ).ts_unix()
                  } else {
                    # RFC3164 with traditional syslog timestamp (no year)
                    let current_year = now().ts_format("2006")
                    (current_year + " " + this.timestamp).parse_timestamp("2006 Jan 02 15:04:05").catch(
                      (current_year + " " + this.timestamp).parse_timestamp("2006 Jan _2 15:04:05").ts_unix()
                    ).ts_unix()
                  }
                  
                  # Application and process context
                  root.application_info = {
                    "name": this.tag,
                    "process_id": this.pid.or(this.proc_id).or("").number().catch(0),
                    "hostname": this.hostname,
                    "has_pid": this.exists("pid") && this.pid != "",
                    "message_id": this.msg_id.or(""),
                    "type": if this.tag in ["kernel", "init"] {
                      "system_core"
                    } else if this.tag in ["sshd", "sudo", "auth", "login"] {
                      "authentication"
                    } else if this.tag in ["nginx", "apache", "httpd", "lighttpd"] {
                      "web_server"
                    } else if this.tag in ["postgres", "mysql", "mongod", "redis"] {
                      "database"
                    } else if this.tag in ["docker", "containerd", "kubelet", "systemd"] {
                      "container_orchestration"
                    } else if this.tag in ["firewall", "iptables", "ufw"] {
                      "security_network"
                    } else {
                      "application"
                    },
                    "criticality": if this.tag in ["kernel", "init", "systemd"] {
                      "critical"
                    } else if this.tag in ["sshd", "sudo", "firewall"] {
                      "high"
                    } else {
                      "normal"
                    }
                  }
                  
                  # Message content analysis for event classification
                  let msg_lower = this.message.lowercase()
                  root.event_classification = {
                    "type": if msg_lower.contains("started") || msg_lower.contains("starting") {
                      "service_start"
                    } else if msg_lower.contains("stopped") || msg_lower.contains("stopping") || msg_lower.contains("shutdown") {
                      "service_stop"
                    } else if msg_lower.contains("failed") || msg_lower.contains("error") || msg_lower.contains("failure") {
                      "error_event"
                    } else if msg_lower.contains("connection") || msg_lower.contains("connect") {
                      "connection_event"
                    } else if msg_lower.contains("authentication") || msg_lower.contains("login") || msg_lower.contains("logout") {
                      "authentication_event"
                    } else if msg_lower.contains("configuration") || msg_lower.contains("config") || msg_lower.contains("reload") {
                      "configuration_event"
                    } else if msg_lower.contains("disk") || msg_lower.contains("memory") || msg_lower.contains("cpu") {
                      "resource_event"
                    } else {
                      "general_event"
                    },
                    
                    # Security relevance indicators
                    "security_relevant": msg_lower.re_match("authentication|login|password|sudo|permission|access denied|unauthorized|failed|breach|attack|intrusion"),
                    
                    # Performance relevance indicators  
                    "performance_relevant": msg_lower.re_match("memory|cpu|disk|timeout|slow|performance|load|capacity"),
                    
                    # Network relevance indicators
                    "network_relevant": msg_lower.re_match("connection|network|tcp|udp|port|firewall|routing|dns")
                  }
                  
                  # Structured data parsing (RFC5424)
                  root.structured_data_parsed = if this.exists("structured_data") && this.structured_data != "-" {
                    # Parse structured data elements (simplified)
                    this.structured_data.re_find_all("\\[([^\\]]+)\\]").map_each(element ->
                      element.split(" ").index(0)
                    )
                  } else {
                    []
                  }
                  
                  # Syslog specific metadata
                  root.parsing_metadata = {
                    "processor": "syslog_parser",
                    "success": this.exists("priority") && this.exists("message"),
                    "rfc_version": if this.exists("version") { "5424" } else { "3164" },
                    "has_structured_data": this.exists("structured_data") && this.structured_data != "-",
                    "hostname_valid": this.hostname != "" && this.hostname != "-",
                    "timestamp_format": if this.exists("version") { "iso8601" } else { "traditional" }
                  }

          #
          # Unknown Format Handler
          # Handles logs that don't match any known pattern
          #
          - processors:
              - mapping: |
                  root = {
                    "original_content": this.original_content,
                    "detection_result": this.detection_result,
                    "timestamp_unix": now().ts_unix(),
                    "parsing_metadata": {
                      "processor": "unknown_handler",
                      "success": false,
                      "requires_investigation": true,
                      "failure_reason": "format_not_recognized"
                    },
                    # Provide hints for manual investigation
                    "investigation_hints": {
                      "content_length": this.original_content.length(),
                      "line_structure": if this.original_content.length() < 100 { "short" } 
                                       else if this.original_content.length() < 500 { "medium" } 
                                       else { "long" },
                      "has_json_elements": this.original_content.contains("{") || this.original_content.contains("}"),
                      "has_csv_elements": this.original_content.count(",") >= 2,
                      "has_log_elements": this.original_content.contains("[") && this.original_content.contains("]"),
                      "has_syslog_elements": this.original_content.contains("<") && this.original_content.contains(">"),
                      "has_timestamp_like": this.original_content.re_match("\\d{4}-\\d{2}-\\d{2}|\\d{2}/\\w{3}/\\d{4}|\\w{3}\\s+\\d{1,2}\\s+\\d{2}:\\d{2}"),
                      "encoding_issues": !this.original_content.is_ascii(),
                      "potential_binary": this.original_content.re_match("[\\x00-\\x08\\x0E-\\x1F\\x7F-\\xFF]"),
                      "suggested_actions": [
                        "Check source configuration",
                        "Verify log format documentation", 
                        "Consider custom grok pattern",
                        "Review encoding settings"
                      ]
                    }
                  }

      #
      # STAGE 4: Universal Enrichment and Quality Assessment
      # Add metadata, calculate quality scores, and prepare for routing
      #
      - mapping: |
          root = this
          
          # Calculate processing time
          root.processing_time_ms = (now().ts_unix() - this.processing_start) * 1000
          
          # Quality assessment based on parsing success and confidence
          let quality_score = if this.parsing_metadata.success {
            let base_score = 0.7
            # Confidence bonus
            let confidence_bonus = if this.detection_result.confidence == "high" {
              0.2
            } else if this.detection_result.confidence == "medium" {
              0.1
            } else {
              0.05
            }
            # Data completeness bonus
            let completeness_bonus = if this.exists("timestamp_unix") && this.timestamp_unix > 0 {
              0.1
            } else {
              0.0
            }
            base_score + confidence_bonus + completeness_bonus
          } else {
            # Low quality for failed parsing, but not zero (might be recoverable)
            0.2
          }
          
          root.quality_assessment = {
            "score": quality_score,
            "grade": if quality_score >= 0.8 {
              "excellent"
            } else if quality_score >= 0.6 {
              "good"
            } else if quality_score >= 0.4 {
              "acceptable"
            } else {
              "poor"
            },
            "has_valid_timestamp": this.exists("timestamp_unix") && this.timestamp_unix > 1577836800,  # After 2020-01-01
            "has_structured_content": this.parsing_metadata.success && this.detection_result.format != "unknown",
            "content_completeness": if this.exists("message") || this.exists("original_content") { 1.0 } else { 0.0 },
            "metadata_richness": this.keys().length() / 20.0  # Normalize by expected field count
          }

      #
      # STAGE 5: Security and Anomaly Analysis
      # Analyze content for security threats and operational anomalies
      #
      - mapping: |
          root = this
          
          # Extract content for analysis
          let analysis_content = if this.exists("message") {
            this.message
          } else if this.exists("original_content") {
            this.original_content
          } else {
            ""
          }
          let content_lower = analysis_content.lowercase()
          
          # Security threat indicators
          root.security_analysis = {
            "indicators": {
              # Authentication and access control
              "authentication_failure": content_lower.re_match("failed login|authentication failed|invalid password|access denied|login failed"),
              "privilege_escalation": content_lower.re_match("sudo|su |elevation|admin|root|administrator"),
              "account_manipulation": content_lower.re_match("user added|user deleted|password changed|account locked|account created"),
              
              # Network security threats
              "network_attack": content_lower.re_match("port scan|brute force|ddos|syn flood|intrusion|attack"),
              "malicious_activity": content_lower.re_match("blocked|banned|suspicious|malicious|threat|exploit"),
              "firewall_activity": content_lower.re_match("firewall|iptables|blocked|dropped|denied|rejected"),
              
              # System integrity
              "malware_indicators": content_lower.re_match("virus|malware|trojan|backdoor|rootkit|worm"),
              "system_compromise": content_lower.re_match("compromise|breach|unauthorized|intrusion|penetration"),
              "data_loss_indicators": content_lower.re_match("corruption|lost|missing|deleted|destroyed"),
              
              # Operational security
              "configuration_changes": content_lower.re_match("configuration changed|config modified|settings updated|policy changed"),
              "certificate_issues": content_lower.re_match("certificate|ssl|tls|expired|invalid|untrusted"),
              "encryption_issues": content_lower.re_match("encryption failed|decryption|key|cipher")
            },
            
            # Calculate overall risk level
            "risk_score": 0.0  # Will be calculated below
          }
          
          # Calculate risk score based on indicators
          let security_indicators_count = root.security_analysis.indicators.values().filter(indicator -> indicator == true).length()
          let base_risk = security_indicators_count / 12.0  # Normalize by total indicators
          
          # Severity-based risk amplification (for syslog)
          let severity_risk = if this.exists("severity_info") && this.severity_info.requires_alert {
            0.3
          } else {
            0.0
          }
          
          # Status code risk (for access logs)
          let status_risk = if this.exists("status_analysis") && this.status_analysis.category == "server_error" {
            0.2
          } else {
            0.0
          }
          
          root.security_analysis.risk_score = base_risk + severity_risk + status_risk
          root.security_analysis.risk_level = if root.security_analysis.risk_score >= 0.7 {
            "critical"
          } else if root.security_analysis.risk_score >= 0.4 {
            "high"
          } else if root.security_analysis.risk_score >= 0.2 {
            "medium"
          } else if root.security_analysis.risk_score > 0.0 {
            "low"
          } else {
            "minimal"
          }

      #
      # STAGE 6: Final Metadata and Cleanup
      # Add comprehensive metadata and remove temporary fields
      #
      - mapping: |
          root = this
          
          # Comprehensive processing metadata
          root.metadata = {
            "pipeline": "complete-log-parser",
            "version": "1.0.0",
            "processed_at": now().ts_unix(),
            "processing_node": env("HOSTNAME").or("unknown"),
            "source": meta("source").or("unknown"),
            
            # Detection and parsing results
            "detected_format": this.detection_result.format,
            "detection_confidence": this.detection_result.confidence,
            "detection_score": this.detection_result.score,
            "parsing_success": this.parsing_metadata.success,
            "processor_used": this.parsing_metadata.processor,
            
            # Quality metrics
            "quality_grade": this.quality_assessment.grade,
            "quality_score": this.quality_assessment.score,
            "content_completeness": this.quality_assessment.content_completeness,
            
            # Security assessment
            "security_risk_level": this.security_analysis.risk_level,
            "security_risk_score": this.security_analysis.risk_score,
            "requires_security_review": this.security_analysis.risk_level in ["high", "critical"],
            
            # Performance metrics
            "processing_time_ms": this.processing_time_ms,
            "detection_overhead": (this.processing_time_ms / 1000.0) < 0.1  # Fast detection
          }
          
          # Remove temporary processing fields to reduce output size
          root = this.without("processing_start", "original_content", "detection_result")

  #
  # OUTPUT CONFIGURATION
  # Route processed logs to appropriate destinations based on quality and content
  #
  output:
    broker:
      pattern: fan_out
      outputs:
        #
        # High-Quality Logs → Analytics Platform
        # Route successfully parsed, high-confidence logs to primary analytics
        #
        - switch:
            - check: this.quality_assessment.grade in ["excellent", "good"]
              output:
                http_client:
                  url: "${ANALYTICS_ENDPOINT}/logs/ingest"
                  verb: POST
                  headers:
                    Content-Type: "application/json"
                    Authorization: "Bearer ${ANALYTICS_TOKEN}"
                    X-Parser-Version: "1.0.0"
                  batching:
                    count: 1000
                    period: 30s
                    byte_size: 10485760  # 10MB batches
                  processors:
                    - archive:
                        format: lines
                  # Connection management
                  max_in_flight: 10
                  timeout: "30s"
                  retry_until_success: false
                  max_retries: 3
                  compression: gzip
            - output:
                drop: {}

        #
        # Security Events → SIEM
        # Route security-relevant events to security information and event management
        #
        - switch:
            - check: this.security_analysis.risk_level in ["high", "critical", "medium"]
              output:
                http_client:
                  url: "${SIEM_ENDPOINT}/events/security"
                  verb: POST
                  headers:
                    Content-Type: "application/json"
                    Authorization: "Bearer ${SIEM_TOKEN}"
                    X-Event-Type: "security"
                    X-Risk-Level: "${! security_analysis.risk_level }"
                  batching:
                    count: 100
                    period: 60s
                  processors:
                    # Enrich with security context
                    - mapping: |
                        root.siem_enrichment = {
                          "event_id": uuid_v4(),
                          "ingestion_time": now().ts_unix(),
                          "source_parser": "expanso-log-parser",
                          "threat_indicators": this.security_analysis.indicators,
                          "risk_assessment": {
                            "score": this.security_analysis.risk_score,
                            "level": this.security_analysis.risk_level,
                            "requires_investigation": this.security_analysis.risk_level in ["high", "critical"]
                          }
                        }
                  timeout: "45s"
                  max_retries: 5  # Critical security data
            - output:
                drop: {}

        #
        # Critical Events → Alerting System
        # Route critical system events for immediate response
        #
        - switch:
            - check: (this.exists("severity_info") && this.severity_info.requires_immediate_action) || 
                    this.security_analysis.risk_level == "critical" ||
                    (this.exists("status_analysis") && this.status_analysis.requires_attention)
              output:
                http_client:
                  url: "${ALERT_ENDPOINT}/incidents/create"
                  verb: POST
                  headers:
                    Content-Type: "application/json"
                    X-Priority: "critical"
                    X-Alert-Source: "log-parser"
                  processors:
                    # Create alert payload
                    - mapping: |
                        root = {
                          "alert_id": uuid_v4(),
                          "timestamp": this.timestamp_unix,
                          "alert_type": if this.exists("severity_info") && this.severity_info.requires_immediate_action {
                            "system_critical"
                          } else if this.security_analysis.risk_level == "critical" {
                            "security_critical"
                          } else {
                            "operational_critical"
                          },
                          "severity": if this.exists("severity_info") {
                            this.severity_info.name
                          } else if this.security_analysis.risk_level == "critical" {
                            "critical"
                          } else {
                            "high"
                          },
                          "title": if this.exists("message") {
                            this.message.slice(0, 100) + (if this.message.length() > 100 { "..." } else { "" })
                          } else {
                            "Critical event detected in logs"
                          },
                          "description": this.message.or("No detailed message available"),
                          "source": {
                            "hostname": this.hostname.or(this.application_info.hostname).or("unknown"),
                            "service": this.service.or(this.tag).or(this.application_info.name).or("unknown"),
                            "log_format": this.metadata.detected_format,
                            "processing_node": this.metadata.processing_node
                          },
                          "context": {
                            "security_indicators": this.security_analysis.indicators,
                            "quality_score": this.quality_assessment.score,
                            "original_priority": this.priority.or(0),
                            "event_classification": this.event_classification.type.or("unknown")
                          },
                          "recommended_actions": if this.security_analysis.risk_level == "critical" {
                            ["Investigate immediately", "Check for lateral movement", "Review access logs"]
                          } else if this.exists("severity_info") && this.severity_info.requires_immediate_action {
                            ["Check system status", "Review related services", "Escalate to on-call"]
                          } else {
                            ["Monitor for pattern", "Check system health"]
                          }
                        }
                  timeout: "15s"  # Fast alerting
                  max_retries: 3
            - output:
                drop: {}

        #
        # Poor Quality Logs → Dead Letter Queue
        # Route unparseable or low-confidence logs for investigation
        #
        - switch:
            - check: this.quality_assessment.grade == "poor" || 
                    this.parsing_metadata.success == false ||
                    this.metadata.detection_confidence == "very_low"
              output:
                file:
                  path: "${LOG_PATH}/dlq/failed-parsing-${! now().ts_format(\"2006-01-02\") }.jsonl"
                  codec: lines
                processors:
                  # Add investigation metadata
                  - mapping: |
                      root.dlq_metadata = {
                        "quarantined_at": now().ts_unix(),
                        "failure_reason": if !this.parsing_metadata.success {
                          "parsing_failure"
                        } else if this.metadata.detection_confidence == "very_low" {
                          "low_confidence_detection"
                        } else {
                          "quality_threshold"
                        },
                        "investigation_priority": if this.exists("investigation_hints") && this.investigation_hints.encoding_issues {
                          "high"  # Encoding issues need attention
                        } else if this.metadata.detection_confidence == "very_low" {
                          "medium"  # Unknown formats
                        } else {
                          "low"  # Quality issues
                        },
                        "retry_suggestions": [
                          "Check source log format documentation",
                          "Verify character encoding", 
                          "Consider custom parser rules",
                          "Review input validation"
                        ]
                      }
            - output:
                drop: {}

        #
        # Metrics and Monitoring → Metrics Store
        # Aggregate processing metrics for monitoring dashboards
        #
        - http_client:
            url: "${METRICS_ENDPOINT}/pipeline/stats"
            verb: POST
            headers:
              Content-Type: "application/json"
              Authorization: "Bearer ${METRICS_TOKEN}"
            batching:
              count: 500
              period: 60s
            processors:
              # Aggregate batch statistics
              - mapping: |
                  let logs = content()
                  root = {
                    "timestamp": now().ts_unix(),
                    "batch_id": uuid_v4(),
                    "metrics": {
                      "total_processed": logs.length(),
                      "processing_rate": logs.length() / 60.0,  # per second
                      
                      # Format distribution
                      "format_distribution": logs.group_by("metadata.detected_format").
                                           map_values(items -> {
                                             "count": items.length(),
                                             "percentage": (items.length() / logs.length()) * 100
                                           }),
                      
                      # Quality distribution
                      "quality_distribution": logs.group_by("quality_assessment.grade").
                                            map_values(items -> items.length()),
                      
                      # Performance metrics
                      "avg_processing_time_ms": logs.map("processing_time_ms").
                                               fold(0, acc -> acc + this) / logs.length(),
                      "max_processing_time_ms": logs.map("processing_time_ms").max(),
                      
                      # Security metrics
                      "security_events": {
                        "total": logs.filter(log -> log.security_analysis.risk_score > 0).length(),
                        "high_risk": logs.filter(log -> log.security_analysis.risk_level in ["high", "critical"]).length(),
                        "critical_alerts": logs.filter(log -> log.security_analysis.risk_level == "critical").length()
                      },
                      
                      # Error metrics
                      "error_rate": (logs.filter(log -> !log.parsing_metadata.success).length() / logs.length()) * 100,
                      "dlq_rate": (logs.filter(log -> log.quality_assessment.grade == "poor").length() / logs.length()) * 100,
                      
                      # Detection accuracy
                      "detection_confidence": {
                        "high": logs.filter(log -> log.metadata.detection_confidence == "high").length(),
                        "medium": logs.filter(log -> log.metadata.detection_confidence == "medium").length(),
                        "low": logs.filter(log -> log.metadata.detection_confidence == "low").length(),
                        "very_low": logs.filter(log -> log.metadata.detection_confidence == "very_low").length()
                      }
                    }
                  }
            timeout: "30s"
            max_retries: 2

        #
        # Audit Trail → Compliance Store
        # Maintain processing audit trail for compliance and debugging
        #
        - file:
            path: "${LOG_PATH}/audit/processing-${! now().ts_format(\"2006-01-02\") }.log"
            codec: lines
            processors:
              # Create audit record
              - mapping: |
                  root = {
                    "audit_timestamp": now().ts_unix(),
                    "event_type": "log_processed",
                    "log_id": this.metadata.sequence_id.or(uuid_v4()),
                    "source_format": this.metadata.detected_format,
                    "detection_confidence": this.metadata.detection_confidence,
                    "parsing_result": this.metadata.parsing_success,
                    "quality_grade": this.metadata.quality_grade,
                    "security_risk": this.metadata.security_risk_level,
                    "processing_time_ms": this.metadata.processing_time_ms,
                    "destination": if this.quality_assessment.grade in ["excellent", "good"] {
                      "analytics"
                    } else if this.security_analysis.risk_level in ["high", "critical"] {
                      "siem"
                    } else if this.quality_assessment.grade == "poor" {
                      "dlq"
                    } else {
                      "metrics_only"
                    },
                    "compliance_metadata": {
                      "data_classification": "log_data",
                      "retention_policy": "standard_30_days",
                      "privacy_processed": true,
                      "gdpr_compliant": true
                    }
                  }

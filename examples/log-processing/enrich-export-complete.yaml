name: enrich-export-complete
type: pipeline
description: Production-ready pipeline for enriching logs with metadata and exporting to Amazon S3.

# Complete Log Enrichment & S3 Export Pipeline
# Production-ready pipeline for enriching logs with metadata and exporting to Amazon S3
# 
# Features:
# - Flexible input sources (generated, file, syslog)
# - Comprehensive lineage metadata tracking
# - Analytics-optimized event/metadata structure
# - Intelligent batching for cost optimization
# - Secure S3 export with partitioning and compression
# - Production monitoring and error handling

input:
  # OPTION 1: Generated data (for testing and development)
  generate:
    interval: ${GENERATE_INTERVAL:-1s}
    count: ${GENERATE_COUNT:-0}  # 0 = infinite
    mapping: |
      # Production-realistic synthetic logs
      root.id = uuid_v4()
      root.timestamp = now()
      
      # Realistic log level distribution (70% INFO, 20% WARN, 10% ERROR)
      let level_random = random_int() % 100
      root.level = if $level_random < 70 {
        "INFO"
      } else if $level_random < 90 {
        "WARN"
      } else {
        "ERROR"
      }
      
      # Production services with realistic distribution
      let services = [
        {"name": "api-gateway", "weight": 30},
        {"name": "auth-service", "weight": 20},
        {"name": "user-service", "weight": 15},
        {"name": "payment-service", "weight": 15},
        {"name": "notification-service", "weight": 10},
        {"name": "analytics-service", "weight": 10}
      ]
      # Simplified weighted selection
      root.service = $services.index(random_int() % 6).name
      
      # User and session context
      root.user_id = "user_" + (random_int() % 10000)
      root.session_id = "session_" + (random_int() % 1000)
      root.request_id = uuid_v4()
      root.trace_id = uuid_v4()
      
      # Performance metrics
      root.duration_ms = random_int() % 5000 + 50
      root.status_code = match {
        this.level == "INFO" => [200, 201, 202, 204].index(random_int() % 4)
        this.level == "WARN" => [400, 401, 403, 404, 429].index(random_int() % 5)
        this.level == "ERROR" => [500, 502, 503, 504].index(random_int() % 4)
        _ => 200
      }
      
      # Service-specific contextual messages
      root.message = match {
        this.service == "api-gateway" => "Request processed: " + this.status_code + " in " + this.duration_ms + "ms"
        this.service == "auth-service" => if this.level == "ERROR" { "Authentication failed" } else { "User authenticated successfully" }
        this.service == "payment-service" => if this.level == "ERROR" { "Payment processing failed" } else { "Payment processed: $" + (random_int() % 1000 + 10) }
        this.service == "user-service" => if this.level == "ERROR" { "Profile update failed" } else { "User profile updated" }
        this.service == "notification-service" => if this.level == "ERROR" { "Notification delivery failed" } else { "Notification sent successfully" }
        this.service == "analytics-service" => if this.level == "ERROR" { "Analytics processing failed" } else { "Events processed: " + (random_int() % 1000 + 100) }
        _ => "Service operation completed with status " + this.status_code
      }

  # OPTION 2: File input (for production application logs)
  # Uncomment and configure for real log files
  # file:
  #   paths:
  #     - ${LOG_FILE_PATH:-/var/log/app/*.log}
  #   codec: lines
  #   scanner:
  #     lines:
  #       max_buffer_size: 1000000  # 1MB buffer for long lines
  #   multipart: false
  #   max_buffer: 1000
  #   processors:
  #     - mapping: |
  #         # Parse JSON logs or treat as raw message
  #         root = this.parse_json().catch({
  #           "raw_message": this,
  #           "parse_error": true
  #         })
  #         root.file_context = {
  #           "source_file": file_path(),
  #           "file_offset": file_offset(),
  #           "ingested_at": now()
  #         }

  # OPTION 3: Syslog input (for centralized log collection)
  # Uncomment and configure for syslog reception
  # socket_server:
  #   network: "udp"
  #   address: "${SYSLOG_ADDRESS:-0.0.0.0:1514}"
  #   processors:
  #     - mapping: |
  #         # Parse RFC5424 syslog format
  #         let parsed = this.parse_syslog().catch({
  #           "raw_syslog": this,
  #           "parse_error": true
  #         })
  #         root = $parsed.structured_data.or({})
  #         root.syslog = {
  #           "facility": $parsed.facility,
  #           "severity": $parsed.severity,
  #           "timestamp": $parsed.timestamp,
  #           "hostname": $parsed.hostname,
  #           "app_name": $parsed.app_name,
  #           "message": $parsed.message
  #         }
  #         root.id = uuid_v4()
  #         root.timestamp = $parsed.timestamp.or(now())
  #         root.level = match {
  #           $parsed.severity <= 3 => "ERROR"
  #           $parsed.severity <= 4 => "WARN"
  #           $parsed.severity <= 6 => "INFO"
  #           _ => "DEBUG"
  #         }

# Buffer configuration for memory management
buffer:
  memory:
    limit: "${BUFFER_MEMORY:-100MB}"

# Processing pipeline
pipeline:
  processors:
    # Step 1: Add comprehensive lineage metadata
    - mapping: |
        # Preserve all original data
        root = this
        
        # Processing timestamp for performance tracking
        let processing_start = now()
        
        # Comprehensive lineage information
        root.lineage = {
          # Node identification
          "node_id": env("NODE_ID").or("edge-node-" + uuid_v4().slice(0, 8)),
          "node_region": env("AWS_REGION").or("us-east-1"),
          "node_zone": env("AVAILABILITY_ZONE").or(env("AWS_REGION").or("us-east-1") + "a"),
          "node_instance_type": env("INSTANCE_TYPE").or("unknown"),
          
          # Pipeline context
          "pipeline_name": "log-enrichment-s3-production",
          "pipeline_version": env("PIPELINE_VERSION").or("5.0.0"),
          "pipeline_step": "lineage_enrichment",
          
          # Temporal tracking
          "processed_at": $processing_start,
          "processing_duration_ms": 2.5,  # Will be updated in later step
          "sequence_number": counter("messages_processed"),
          "batch_id": "batch_" + $processing_start.format_timestamp("20060102_150405", "UTC"),
          
          # Environment context
          "environment": env("ENVIRONMENT").or("production"),
          "deployment_id": env("DEPLOYMENT_ID").or("unknown"),
          "git_commit": env("GIT_COMMIT").or("unknown"),
          "build_timestamp": env("BUILD_TIMESTAMP").or("unknown"),
          
          # Data quality indicators
          "data_freshness_ms": ($processing_start.ts_unix_nano() - (this.timestamp.or(now())).parse_timestamp("2006-01-02T15:04:05Z").ts_unix_nano()) / 1000000
        }

    # Step 2: Restructure into analytics-optimized event/metadata format
    - mapping: |
        # Create structured event section (business data)
        root.event = {
          # Core event identification
          "id": this.id.or(uuid_v4()),
          "timestamp": this.timestamp.or(now()),
          "type": "application_log",
          "version": "5.0",
          
          # Application context
          "application": {
            "service": this.service.or("unknown"),
            "level": this.level.or("INFO"),
            "message": this.message.or(""),
            "duration_ms": this.duration_ms,
            "status_code": this.status_code
          },
          
          # User and session context
          "user": if this.user_id != null {
            {
              "id": this.user_id,
              "session_id": this.session_id
            }
          } else { null },
          
          # Request tracing
          "trace": {
            "request_id": this.request_id.or(uuid_v4()),
            "trace_id": this.trace_id.or(this.request_id).or(uuid_v4()),
            "parent_span_id": null
          }
        }
        
        # Create structured metadata section (operational data)
        root.metadata = {
          # Lineage information
          "lineage": this.lineage,
          
          # Data quality metrics
          "quality": {
            "completeness_score": (
              (if this.user_id != null { 1 } else { 0 }) +
              (if this.request_id != null { 1 } else { 0 }) +
              (if this.message != null && this.message != "" { 1 } else { 0 }) +
              (if this.service != null { 1 } else { 0 })
            ) / 4.0,
            "has_user_context": this.user_id != null,
            "has_trace_context": this.trace_id != null,
            "is_error": this.level == "ERROR",
            "message_length": this.message.or("").length()
          },
          
          # Processing metadata
          "processing": {
            "transformations_applied": ["lineage_enrichment", "format_restructuring"],
            "schema_version": "5.0",
            "format": "event_metadata_v5",
            "compression_eligible": true
          },
          
          # Storage optimization hints
          "storage": {
            "partition_key": this.service.or("unknown") + "_" + this.level.or("INFO"),
            "retention_policy": if this.level == "ERROR" { "extended" } else { "standard" },
            "analytics_priority": if this.user_id != null { "high" } else { "normal" }
          }
        }

    # Step 3: Performance tracking and final metadata
    - mapping: |
        root = this
        
        # Update processing duration (final step)
        let processing_end = now()
        let processing_duration_ms = ($processing_end.ts_unix_nano() - this.metadata.lineage.processed_at.ts_unix_nano()) / 1000000
        
        root.metadata.lineage.processing_duration_ms = $processing_duration_ms
        root.metadata.lineage.completed_at = $processing_end
        
        # Add performance alerts if needed
        if $processing_duration_ms > 1000 {
          root.metadata.alerts = root.metadata.alerts.or([]) + ["high_processing_latency"]
        }

# Output configuration with multiple destinations
output:
  broker:
    pattern: fan_out
    outputs:
      # Primary S3 export with intelligent batching
      - aws_s3:
          bucket: ${S3_BUCKET_NAME}
          
          # Hive-style partitioning for analytics efficiency
          path: logs/year=${!timestamp("2006")}/month=${!timestamp("01")}/day=${!timestamp("02")}/hour=${!timestamp("15")}/service=${!json("event.application.service")}/level=${!json("event.application.level")}/logs_${!timestamp_unix()}_${!hostname()}.jsonl.gz
          
          # Production-optimized batching
          batching:
            count: ${BATCH_COUNT:-200}           # 200 messages per batch (balance of cost and latency)
            period: ${BATCH_PERIOD:-2m}          # 2-minute maximum latency
            byte_size: ${BATCH_SIZE:-5242880}    # 5MB maximum uncompressed batch size
            
            # Pre-upload compression for storage cost reduction
            processors:
              - compress:
                  algorithm: gzip
                  level: 6  # Good compression/speed balance
          
          # S3 optimization settings
          content_type: application/x-ndjson
          content_encoding: gzip
          storage_class: ${S3_STORAGE_CLASS:-STANDARD_IA}  # Cost-optimized storage
          
          # Metadata for cost tracking and lifecycle management
          metadata:
            environment: ${ENVIRONMENT:-production}
            pipeline: log-enrichment-production
            version: ${PIPELINE_VERSION:-5.0.0}
            node_id: ${NODE_ID:-unknown}
            created_by: expanso-platform
            data_type: application_logs
          
          # Upload optimization for performance
          upload_cutoff: 104857600   # 100MB multipart upload threshold
          chunk_size: 10485760       # 10MB chunk size for optimal throughput
          
          # AWS configuration
          credentials:
            profile: ${AWS_PROFILE}  # Remove this line to use IAM role
          region: ${AWS_REGION}

      # High-priority error stream (separate processing)
      - aws_s3:
          bucket: ${S3_ERROR_BUCKET_NAME:-${S3_BUCKET_NAME}}
          path: errors/year=${!timestamp("2006")}/month=${!timestamp("01")}/day=${!timestamp("02")}/error_${!timestamp_unix()}_${!hostname()}.jsonl
          
          # Filter for errors and high-priority events only
          processors:
            - mapping: |
                root = if this.event.application.level == "ERROR" || this.metadata.quality.is_error {
                  this
                } else {
                  deleted()
                }
          
          # Fast processing for critical errors
          batching:
            count: 50               # Smaller batches for faster processing
            period: 1m              # 1-minute maximum latency for errors
            byte_size: 1048576      # 1MB batches
          
          # No compression for fast error access
          content_type: application/x-ndjson
          storage_class: STANDARD   # Fast access tier for errors
          
          credentials:
            profile: ${AWS_PROFILE}
          region: ${AWS_REGION}

      # Local backup and debugging (optional)
      # Uncomment for local file backup
      # - file:
      #     path: /var/log/expanso/backup/backup_${!timestamp_unix()}.jsonl
      #     codec: lines
      #     batching:
      #       count: 1000
      #       period: 5m

# Metrics and monitoring configuration
metrics:
  prometheus:
    prefix: log_enrichment_prod
    labels:
      environment: ${ENVIRONMENT:-production}
      node_id: ${NODE_ID:-unknown}
      pipeline_version: ${PIPELINE_VERSION:-5.0.0}
      deployment_id: ${DEPLOYMENT_ID:-unknown}
    
    # Push to Prometheus pushgateway if available
    push_gateway:
      url: ${PROMETHEUS_PUSHGATEWAY_URL}
      job_name: log_enrichment_production
      push_interval: "${METRICS_PUSH_INTERVAL:-30s}"

# Logging configuration
logger:
  level: ${LOG_LEVEL:-info}  # debug, info, warn, error
  format: json
  add_timestamp: true
  timestamp_name: "@timestamp"
  message_name: "message"
  level_name: "level"

# HTTP server for health checks and metrics (optional)
http:
  enabled: ${HTTP_ENABLED:-true}
  address: "${HTTP_ADDRESS:-0.0.0.0:4040}"
  root_path: "/expanso"
  debug_endpoints: ${DEBUG_ENABLED:-false}
  
  # CORS configuration for web dashboards
  cors:
    enabled: true
    allowed_origins: ["*"]

# Shutdown configuration
shutdown_timeout: "${SHUTDOWN_TIMEOUT:-20s}"

# Environment-specific overrides
# Uncomment and modify based on your deployment environment

# Development environment
# input:
#   generate:
#     interval: 5s
#     count: 100  # Limited for development
# output:
#   stdout: {}  # Console output for development

# Staging environment  
# metrics:
#   prometheus:
#     labels:
#       environment: staging
# output:
#   aws_s3:
#     bucket: your-staging-logs
#     storage_class: STANDARD

# Production environment
# buffer:
#   memory:
#     limit: "200MB"  # More memory for production
# output:
#   aws_s3:
#     storage_class: STANDARD_IA
#     metadata:
#       retention_policy: "7_years"
#       compliance: "gdpr_compliant"